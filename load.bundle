25621
{"version":0,"id":null,"main":"file:///load.js","imports":{},"resolutions":{"file:///load.js":{"#package":"file:///package.json","bare-bundle":"file:///node_modules/bare-bundle/index.js","localdrive":"file:///node_modules/localdrive/index.js","pear-appdrive":"file:///node_modules/pear-appdrive/index.js","pear-pack":"file:///node_modules/pear-pack/index.js","url-file-url":"file:///node_modules/url-file-url/index.js"},"file:///node_modules/b4a/index.js":{"#package":"file:///node_modules/b4a/package.json"},"file:///node_modules/b4a/package.json":{},"file:///node_modules/bare-addon-resolve/index.js":{"#package":"file:///node_modules/bare-addon-resolve/package.json","./lib/errors":"file:///node_modules/bare-addon-resolve/lib/errors.js","bare-module-resolve":"file:///node_modules/bare-module-resolve/index.js","bare-semver":"file:///node_modules/bare-semver/index.js"},"file:///node_modules/bare-addon-resolve/lib/errors.js":{"#package":"file:///node_modules/bare-addon-resolve/package.json"},"file:///node_modules/bare-addon-resolve/package.json":{},"file:///node_modules/bare-bundle/index.js":{"#package":"file:///node_modules/bare-bundle/package.json","./lib/errors":"file:///node_modules/bare-bundle/lib/errors.js"},"file:///node_modules/bare-bundle/lib/errors.js":{"#package":"file:///node_modules/bare-bundle/package.json"},"file:///node_modules/bare-bundle/package.json":{},"file:///node_modules/bare-module-lexer/binding/node.js":{"#package":"file:///node_modules/bare-module-lexer/package.json","..":{"darwin":{"arm64":"/prebuilds/darwin-arm64/3bffcbf6db68bd18ae83b3017457d1f37a355e35b6ba890c5d1679f88c7d33ef.node","x64":"/prebuilds/darwin-x64/66b04bb21aee92f45663da20a57dfe347f77e95daf06b218b9cef0c3eed086ee.node"},"linux":{"arm64":"/prebuilds/linux-arm64/29199e6e3a17889c1df3c8e0071130ad7eba52ba01d6048a3bcab894aa663c5b.node","x64":"/prebuilds/linux-x64/89d32de6f2420cd6b07687f944820d3f573caa772882d6795c1af29a2b707dfc.node"},"win32":"/prebuilds/win32-x64/e6f1facc574d4fa7f4c1c716f063b2b8e7fb3d37b700141dfba497d82f4d756e.node"},"require-addon":"file:///node_modules/require-addon/index.js"},"file:///node_modules/bare-module-lexer/index.js":{"#binding":"file:///node_modules/bare-module-lexer/binding/node.js","#package":"file:///node_modules/bare-module-lexer/package.json"},"file:///node_modules/bare-module-lexer/package.json":{},"file:///node_modules/bare-module-resolve/index.js":{"#package":"file:///node_modules/bare-module-resolve/package.json","./lib/errors":"file:///node_modules/bare-module-resolve/lib/errors.js","bare-semver":"file:///node_modules/bare-semver/index.js"},"file:///node_modules/bare-module-resolve/lib/errors.js":{"#package":"file:///node_modules/bare-module-resolve/package.json"},"file:///node_modules/bare-module-resolve/package.json":{},"file:///node_modules/bare-module-traverse/index.js":{"#package":"file:///node_modules/bare-module-traverse/package.json","./lib/errors":"file:///node_modules/bare-module-traverse/lib/errors.js","./lib/resolve":"file:///node_modules/bare-module-traverse/lib/resolve.js","bare-addon-resolve":"file:///node_modules/bare-addon-resolve/index.js","bare-module-lexer":"file:///node_modules/bare-module-lexer/index.js","bare-module-resolve":"file:///node_modules/bare-module-resolve/index.js"},"file:///node_modules/bare-module-traverse/lib/errors.js":{"#package":"file:///node_modules/bare-module-traverse/package.json"},"file:///node_modules/bare-module-traverse/lib/resolve.js":{"#package":"file:///node_modules/bare-module-traverse/package.json","./resolve/bare":"file:///node_modules/bare-module-traverse/lib/resolve/bare.js","./resolve/default":"file:///node_modules/bare-module-traverse/lib/resolve/default.js","./resolve/node":"file:///node_modules/bare-module-traverse/lib/resolve/node.js","bare-addon-resolve":"file:///node_modules/bare-addon-resolve/index.js","bare-module-resolve":"file:///node_modules/bare-module-resolve/index.js"},"file:///node_modules/bare-module-traverse/lib/resolve/bare.js":{"#package":"file:///node_modules/bare-module-traverse/package.json","../resolve":"file:///node_modules/bare-module-traverse/lib/resolve.js","../runtime":"file:///node_modules/bare-module-traverse/lib/runtime.js","bare-module-lexer":"file:///node_modules/bare-module-lexer/index.js"},"file:///node_modules/bare-module-traverse/lib/resolve/default.js":{"#package":"file:///node_modules/bare-module-traverse/package.json","../resolve":"file:///node_modules/bare-module-traverse/lib/resolve.js","bare-module-lexer":"file:///node_modules/bare-module-lexer/index.js"},"file:///node_modules/bare-module-traverse/lib/resolve/node.js":{"#package":"file:///node_modules/bare-module-traverse/package.json","../resolve":"file:///node_modules/bare-module-traverse/lib/resolve.js","../runtime":"file:///node_modules/bare-module-traverse/lib/runtime.js","bare-module-lexer":"file:///node_modules/bare-module-lexer/index.js"},"file:///node_modules/bare-module-traverse/lib/runtime.js":{"#package":"file:///node_modules/bare-module-traverse/package.json","#runtime":"file:///node_modules/bare-module-traverse/lib/runtime/node.js"},"file:///node_modules/bare-module-traverse/lib/runtime/node.js":{"#package":"file:///node_modules/bare-module-traverse/package.json"},"file:///node_modules/bare-module-traverse/package.json":{},"file:///node_modules/bare-pack-drive/index.js":{"#package":"file:///node_modules/bare-pack-drive/package.json","bare-pack":"file:///node_modules/bare-pack/index.js"},"file:///node_modules/bare-pack-drive/package.json":{},"file:///node_modules/bare-pack/index.js":{"#package":"file:///node_modules/bare-pack/package.json","./lib/preset":"file:///node_modules/bare-pack/lib/preset.js","bare-bundle":"file:///node_modules/bare-bundle/index.js","bare-module-traverse":"file:///node_modules/bare-module-traverse/index.js","promaphore":"file:///node_modules/promaphore/index.js"},"file:///node_modules/bare-pack/lib/preset.js":{"#package":"file:///node_modules/bare-pack/package.json","./preset/android":"file:///node_modules/bare-pack/lib/preset/android.js","./preset/darwin":"file:///node_modules/bare-pack/lib/preset/darwin.js","./preset/desktop":"file:///node_modules/bare-pack/lib/preset/desktop.js","./preset/ios":"file:///node_modules/bare-pack/lib/preset/ios.js","./preset/linux":"file:///node_modules/bare-pack/lib/preset/linux.js","./preset/mobile":"file:///node_modules/bare-pack/lib/preset/mobile.js","./preset/win32":"file:///node_modules/bare-pack/lib/preset/win32.js"},"file:///node_modules/bare-pack/lib/preset/android.js":{"#package":"file:///node_modules/bare-pack/package.json"},"file:///node_modules/bare-pack/lib/preset/darwin.js":{"#package":"file:///node_modules/bare-pack/package.json"},"file:///node_modules/bare-pack/lib/preset/desktop.js":{"#package":"file:///node_modules/bare-pack/package.json"},"file:///node_modules/bare-pack/lib/preset/ios.js":{"#package":"file:///node_modules/bare-pack/package.json"},"file:///node_modules/bare-pack/lib/preset/linux.js":{"#package":"file:///node_modules/bare-pack/package.json"},"file:///node_modules/bare-pack/lib/preset/mobile.js":{"#package":"file:///node_modules/bare-pack/package.json"},"file:///node_modules/bare-pack/lib/preset/win32.js":{"#package":"file:///node_modules/bare-pack/package.json"},"file:///node_modules/bare-pack/package.json":{},"file:///node_modules/bare-semver/index.js":{"#package":"file:///node_modules/bare-semver/package.json","./lib/comparator":"file:///node_modules/bare-semver/lib/comparator.js","./lib/constants":"file:///node_modules/bare-semver/lib/constants.js","./lib/errors":"file:///node_modules/bare-semver/lib/errors.js","./lib/range":"file:///node_modules/bare-semver/lib/range.js","./lib/version":"file:///node_modules/bare-semver/lib/version.js"},"file:///node_modules/bare-semver/lib/comparator.js":{"#package":"file:///node_modules/bare-semver/package.json","./constants":"file:///node_modules/bare-semver/lib/constants.js"},"file:///node_modules/bare-semver/lib/constants.js":{"#package":"file:///node_modules/bare-semver/package.json"},"file:///node_modules/bare-semver/lib/errors.js":{"#package":"file:///node_modules/bare-semver/package.json"},"file:///node_modules/bare-semver/lib/range.js":{"#package":"file:///node_modules/bare-semver/package.json","./comparator":"file:///node_modules/bare-semver/lib/comparator.js","./constants":"file:///node_modules/bare-semver/lib/constants.js","./errors":"file:///node_modules/bare-semver/lib/errors.js","./version":"file:///node_modules/bare-semver/lib/version.js"},"file:///node_modules/bare-semver/lib/version.js":{"#package":"file:///node_modules/bare-semver/package.json","./errors":"file:///node_modules/bare-semver/lib/errors.js"},"file:///node_modules/bare-semver/package.json":{},"file:///node_modules/bare-unpack/index.js":{"#package":"file:///node_modules/bare-unpack/package.json","bare-bundle":"file:///node_modules/bare-bundle/index.js","promaphore":"file:///node_modules/promaphore/index.js"},"file:///node_modules/bare-unpack/package.json":{},"file:///node_modules/binary-stream-equals/index.js":{"#package":"file:///node_modules/binary-stream-equals/package.json","b4a":"file:///node_modules/b4a/index.js"},"file:///node_modules/binary-stream-equals/package.json":{},"file:///node_modules/events-universal/default.js":{"#package":"file:///node_modules/events-universal/package.json","events":"builtin:events"},"file:///node_modules/events-universal/package.json":{},"file:///node_modules/fast-fifo/fixed-size.js":{"#package":"file:///node_modules/fast-fifo/package.json"},"file:///node_modules/fast-fifo/index.js":{"#package":"file:///node_modules/fast-fifo/package.json","./fixed-size":"file:///node_modules/fast-fifo/fixed-size.js"},"file:///node_modules/fast-fifo/package.json":{},"file:///node_modules/localdrive/index.js":{"#package":"file:///node_modules/localdrive/package.json","./streams.js":"file:///node_modules/localdrive/streams.js","b4a":"file:///node_modules/b4a/index.js","fs":"builtin:fs","fs/promises":"builtin:fs/promises","mirror-drive":"file:///node_modules/mirror-drive/index.js","mutexify/promise":"file:///node_modules/mutexify/promise.js","path":"builtin:path","unix-path-resolve":"file:///node_modules/unix-path-resolve/index.js"},"file:///node_modules/localdrive/package.json":{},"file:///node_modules/localdrive/streams.js":{"#package":"file:///node_modules/localdrive/package.json","b4a":"file:///node_modules/b4a/index.js","fs":"builtin:fs","fs/promises":"builtin:fs/promises","path":"builtin:path","streamx":"file:///node_modules/streamx/index.js"},"file:///node_modules/mirror-drive/index.js":{"#package":"file:///node_modules/mirror-drive/package.json","binary-stream-equals":"file:///node_modules/binary-stream-equals/index.js","same-data":"file:///node_modules/same-data/index.js","streamx":"file:///node_modules/streamx/index.js","unix-path-resolve":"file:///node_modules/unix-path-resolve/index.js"},"file:///node_modules/mirror-drive/package.json":{},"file:///node_modules/mutexify/index.js":{"#package":"file:///node_modules/mutexify/package.json","queue-tick":"file:///node_modules/queue-tick/process-next-tick.js"},"file:///node_modules/mutexify/package.json":{},"file:///node_modules/mutexify/promise.js":{"#package":"file:///node_modules/mutexify/package.json",".":"file:///node_modules/mutexify/index.js"},"file:///node_modules/pear-appdrive/index.js":{"#package":"file:///node_modules/pear-appdrive/package.json","pear-ref":"file:///node_modules/pear-ref/index.js","ready-resource":"file:///node_modules/ready-resource/index.js"},"file:///node_modules/pear-appdrive/package.json":{},"file:///node_modules/pear-pack/index.js":{"#package":"file:///node_modules/pear-pack/package.json","bare-module-lexer":"file:///node_modules/bare-module-lexer/index.js","bare-module-traverse":"file:///node_modules/bare-module-traverse/index.js","bare-pack-drive":"file:///node_modules/bare-pack-drive/index.js","bare-unpack":"file:///node_modules/bare-unpack/index.js","sodium-native":"file:///node_modules/sodium-native/index.js"},"file:///node_modules/pear-pack/package.json":{},"file:///node_modules/pear-ref/index.js":{"#package":"file:///node_modules/pear-ref/package.json","events":"builtin:events"},"file:///node_modules/pear-ref/package.json":{},"file:///node_modules/promaphore/index.js":{"#package":"file:///node_modules/promaphore/package.json"},"file:///node_modules/promaphore/package.json":{},"file:///node_modules/queue-tick/package.json":{},"file:///node_modules/queue-tick/process-next-tick.js":{"#package":"file:///node_modules/queue-tick/package.json","./queue-microtask":"file:///node_modules/queue-tick/queue-microtask.js"},"file:///node_modules/queue-tick/queue-microtask.js":{"#package":"file:///node_modules/queue-tick/package.json"},"file:///node_modules/ready-resource/index.js":{"#package":"file:///node_modules/ready-resource/package.json","events":"builtin:events"},"file:///node_modules/ready-resource/package.json":{},"file:///node_modules/require-addon/index.js":{"#package":"file:///node_modules/require-addon/package.json","./lib/runtime":"file:///node_modules/require-addon/lib/runtime.js","./lib/runtime/bare":"file:///node_modules/require-addon/lib/runtime/bare.js","./lib/runtime/default":"file:///node_modules/require-addon/lib/runtime/default.js","./lib/runtime/node":"file:///node_modules/require-addon/lib/runtime/node.js"},"file:///node_modules/require-addon/lib/runtime.js":{"#package":"file:///node_modules/require-addon/package.json"},"file:///node_modules/require-addon/lib/runtime/bare.js":{"#package":"file:///node_modules/require-addon/package.json"},"file:///node_modules/require-addon/lib/runtime/default.js":{"#package":"file:///node_modules/require-addon/package.json"},"file:///node_modules/require-addon/lib/runtime/node.js":{"#package":"file:///node_modules/require-addon/package.json","bare-addon-resolve":"file:///node_modules/bare-addon-resolve/index.js","url":"builtin:url"},"file:///node_modules/require-addon/package.json":{},"file:///node_modules/same-data/index.js":{"#package":"file:///node_modules/same-data/package.json"},"file:///node_modules/same-data/package.json":{},"file:///node_modules/sodium-native/binding.js":{"#package":"file:///node_modules/sodium-native/package.json",".":{"darwin":{"arm64":"/prebuilds/darwin-arm64/90970c3451187fc9e9bb421effa154df61cf39890549c64306cd0d0cb0a35c0d.node","x64":"/prebuilds/darwin-x64/f2649fa72d517887350904f8de92f97cdb8b92e5729eb937ac9d6709c2001743.node"},"linux":{"arm64":"/prebuilds/linux-arm64/75a558f5cb14f6459f6b7a4b09b72733c8e2afaf43d7c9bd5fc362d17b4b98cd.node","x64":"/prebuilds/linux-x64/52fc7666c0fb44cc3f1a39bc6d51d12f0af8460f154e88ef19b11368e5d510a7.node"},"win32":"/prebuilds/win32-x64/978a4842cb2d6a5af13fdcfa6e7511e3af86d390271730738a7d38477132371d.node"},"require-addon":"file:///node_modules/require-addon/index.js"},"file:///node_modules/sodium-native/index.js":{"#package":"file:///node_modules/sodium-native/package.json","./binding":"file:///node_modules/sodium-native/binding.js","which-runtime":"file:///node_modules/which-runtime/index.js"},"file:///node_modules/sodium-native/package.json":{},"file:///node_modules/streamx/index.js":{"#package":"file:///node_modules/streamx/package.json","events-universal":"file:///node_modules/events-universal/default.js","fast-fifo":"file:///node_modules/fast-fifo/index.js","text-decoder":"file:///node_modules/text-decoder/index.js"},"file:///node_modules/streamx/package.json":{},"file:///node_modules/text-decoder/index.js":{"#package":"file:///node_modules/text-decoder/package.json","./lib/pass-through-decoder":"file:///node_modules/text-decoder/lib/pass-through-decoder.js","./lib/utf8-decoder":"file:///node_modules/text-decoder/lib/utf8-decoder.js"},"file:///node_modules/text-decoder/lib/pass-through-decoder.js":{"#package":"file:///node_modules/text-decoder/package.json","b4a":"file:///node_modules/b4a/index.js"},"file:///node_modules/text-decoder/lib/utf8-decoder.js":{"#package":"file:///node_modules/text-decoder/package.json","b4a":"file:///node_modules/b4a/index.js"},"file:///node_modules/text-decoder/package.json":{},"file:///node_modules/unix-path-resolve/index.js":{"#package":"file:///node_modules/unix-path-resolve/package.json"},"file:///node_modules/unix-path-resolve/package.json":{},"file:///node_modules/url-file-url/index.js":{"#package":"file:///node_modules/url-file-url/package.json","path":"builtin:path","which-runtime":"file:///node_modules/which-runtime/index.js"},"file:///node_modules/url-file-url/package.json":{},"file:///node_modules/which-runtime/index.js":{"#package":"file:///node_modules/which-runtime/package.json"},"file:///node_modules/which-runtime/package.json":{},"file:///package.json":{}},"addons":[],"assets":[],"files":{"file:///load.js":{"offset":0,"length":1031,"mode":420},"file:///node_modules/b4a/index.js":{"offset":1031,"length":4054,"mode":420},"file:///node_modules/b4a/package.json":{"offset":5085,"length":1140,"mode":420},"file:///node_modules/bare-addon-resolve/index.js":{"offset":6225,"length":10801,"mode":420},"file:///node_modules/bare-addon-resolve/lib/errors.js":{"offset":17026,"length":642,"mode":420},"file:///node_modules/bare-addon-resolve/package.json":{"offset":17668,"length":1175,"mode":420},"file:///node_modules/bare-bundle/index.js":{"offset":18843,"length":13491,"mode":420},"file:///node_modules/bare-bundle/lib/errors.js":{"offset":32334,"length":467,"mode":420},"file:///node_modules/bare-bundle/package.json":{"offset":32801,"length":1059,"mode":420},"file:///node_modules/bare-module-lexer/binding/node.js":{"offset":33860,"length":91,"mode":420},"file:///node_modules/bare-module-lexer/index.js":{"offset":33951,"length":1386,"mode":420},"file:///node_modules/bare-module-lexer/package.json":{"offset":35337,"length":1545,"mode":420},"file:///node_modules/bare-module-resolve/index.js":{"offset":36882,"length":21186,"mode":420},"file:///node_modules/bare-module-resolve/lib/errors.js":{"offset":58068,"length":1195,"mode":420},"file:///node_modules/bare-module-resolve/package.json":{"offset":59263,"length":1143,"mode":420},"file:///node_modules/bare-module-traverse/index.js":{"offset":60406,"length":15478,"mode":420},"file:///node_modules/bare-module-traverse/lib/errors.js":{"offset":75884,"length":1417,"mode":420},"file:///node_modules/bare-module-traverse/lib/resolve.js":{"offset":77301,"length":237,"mode":420},"file:///node_modules/bare-module-traverse/lib/resolve/bare.js":{"offset":77538,"length":1384,"mode":420},"file:///node_modules/bare-module-traverse/lib/resolve/default.js":{"offset":78922,"length":306,"mode":420},"file:///node_modules/bare-module-traverse/lib/resolve/node.js":{"offset":79228,"length":1136,"mode":420},"file:///node_modules/bare-module-traverse/lib/runtime.js":{"offset":80364,"length":37,"mode":420},"file:///node_modules/bare-module-traverse/lib/runtime/node.js":{"offset":80401,"length":90,"mode":420},"file:///node_modules/bare-module-traverse/package.json":{"offset":80491,"length":1919,"mode":420},"file:///node_modules/bare-pack-drive/index.js":{"offset":82410,"length":772,"mode":420},"file:///node_modules/bare-pack-drive/package.json":{"offset":83182,"length":999,"mode":420},"file:///node_modules/bare-pack/index.js":{"offset":84181,"length":2185,"mode":420},"file:///node_modules/bare-pack/lib/preset.js":{"offset":86366,"length":302,"mode":420},"file:///node_modules/bare-pack/lib/preset/android.js":{"offset":86668,"length":111,"mode":420},"file:///node_modules/bare-pack/lib/preset/darwin.js":{"offset":86779,"length":62,"mode":420},"file:///node_modules/bare-pack/lib/preset/desktop.js":{"offset":86841,"length":146,"mode":420},"file:///node_modules/bare-pack/lib/preset/ios.js":{"offset":86987,"length":105,"mode":420},"file:///node_modules/bare-pack/lib/preset/linux.js":{"offset":87092,"length":60,"mode":420},"file:///node_modules/bare-pack/lib/preset/mobile.js":{"offset":87152,"length":200,"mode":420},"file:///node_modules/bare-pack/lib/preset/win32.js":{"offset":87352,"length":60,"mode":420},"file:///node_modules/bare-pack/package.json":{"offset":87412,"length":1760,"mode":420},"file:///node_modules/bare-semver/index.js":{"offset":89172,"length":469,"mode":420},"file:///node_modules/bare-semver/lib/comparator.js":{"offset":89641,"length":705,"mode":420},"file:///node_modules/bare-semver/lib/constants.js":{"offset":90346,"length":67,"mode":420},"file:///node_modules/bare-semver/lib/errors.js":{"offset":90413,"length":529,"mode":420},"file:///node_modules/bare-semver/lib/range.js":{"offset":90942,"length":2429,"mode":420},"file:///node_modules/bare-semver/lib/version.js":{"offset":93371,"length":3824,"mode":420},"file:///node_modules/bare-semver/package.json":{"offset":97195,"length":868,"mode":420},"file:///node_modules/bare-unpack/index.js":{"offset":98063,"length":2350,"mode":420},"file:///node_modules/bare-unpack/package.json":{"offset":100413,"length":1193,"mode":420},"file:///node_modules/binary-stream-equals/index.js":{"offset":101606,"length":2018,"mode":420},"file:///node_modules/binary-stream-equals/package.json":{"offset":103624,"length":687,"mode":420},"file:///node_modules/events-universal/default.js":{"offset":104311,"length":35,"mode":420},"file:///node_modules/events-universal/package.json":{"offset":104346,"length":908,"mode":420},"file:///node_modules/fast-fifo/fixed-size.js":{"offset":105254,"length":875,"mode":420},"file:///node_modules/fast-fifo/index.js":{"offset":106129,"length":972,"mode":420},"file:///node_modules/fast-fifo/package.json":{"offset":107101,"length":682,"mode":420},"file:///node_modules/localdrive/index.js":{"offset":107783,"length":7936,"mode":420},"file:///node_modules/localdrive/package.json":{"offset":115719,"length":1139,"mode":420},"file:///node_modules/localdrive/streams.js":{"offset":116858,"length":5155,"mode":420},"file:///node_modules/mirror-drive/index.js":{"offset":122013,"length":5725,"mode":420},"file:///node_modules/mirror-drive/package.json":{"offset":127738,"length":926,"mode":420},"file:///node_modules/mutexify/index.js":{"offset":128664,"length":536,"mode":420},"file:///node_modules/mutexify/package.json":{"offset":129200,"length":679,"mode":420},"file:///node_modules/mutexify/promise.js":{"offset":129879,"length":324,"mode":420},"file:///node_modules/pear-appdrive/index.js":{"offset":130203,"length":1141,"mode":420},"file:///node_modules/pear-appdrive/package.json":{"offset":131344,"length":646,"mode":420},"file:///node_modules/pear-pack/index.js":{"offset":131990,"length":2469,"mode":420},"file:///node_modules/pear-pack/package.json":{"offset":134459,"length":757,"mode":420},"file:///node_modules/pear-ref/index.js":{"offset":135216,"length":633,"mode":420},"file:///node_modules/pear-ref/package.json":{"offset":135849,"length":528,"mode":420},"file:///node_modules/promaphore/index.js":{"offset":136377,"length":995,"mode":420},"file:///node_modules/promaphore/package.json":{"offset":137372,"length":450,"mode":420},"file:///node_modules/queue-tick/package.json":{"offset":137822,"length":669,"mode":420},"file:///node_modules/queue-tick/process-next-tick.js":{"offset":138491,"length":160,"mode":420},"file:///node_modules/queue-tick/queue-microtask.js":{"offset":138651,"length":108,"mode":420},"file:///node_modules/ready-resource/index.js":{"offset":138759,"length":1091,"mode":420},"file:///node_modules/ready-resource/package.json":{"offset":139850,"length":812,"mode":420},"file:///node_modules/require-addon/index.js":{"offset":140662,"length":262,"mode":420},"file:///node_modules/require-addon/lib/runtime.js":{"offset":140924,"length":130,"mode":420},"file:///node_modules/require-addon/lib/runtime/bare.js":{"offset":141054,"length":45,"mode":420},"file:///node_modules/require-addon/lib/runtime/default.js":{"offset":141099,"length":260,"mode":420},"file:///node_modules/require-addon/lib/runtime/node.js":{"offset":141359,"length":1073,"mode":420},"file:///node_modules/require-addon/package.json":{"offset":142432,"length":1335,"mode":420},"file:///node_modules/same-data/index.js":{"offset":143767,"length":1126,"mode":420},"file:///node_modules/same-data/package.json":{"offset":144893,"length":629,"mode":420},"file:///node_modules/sodium-native/binding.js":{"offset":145522,"length":89,"mode":420},"file:///node_modules/sodium-native/index.js":{"offset":145611,"length":64313,"mode":420},"file:///node_modules/sodium-native/package.json":{"offset":209924,"length":1303,"mode":420},"file:///node_modules/streamx/index.js":{"offset":211227,"length":33350,"mode":420},"file:///node_modules/streamx/package.json":{"offset":244577,"length":836,"mode":420},"file:///node_modules/text-decoder/index.js":{"offset":245413,"length":1378,"mode":420},"file:///node_modules/text-decoder/lib/pass-through-decoder.js":{"offset":246791,"length":273,"mode":420},"file:///node_modules/text-decoder/lib/utf8-decoder.js":{"offset":247064,"length":2529,"mode":420},"file:///node_modules/text-decoder/package.json":{"offset":249593,"length":987,"mode":420},"file:///node_modules/unix-path-resolve/index.js":{"offset":250580,"length":1356,"mode":420},"file:///node_modules/unix-path-resolve/package.json":{"offset":251936,"length":651,"mode":420},"file:///node_modules/url-file-url/index.js":{"offset":252587,"length":1702,"mode":420},"file:///node_modules/url-file-url/package.json":{"offset":254289,"length":755,"mode":420},"file:///node_modules/which-runtime/index.js":{"offset":255044,"length":1231,"mode":420},"file:///node_modules/which-runtime/package.json":{"offset":256275,"length":602,"mode":420},"file:///package.json":{"offset":256877,"length":2917,"mode":420}}}
const pack = require('pear-pack')
const Bundle = require('bare-bundle')
const AppDrive = require('pear-appdrive')
const Localdrive = require('localdrive')
const { pathToFileURL } = require('url-file-url')

const builtins = [
  'electron', 'net', 'assert', 'console', 'events', 'fs', 'fs/promises', 'http', 'https', 'os',
  'path', 'child_process', 'repl', 'url', 'tty', 'module', 'process', 'timers', 'inspector'
]
const hosts = [process.platform + '-' + process.arch]

async function load(entry) {
  const API = global.Pear.constructor
  const drive = new AppDrive()
  await drive.ready()
  const packed = await pack(drive, { entry, hosts, builtins, prebuildPrefix: pathToFileURL(API.RTI.ui).toString() })
  await drive.close()
  const ldrive = new Localdrive(API.RTI.ui)
  for (const [prebuild, addon] of packed.prebuilds) {
    if (await ldrive.entry(prebuild) !== null) continue
    await ldrive.put(prebuild, addon) // add any new prebuilds into asset prebuilds
  }
  return Bundle.from(packed.bundle)
}

module.exports = loadfunction isBuffer(value) {
  return Buffer.isBuffer(value) || value instanceof Uint8Array
}

function isEncoding(encoding) {
  return Buffer.isEncoding(encoding)
}

function alloc(size, fill, encoding) {
  return Buffer.alloc(size, fill, encoding)
}

function allocUnsafe(size) {
  return Buffer.allocUnsafe(size)
}

function allocUnsafeSlow(size) {
  return Buffer.allocUnsafeSlow(size)
}

function byteLength(string, encoding) {
  return Buffer.byteLength(string, encoding)
}

function compare(a, b) {
  return Buffer.compare(a, b)
}

function concat(buffers, totalLength) {
  return Buffer.concat(buffers, totalLength)
}

function copy(source, target, targetStart, start, end) {
  return toBuffer(source).copy(target, targetStart, start, end)
}

function equals(a, b) {
  return toBuffer(a).equals(b)
}

function fill(buffer, value, offset, end, encoding) {
  return toBuffer(buffer).fill(value, offset, end, encoding)
}

function from(value, encodingOrOffset, length) {
  return Buffer.from(value, encodingOrOffset, length)
}

function includes(buffer, value, byteOffset, encoding) {
  return toBuffer(buffer).includes(value, byteOffset, encoding)
}

function indexOf(buffer, value, byfeOffset, encoding) {
  return toBuffer(buffer).indexOf(value, byfeOffset, encoding)
}

function lastIndexOf(buffer, value, byteOffset, encoding) {
  return toBuffer(buffer).lastIndexOf(value, byteOffset, encoding)
}

function swap16(buffer) {
  return toBuffer(buffer).swap16()
}

function swap32(buffer) {
  return toBuffer(buffer).swap32()
}

function swap64(buffer) {
  return toBuffer(buffer).swap64()
}

function toBuffer(buffer) {
  if (Buffer.isBuffer(buffer)) return buffer
  return Buffer.from(buffer.buffer, buffer.byteOffset, buffer.byteLength)
}

function toString(buffer, encoding, start, end) {
  return toBuffer(buffer).toString(encoding, start, end)
}

function write(buffer, string, offset, length, encoding) {
  return toBuffer(buffer).write(string, offset, length, encoding)
}

function readDoubleBE(buffer, offset) {
  return toBuffer(buffer).readDoubleBE(offset)
}

function readDoubleLE(buffer, offset) {
  return toBuffer(buffer).readDoubleLE(offset)
}

function readFloatBE(buffer, offset) {
  return toBuffer(buffer).readFloatBE(offset)
}

function readFloatLE(buffer, offset) {
  return toBuffer(buffer).readFloatLE(offset)
}

function readInt32BE(buffer, offset) {
  return toBuffer(buffer).readInt32BE(offset)
}

function readInt32LE(buffer, offset) {
  return toBuffer(buffer).readInt32LE(offset)
}

function readUInt32BE(buffer, offset) {
  return toBuffer(buffer).readUInt32BE(offset)
}

function readUInt32LE(buffer, offset) {
  return toBuffer(buffer).readUInt32LE(offset)
}

function writeDoubleBE(buffer, value, offset) {
  return toBuffer(buffer).writeDoubleBE(value, offset)
}

function writeDoubleLE(buffer, value, offset) {
  return toBuffer(buffer).writeDoubleLE(value, offset)
}

function writeFloatBE(buffer, value, offset) {
  return toBuffer(buffer).writeFloatBE(value, offset)
}

function writeFloatLE(buffer, value, offset) {
  return toBuffer(buffer).writeFloatLE(value, offset)
}

function writeInt32BE(buffer, value, offset) {
  return toBuffer(buffer).writeInt32BE(value, offset)
}

function writeInt32LE(buffer, value, offset) {
  return toBuffer(buffer).writeInt32LE(value, offset)
}

function writeUInt32BE(buffer, value, offset) {
  return toBuffer(buffer).writeUInt32BE(value, offset)
}

function writeUInt32LE(buffer, value, offset) {
  return toBuffer(buffer).writeUInt32LE(value, offset)
}

module.exports = {
  isBuffer,
  isEncoding,
  alloc,
  allocUnsafe,
  allocUnsafeSlow,
  byteLength,
  compare,
  concat,
  copy,
  equals,
  fill,
  from,
  includes,
  indexOf,
  lastIndexOf,
  swap16,
  swap32,
  swap64,
  toBuffer,
  toString,
  write,
  readDoubleBE,
  readDoubleLE,
  readFloatBE,
  readFloatLE,
  readInt32BE,
  readInt32LE,
  readUInt32BE,
  readUInt32LE,
  writeDoubleBE,
  writeDoubleLE,
  writeFloatBE,
  writeFloatLE,
  writeInt32BE,
  writeInt32LE,
  writeUInt32BE,
  writeUInt32LE
}
{
  "name": "b4a",
  "version": "1.7.3",
  "description": "Bridging the gap between buffers and typed arrays",
  "exports": {
    "./package": "./package.json",
    ".": {
      "react-native": "./react-native.js",
      "browser": "./browser.js",
      "default": "./index.js"
    }
  },
  "files": [
    "browser.js",
    "index.js",
    "react-native.js",
    "lib"
  ],
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.mjs",
    "test:node": "node test.mjs",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/b4a.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/b4a/issues"
  },
  "homepage": "https://github.com/holepunchto/b4a#readme",
  "devDependencies": {
    "brittle": "^3.5.2",
    "nanobench": "^3.0.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^1.0.0"
  },
  "peerDependencies": {
    "react-native-b4a": "*"
  },
  "peerDependenciesMeta": {
    "react-native-b4a": {
      "optional": true
    }
  }
}
const resolve = require('bare-module-resolve')
const { Version } = require('bare-semver')
const errors = require('./lib/errors')

module.exports = exports = function resolve(
  specifier,
  parentURL,
  opts,
  readPackage
) {
  if (typeof opts === 'function') {
    readPackage = opts
    opts = {}
  } else if (typeof readPackage !== 'function') {
    readPackage = defaultReadPackage
  }

  return {
    *[Symbol.iterator]() {
      const generator = exports.addon(specifier, parentURL, opts)

      let next = generator.next()

      while (next.done !== true) {
        const value = next.value

        if (value.package) {
          next = generator.next(readPackage(value.package))
        } else {
          next = generator.next(yield value.resolution)
        }
      }

      return next.value
    },

    async *[Symbol.asyncIterator]() {
      const generator = exports.addon(specifier, parentURL, opts)

      let next = generator.next()

      while (next.done !== true) {
        const value = next.value

        if (value.package) {
          next = generator.next(await readPackage(value.package))
        } else {
          next = generator.next(yield value.resolution)
        }
      }

      return next.value
    }
  }
}

function defaultReadPackage() {
  return null
}

const { UNRESOLVED, YIELDED, RESOLVED } = resolve.constants

exports.constants = {
  UNRESOLVED,
  YIELDED,
  RESOLVED
}

exports.addon = function* (specifier, parentURL, opts = {}) {
  const { resolutions = null } = opts

  if (exports.startsWithWindowsDriveLetter(specifier)) {
    specifier = '/' + specifier
  }

  let status

  if (resolutions) {
    status = yield* resolve.preresolved(specifier, resolutions, parentURL, opts)

    if (status) return status
  }

  status = yield* exports.url(specifier, parentURL, opts)

  if (status) return status

  let version = null

  const i = specifier.lastIndexOf('@')

  if (i > 0) {
    version = specifier.substring(i + 1)

    try {
      Version.parse(version)

      specifier = specifier.substring(0, i)
    } catch {
      version = null
    }
  }

  if (
    specifier === '.' ||
    specifier === '..' ||
    specifier[0] === '/' ||
    specifier[0] === '\\' ||
    specifier.startsWith('./') ||
    specifier.startsWith('.\\') ||
    specifier.startsWith('../') ||
    specifier.startsWith('..\\')
  ) {
    return yield* exports.directory(specifier, version, parentURL, opts)
  }

  return yield* exports.package(specifier, version, parentURL, opts)
}

exports.url = function* (url, parentURL, opts = {}) {
  let resolution
  try {
    resolution = new URL(url)
  } catch {
    return UNRESOLVED
  }

  const resolved = yield { resolution }

  return resolved ? RESOLVED : YIELDED
}

exports.package = function* (
  packageSpecifier,
  packageVersion,
  parentURL,
  opts = {}
) {
  if (packageSpecifier === '') {
    throw errors.INVALID_ADDON_SPECIFIER(
      `Addon specifier '${packageSpecifier}' is not a valid package name`
    )
  }

  let packageName

  if (packageSpecifier[0] !== '@') {
    packageName = packageSpecifier.split('/', 1).join()
  } else {
    if (!packageSpecifier.includes('/')) {
      throw errors.INVALID_ADDON_SPECIFIER(
        `Addon specifier '${packageSpecifier}' is not a valid package name`
      )
    }

    packageName = packageSpecifier.split('/', 2).join('/')
  }

  if (
    packageName[0] === '.' ||
    packageName.includes('\\') ||
    packageName.includes('%')
  ) {
    throw errors.INVALID_ADDON_SPECIFIER(
      `Addon specifier '${packageSpecifier}' is not a valid package name`
    )
  }

  const packageSubpath = '.' + packageSpecifier.substring(packageName.length)

  const status = yield* exports.packageSelf(
    packageName,
    packageSubpath,
    packageVersion,
    parentURL,
    opts
  )

  if (status) return status

  parentURL = new URL(parentURL.href)

  do {
    const packageURL = new URL('node_modules/' + packageName + '/', parentURL)

    parentURL.pathname = parentURL.pathname.substring(
      0,
      parentURL.pathname.lastIndexOf('/')
    )

    const info = yield { package: new URL('package.json', packageURL) }

    if (info) {
      return yield* exports.directory(
        packageSubpath,
        packageVersion,
        packageURL,
        opts
      )
    }
  } while (parentURL.pathname !== '' && parentURL.pathname !== '/')

  return UNRESOLVED
}

exports.packageSelf = function* (
  packageName,
  packageSubpath,
  packageVersion,
  parentURL,
  opts = {}
) {
  for (const packageURL of resolve.lookupPackageScope(parentURL, opts)) {
    const info = yield { package: packageURL }

    if (info) {
      if (info.name === packageName) {
        return yield* exports.directory(
          packageSubpath,
          packageVersion,
          packageURL,
          opts
        )
      }

      break
    }
  }

  return UNRESOLVED
}

exports.lookupPrebuildsScope = function* lookupPrebuildsScope(url, opts = {}) {
  const { resolutions = null } = opts

  if (resolutions) {
    for (const { resolution } of resolve.preresolved(
      '#prebuilds',
      resolutions,
      url,
      opts
    )) {
      if (resolution) return yield resolution
    }
  }

  const scopeURL = new URL(url.href)

  do {
    yield new URL('prebuilds/', scopeURL)

    scopeURL.pathname = scopeURL.pathname.substring(
      0,
      scopeURL.pathname.lastIndexOf('/')
    )

    if (
      scopeURL.pathname.length === 3 &&
      exports.isWindowsDriveLetter(scopeURL.pathname.substring(1))
    ) {
      break
    }
  } while (scopeURL.pathname !== '' && scopeURL.pathname !== '/')
}

exports.file = function* (filename, parentURL, opts = {}) {
  if (parentURL.protocol === 'file:' && /%2f|%5c/i.test(filename)) {
    throw errors.INVALID_ADDON_SPECIFIER(
      `Addon specifier '${filename}' is invalid`
    )
  }

  const { extensions = [] } = opts

  let status = UNRESOLVED

  for (const ext of extensions) {
    if (yield { resolution: new URL(filename + ext, parentURL) }) {
      return RESOLVED
    }

    status = YIELDED
  }

  return status
}

exports.directory = function* (dirname, version, parentURL, opts = {}) {
  const {
    resolutions = null,
    host = null, // Shorthand for single host resolution
    hosts = host !== null ? [host] : [],
    builtins = [],
    matchedConditions = []
  } = opts

  let directoryURL

  if (
    dirname[dirname.length - 1] === '/' ||
    dirname[dirname.length - 1] === '\\'
  ) {
    directoryURL = new URL(dirname, parentURL)
  } else {
    directoryURL = new URL(dirname + '/', parentURL)
  }

  // Internal preresolution path, do not depend on this! It will be removed without
  // warning.
  if (resolutions) {
    const status = yield* resolve.preresolved(
      'bare:addon',
      resolutions,
      directoryURL,
      opts
    )

    if (status) return status
  }

  const unversioned = version === null

  let name = null

  const info = yield { package: new URL('package.json', directoryURL) }

  if (info) {
    if (typeof info.name === 'string' && info.name !== '') {
      if (info.name.includes('__')) {
        throw errors.INVALID_PACKAGE_NAME(
          `Package name '${info.name}' is invalid`
        )
      }

      name = info.name.replace(/\//g, '__').replace(/^@/, '')
    } else {
      return UNRESOLVED
    }

    if (typeof info.version === 'string' && info.version !== '') {
      if (version !== null && info.version !== version) return UNRESOLVED

      version = info.version
    }
  } else {
    return UNRESOLVED
  }

  let status

  status = yield* resolve.builtinTarget(name, version, builtins, opts)

  if (status) return status

  for (const prebuildsURL of exports.lookupPrebuildsScope(directoryURL, opts)) {
    status = UNRESOLVED

    for (const host of hosts) {
      const conditions = host.split('-')

      matchedConditions.push(...conditions)

      if (version !== null) {
        status |= yield* exports.file(
          host + '/' + name + '@' + version,
          prebuildsURL,
          opts
        )
      }

      if (unversioned) {
        status |= yield* exports.file(host + '/' + name, prebuildsURL, opts)
      }

      for (const _ of conditions) matchedConditions.pop()
    }

    if (status === RESOLVED) return status
  }

  return yield* exports.linked(name, version, opts)
}

exports.linked = function* (name, version = null, opts = {}) {
  const {
    linked = true,
    host = null, // Shorthand for single host resolution
    hosts = host !== null ? [host] : [],
    matchedConditions = []
  } = opts

  if (linked === false || hosts.length === 0) return UNRESOLVED

  let status = UNRESOLVED

  for (const host of hosts) {
    const [platform = null] = host.split('-', 1)

    if (platform === null) continue

    matchedConditions.push(platform)

    status |= yield* platformArtefact(name, version, platform, opts)

    matchedConditions.pop()
  }

  return status
}

function* platformArtefact(name, version = null, platform, opts = {}) {
  const { linkedProtocol = 'linked:' } = opts

  if (platform === 'darwin' || platform === 'ios') {
    if (version !== null) {
      if (
        yield {
          resolution: new URL(
            `${linkedProtocol}${name}.${version}.framework/${name}.${version}`
          )
        }
      ) {
        return RESOLVED
      }

      if (platform === 'darwin') {
        if (
          yield {
            resolution: new URL(`${linkedProtocol}lib${name}.${version}.dylib`)
          }
        ) {
          return RESOLVED
        }
      }
    }

    if (
      yield {
        resolution: new URL(`${linkedProtocol}${name}.framework/${name}`)
      }
    ) {
      return RESOLVED
    }

    if (platform === 'darwin') {
      if (
        yield {
          resolution: new URL(`${linkedProtocol}lib${name}.dylib`)
        }
      ) {
        return RESOLVED
      }
    }

    return YIELDED
  }

  if (platform === 'linux' || platform === 'android') {
    if (version !== null) {
      if (
        yield {
          resolution: new URL(`${linkedProtocol}lib${name}.${version}.so`)
        }
      ) {
        return RESOLVED
      }
    }

    if (
      yield {
        resolution: new URL(`${linkedProtocol}lib${name}.so`)
      }
    ) {
      return RESOLVED
    }

    return YIELDED
  }

  if (platform === 'win32') {
    if (version !== null) {
      if (
        yield {
          resolution: new URL(`${linkedProtocol}${name}-${version}.dll`)
        }
      ) {
        return RESOLVED
      }
    }

    if (
      yield {
        resolution: new URL(`${linkedProtocol}${name}.dll`)
      }
    ) {
      return RESOLVED
    }
  }

  return UNRESOLVED
}

exports.isWindowsDriveLetter = resolve.isWindowsDriveLetter

exports.startsWithWindowsDriveLetter = resolve.startsWithWindowsDriveLetter
module.exports = class AddonResolveError extends Error {
  constructor(msg, code, fn = AddonResolveError) {
    super(`${code}: ${msg}`)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name() {
    return 'AddonResolveError'
  }

  static INVALID_ADDON_SPECIFIER(msg) {
    return new AddonResolveError(
      msg,
      'INVALID_ADDON_SPECIFIER',
      AddonResolveError.INVALID_ADDON_SPECIFIER
    )
  }

  static INVALID_PACKAGE_NAME(msg) {
    return new AddonResolveError(
      msg,
      'INVALID_PACKAGE_NAME',
      AddonResolveError.INVALID_PACKAGE_NAME
    )
  }
}
{
  "name": "bare-addon-resolve",
  "version": "1.9.4",
  "description": "Low-level addon resolution algorithm for Bare",
  "exports": {
    "./package": "./package.json",
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./errors": {
      "types": "./lib/errors.d.ts",
      "default": "./lib/errors.js"
    }
  },
  "files": [
    "index.js",
    "index.d.ts",
    "lib"
  ],
  "scripts": {
    "test": "prettier . --check && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-addon-resolve.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-addon-resolve/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-addon-resolve#readme",
  "dependencies": {
    "bare-module-resolve": "^1.10.0",
    "bare-semver": "^1.0.0"
  },
  "devDependencies": {
    "bare-url": "^2.1.3",
    "brittle": "^3.2.1",
    "prettier": "^3.3.3",
    "prettier-config-standard": "^7.0.0"
  },
  "peerDependencies": {
    "bare-url": "*"
  },
  "peerDependenciesMeta": {
    "bare-url": {
      "optional": true
    }
  }
}
const errors = require('./lib/errors')

const kind = Symbol.for('bare.bundle.kind')

class MemoryFile {
  constructor(data, opts = {}) {
    const { executable = false, mode = executable ? 0o755 : 0o644 } = opts

    this._data = typeof data === 'string' ? Buffer.from(data) : data
    this._mode = mode
  }

  size() {
    return this._data.byteLength
  }

  mode() {
    return this._mode
  }

  read() {
    return this._data
  }

  inspect() {
    return {
      __proto__: { constructor: MemoryFile },

      data: this._data,
      mode: this._mode.toString(8)
    }
  }

  [Symbol.for('bare.inspect')]() {
    return this.inspect()
  }

  [Symbol.for('nodejs.util.inspect.custom')]() {
    return this.inspect()
  }
}

module.exports = exports = class Bundle {
  static get [kind]() {
    return 0 // Compatibility version
  }

  static get version() {
    return 0
  }

  constructor(opts = {}) {
    const { File = MemoryFile } = opts

    this._File = File
    this._id = null
    this._main = null
    this._imports = {}
    this._resolutions = {}
    this._addons = []
    this._assets = []
    this._files = new Map()
  }

  get [kind]() {
    return Bundle[kind]
  }

  get version() {
    return Bundle.version
  }

  get id() {
    return this._id
  }

  set id(value) {
    if (typeof value !== 'string' && value !== null) {
      throw new TypeError(
        `ID must be a string or null. Received type ${typeof value} (${value})`
      )
    }

    this._id = value
  }

  get main() {
    return this._main
  }

  set main(value) {
    if (typeof value !== 'string' && value !== null) {
      throw new TypeError(
        `Main must be a string or null. Received type ${typeof value} (${value})`
      )
    }

    this._main = value
  }

  get imports() {
    return this._imports
  }

  set imports(value) {
    this._imports = cloneImportsMap(value)
  }

  get resolutions() {
    return this._resolutions
  }

  set resolutions(value) {
    this._resolutions = cloneResolutionsMap(value)
  }

  get addons() {
    return this._addons
  }

  set addons(value) {
    this._addons = cloneFilesList(value, 'Addons')
  }

  get assets() {
    return this._assets
  }

  set assets(value) {
    this._assets = cloneFilesList(value, 'Assets')
  }

  get files() {
    return Object.fromEntries(this._files.entries())
  }

  *[Symbol.iterator]() {
    for (const [key, file] of this._files) {
      yield [key, file.read(), file.mode()]
    }
  }

  empty() {
    return this._files.size === 0
  }

  keys() {
    return this._files.keys()
  }

  exists(key) {
    return this._files.has(key)
  }

  size(key) {
    const file = this._files.get(key) || null
    if (file === null) return 0
    return file.size()
  }

  mode(key) {
    const file = this._files.get(key) || null
    if (file === null) return 0
    return file.mode()
  }

  read(key) {
    const file = this._files.get(key) || null
    if (file === null) return null
    return file.read()
  }

  write(key, data, opts = {}) {
    if (typeof key !== 'string') {
      throw new TypeError(
        `File path must be a string. Received type ${typeof key} (${key})`
      )
    }

    const {
      main = false,
      alias = null,
      imports = null,
      addon = false,
      asset = false
    } = opts

    this._files.set(key, new MemoryFile(data, opts))

    if (main) this._main = key
    if (alias) this._imports[alias] = key
    if (imports) this._resolutions[key] = cloneImportsMap(imports)
    if (addon) this._addons.push(key)
    if (asset) this._assets.push(key)

    return this
  }

  mount(root, opts = {}) {
    const bundle = new Bundle()

    // Go through the private API properties as we're operating on already
    // validated values.

    bundle._File = this._File
    bundle._id = this._id

    if (this._main) bundle._main = mountSpecifier(this._main, root)

    bundle._imports = transformImportsMap(
      this._imports,
      root,
      null,
      opts,
      mountSpecifier
    )
    bundle._resolutions = transformResolutionsMap(
      this._resolutions,
      root,
      opts,
      mountSpecifier
    )

    for (const [key, file] of this._files) {
      bundle._files.set(mountSpecifier(key, root), file)
    }

    bundle._addons = transformFilesList(this._addons, root, mountSpecifier)
    bundle._assets = transformFilesList(this._assets, root, mountSpecifier)

    return bundle
  }

  unmount(root, opts = {}) {
    const bundle = new Bundle()

    // Go through the private API properties as we're operating on already
    // validated values.

    bundle._File = this._File
    bundle._id = this._id

    if (this._main) bundle._main = unmountSpecifier(this._main, root)

    bundle._imports = transformImportsMap(
      this._imports,
      root,
      null,
      opts,
      unmountSpecifier
    )
    bundle._resolutions = transformResolutionsMap(
      this._resolutions,
      root,
      opts,
      unmountSpecifier
    )

    for (const [key, file] of this._files) {
      bundle._files.set(unmountSpecifier(key, root), file)
    }

    bundle._addons = transformFilesList(this._addons, root, unmountSpecifier)
    bundle._assets = transformFilesList(this._assets, root, unmountSpecifier)

    return bundle
  }

  toBuffer(opts = {}) {
    const { indent = 0 } = opts

    const header = {
      version: Bundle.version,
      id: this._id,
      main: this._main,
      imports: cloneImportsMap(this._imports),
      resolutions: cloneResolutionsMap(this._resolutions),
      addons: cloneFilesList(this._addons, 'Addons'),
      assets: cloneFilesList(this._assets, 'Assets'),
      files: {}
    }

    const keys = [...this._files.keys()].sort()

    let offset = 0

    for (const key of keys) {
      const length = this.size(key)

      header.files[key] = { offset, length, mode: this.mode(key) }
      offset += length
    }

    const json = Buffer.from(`\n${JSON.stringify(header, null, indent)}\n`)

    const len = Buffer.from(json.byteLength.toString(10))

    const buffer = Buffer.alloc(len.byteLength + json.byteLength + offset)

    offset = 0

    buffer.set(len, offset)
    offset += len.byteLength

    buffer.set(json, offset)
    offset += json.byteLength

    for (const key of keys) {
      buffer.set(this.read(key), offset)
      offset += this.size(key)
    }

    return buffer
  }

  inspect() {
    return {
      __proto__: { constructor: Bundle },

      version: this.version,
      id: this.id,
      main: this.main,
      imports: this.imports,
      resolutions: this.resolutions,
      addons: this.addons,
      assets: this.assets,
      files: this.files
    }
  }

  [Symbol.for('bare.inspect')]() {
    return this.inspect()
  }

  [Symbol.for('nodejs.util.inspect.custom')]() {
    return this.inspect()
  }
}

const Bundle = exports

exports.errors = errors

exports.isBundle = function isBundle(value) {
  if (value instanceof Bundle) return true

  return (
    typeof value === 'object' && value !== null && value[kind] === Bundle[kind]
  )
}

exports.from = function from(value) {
  // from(string)
  if (typeof value === 'string') return fromString(value)

  // from(buffer)
  if (Buffer.isBuffer(value)) return fromBuffer(value)

  // from(bundle)
  return value
}

function fromString(string) {
  return fromBuffer(Buffer.from(string))
}

function fromBuffer(buffer) {
  if (buffer[0] === 0x23 /* # */ && buffer[1] === 0x21 /* ! */) {
    let end = 2

    while (buffer[end] !== 0xa /* \n */) end++

    buffer = buffer.subarray(end + 1)
  }

  let end = 0

  while (isDecimal(buffer[end])) end++

  const len = parseInt(buffer.toString('utf8', 0, end), 10)

  let header
  try {
    header = JSON.parse(buffer.toString('utf8', end, end + len))
  } catch (err) {
    throw errors.INVALID_BUNDLE_HEADER('Invalid bundle header', err)
  }

  const bundle = new Bundle()

  // Go through the public API setters to ensure that the header fields are
  // validated.

  if (header.id) bundle.id = header.id
  if (header.main) bundle.main = header.main
  if (header.imports) bundle.imports = header.imports
  if (header.resolutions) bundle.resolutions = header.resolutions
  if (header.addons) bundle.addons = header.addons
  if (header.assets) bundle.assets = header.assets

  let offset = end + len

  for (const [file, info] of Object.entries(header.files)) {
    bundle.write(file, buffer.subarray(offset, offset + info.length), {
      mode: info.mode || 0o644
    })

    offset += info.length
  }

  return bundle
}

function isDecimal(c) {
  return c >= 0x30 && c <= 0x39
}

function compareKeys([a], [b]) {
  return a > b ? 1 : a < b ? -1 : 0
}

function cloneImportsMap(value) {
  if (typeof value === 'object' && value !== null) {
    const imports = {}

    for (const entry of Object.entries(value).sort(compareKeys)) {
      imports[entry[0]] = cloneImportsMapEntry(entry[1])
    }

    return imports
  }

  throw new TypeError(
    `Imports map must be an object. Received type ${typeof value} (${value})`
  )
}

function cloneImportsMapEntry(value) {
  if (typeof value === 'string') return value

  if (typeof value === 'object' && value !== null) {
    const imports = {}

    for (const entry of Object.entries(value)) {
      imports[entry[0]] = cloneImportsMapEntry(entry[1])
    }

    return imports
  }

  throw new TypeError(
    `Imports map entry must be a string or object. Received type ${typeof value} (${value})`
  )
}

function cloneResolutionsMap(value) {
  if (typeof value === 'object' && value !== null) {
    const resolutions = {}

    for (const entry of Object.entries(value).sort(compareKeys)) {
      resolutions[entry[0]] = cloneImportsMap(entry[1])
    }

    return resolutions
  }

  throw new TypeError(
    `Resolutions map must be an object. Received type ${typeof value} (${value})`
  )
}

function cloneFilesList(value, name) {
  if (Array.isArray(value)) {
    const files = []

    for (const entry of value) {
      if (typeof entry !== 'string') {
        throw new TypeError(
          `${name} entry must be a string. Received type ${typeof entry} (${entry})`
        )
      }

      files.push(entry)
    }

    return files.sort()
  }

  throw new TypeError(
    `${name} list must be an array. Received type ${typeof value} (${value})`
  )
}

function transformImportsMap(value, root, conditionalRoot, opts, fn) {
  const { conditions = {} } = opts

  const imports = {}

  for (const entry of Object.entries(value)) {
    const condition = entry[0]

    imports[condition] = transformImportsMapEntry(
      entry[1],
      root,
      conditionalRoot || conditions[condition],
      opts,
      fn
    )
  }

  return imports
}

function transformImportsMapEntry(value, root, conditionalRoot, opts, fn) {
  const { conditions = {} } = opts

  if (typeof value === 'string') {
    return fn(value, conditionalRoot || conditions.default || root)
  }

  return transformImportsMap(value, root, conditionalRoot, opts, fn)
}

function transformResolutionsMap(value, root, opts, fn) {
  const resolutions = {}

  for (const entry of Object.entries(value)) {
    resolutions[fn(entry[0], root)] = transformImportsMap(
      entry[1],
      root,
      null,
      opts,
      fn
    )
  }

  return resolutions
}

function transformFilesList(value, root, fn) {
  const files = []

  for (const entry of value) {
    files.push(fn(entry, root))
  }

  return files
}

function mountSpecifier(specifier, root) {
  if (startsWithWindowsDriveLetter(specifier)) {
    specifier = '/' + specifier
  }

  if (specifier[0] === '/' || specifier[0] === '\\') {
    specifier = '.' + specifier
  }

  if (specifier.startsWith('./') || specifier.startsWith('.\\')) {
    return new URL(specifier, root).href
  }

  return specifier
}

function unmountSpecifier(specifier, root) {
  specifier = new URL(specifier)

  if (typeof root === 'string') root = new URL(root)

  if (
    specifier.protocol !== root.protocol ||
    specifier.host !== root.host ||
    specifier.port !== root.port
  ) {
    return specifier.href
  }

  const specifierPath = splitPath(specifier.pathname)
  const rootPath = splitPath(root.pathname)

  while (specifierPath.length > 0 && rootPath[0] === specifierPath[0]) {
    specifierPath.shift()
    rootPath.shift()
  }

  rootPath.fill('..')

  return '/' + rootPath.concat(specifierPath).join('/')
}

function splitPath(path) {
  const parts = path.split('/')

  if (!parts[0]) parts.shift()
  if (!parts[parts.length - 1]) parts.pop()

  return parts
}

// https://infra.spec.whatwg.org/#ascii-upper-alpha
function isASCIIUpperAlpha(c) {
  return c >= 0x41 && c <= 0x5a
}

// https://infra.spec.whatwg.org/#ascii-lower-alpha
function isASCIILowerAlpha(c) {
  return c >= 0x61 && c <= 0x7a
}

// https://infra.spec.whatwg.org/#ascii-alpha
function isASCIIAlpha(c) {
  return isASCIIUpperAlpha(c) || isASCIILowerAlpha(c)
}

// https://url.spec.whatwg.org/#windows-drive-letter
function isWindowsDriveLetter(input) {
  return (
    input.length >= 2 &&
    isASCIIAlpha(input.charCodeAt(0)) &&
    (input.charCodeAt(1) === 0x3a || input.charCodeAt(1) === 0x7c)
  )
}

// https://url.spec.whatwg.org/#start-with-a-windows-drive-letter
function startsWithWindowsDriveLetter(input) {
  return (
    input.length >= 2 &&
    isWindowsDriveLetter(input) &&
    (input.length === 2 ||
      input.charCodeAt(2) === 0x2f ||
      input.charCodeAt(2) === 0x5c ||
      input.charCodeAt(2) === 0x3f ||
      input.charCodeAt(2) === 0x23)
  )
}
module.exports = class BundleError extends Error {
  constructor(msg, fn = BundleError, opts = {}) {
    const { cause, code = fn.name } = opts

    super(`${code}: ${msg}`, { cause })
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name() {
    return 'BundleError'
  }

  static INVALID_BUNDLE_HEADER(msg, cause) {
    return new BundleError(msg, BundleError.INVALID_BUNDLE_HEADER, { cause })
  }
}
{
  "name": "bare-bundle",
  "version": "1.9.0",
  "description": "Application bundle format for JavaScript",
  "exports": {
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./package": "./package.json"
  },
  "files": [
    "index.js",
    "index.d.ts",
    "lib"
  ],
  "scripts": {
    "test": "prettier . --check && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-bundle.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-bundle/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-bundle#readme",
  "devDependencies": {
    "bare-buffer": "^3.0.2",
    "bare-url": "^2.1.3",
    "brittle": "^3.1.1",
    "prettier": "^3.4.2",
    "prettier-config-standard": "^7.0.0"
  },
  "peerDependencies": {
    "bare-buffer": "*",
    "bare-url": "*"
  },
  "peerDependenciesMeta": {
    "bare-buffer": {
      "optional": true
    },
    "bare-url": {
      "optional": true
    }
  }
}
require.addon = require('require-addon')

module.exports = require.addon('..', __filename)
const binding = require('#binding')

module.exports = exports = function lex(input, encoding, opts = {}) {
  if (typeof encoding === 'object' && encoding !== null) {
    opts = encoding
    encoding = null
  }

  if (typeof input !== 'string' && !ArrayBuffer.isView(input)) {
    throw new TypeError(
      `Input must be a string or buffer. Received type ${typeof input}`
    )
  }

  return binding.lex(
    typeof input === 'string' ? Buffer.from(input, encoding) : input
  )
}

exports.constants = {
  /**
   * CommonJS `require()`.
   */
  REQUIRE: binding.REQUIRE,

  /**
   * ES module `import`.
   */
  IMPORT: binding.IMPORT,

  /**
   * ES module `import()` if `IMPORT` is set.
   */
  DYNAMIC: binding.DYNAMIC,

  /**
   * CommonJS `require.addon()` if `REQUIRE` is set, or ES module `import.meta.addon()` if `IMPORT` is set.
   */
  ADDON: binding.ADDON,

  /**
   * CommonJS `require.asset()` if `REQUIRE` is set, or ES module `import.meta.asset()` if `IMPORT` is set.
   */
  ASSET: binding.ASSET,

  /**
   * CommonJS `require.resolve()` or `require.addon.resolve()` if `REQUIRE` and optionally `ADDON` are set, or ES module
   * `import.meta.resolve()` or `import.meta.addon.resolve()` if `IMPORT` and optionally `ADDON` are set.
   */
  RESOLVE: binding.RESOLVE,

  /**
   * Re-export of a CommonJS `require()` if `REQUIRE` is set.
   */
  REEXPORT: binding.REEXPORT
}
{
  "name": "bare-module-lexer",
  "version": "1.4.6",
  "description": "Heuristic lexer for detecting imports and exports in JavaScript modules",
  "exports": {
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./package": "./package.json"
  },
  "imports": {
    "#binding": {
      "bare": "./binding/bare.js",
      "node": "./binding/node.js"
    }
  },
  "files": [
    "index.js",
    "index.d.ts",
    "lex.h",
    "binding.c",
    "binding/bare.js",
    "binding/node.js",
    "CMakeLists.txt",
    "prebuilds"
  ],
  "addon": true,
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-module-lexer.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-module-lexer/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-module-lexer#readme",
  "dependencies": {
    "require-addon": "^1.0.2"
  },
  "devDependencies": {
    "bare-buffer": "^3.0.0",
    "bare-compat-napi": "^1.0.0",
    "brittle": "^3.1.1",
    "cmake-bare": "^1.1.7",
    "cmake-napi": "^1.0.5",
    "cmake-npm": "^1.0.2",
    "prettier": "^3.4.1",
    "prettier-config-standard": "^7.0.0"
  },
  "peerDependencies": {
    "bare-buffer": "*"
  },
  "peerDependenciesMeta": {
    "bare-buffer": {
      "optional": true
    }
  }
}
const { satisfies } = require('bare-semver')
const errors = require('./lib/errors')

module.exports = exports = function resolve(
  specifier,
  parentURL,
  opts,
  readPackage
) {
  if (typeof opts === 'function') {
    readPackage = opts
    opts = {}
  } else if (typeof readPackage !== 'function') {
    readPackage = defaultReadPackage
  }

  return {
    *[Symbol.iterator]() {
      const generator = exports.module(specifier, parentURL, opts)

      let next = generator.next()

      while (next.done !== true) {
        const value = next.value

        if (value.package) {
          next = generator.next(readPackage(value.package))
        } else {
          next = generator.next(yield value.resolution)
        }
      }

      return next.value
    },

    async *[Symbol.asyncIterator]() {
      const generator = exports.module(specifier, parentURL, opts)

      let next = generator.next()

      while (next.done !== true) {
        const value = next.value

        if (value.package) {
          next = generator.next(await readPackage(value.package))
        } else {
          next = generator.next(yield value.resolution)
        }
      }

      return next.value
    }
  }
}

function defaultReadPackage() {
  return null
}

// No resolution candidate was yielded
const UNRESOLVED = 0x0
// At least 1 resolution candidate was yielded
const YIELDED = 0x1
// At least 1 resolution candidate was yielded and resolved
const RESOLVED = YIELDED | 0x2

exports.constants = {
  UNRESOLVED,
  YIELDED,
  RESOLVED
}

exports.module = function* (specifier, parentURL, opts = {}) {
  const { resolutions = null, imports = null } = opts
  // console.trace('HERE', specifier, parentURL, opts)
  if (exports.startsWithWindowsDriveLetter(specifier)) {
    specifier = '/' + specifier
  }

  let status

  if (resolutions) {
    status = yield* exports.preresolved(specifier, resolutions, parentURL, opts)

    if (status) return status
  }

  status = yield* exports.url(specifier, parentURL, opts)

  if (status) return status

  status = yield* exports.packageImports(specifier, parentURL, opts)

  if (status) return status

  if (
    specifier === '.' ||
    specifier === '..' ||
    specifier[0] === '/' ||
    specifier[0] === '\\' ||
    specifier.startsWith('./') ||
    specifier.startsWith('.\\') ||
    specifier.startsWith('../') ||
    specifier.startsWith('..\\')
  ) {

    if (imports) {
      status = yield* exports.packageImportsExports(
        specifier,
        imports,
        parentURL,
        true,
        opts
      )

      if (status) return status
    }

    status = yield* exports.deferred(specifier, opts)

    if (status) return status

    status = yield* exports.file(specifier, parentURL, false, opts)

    if (status === RESOLVED) return status

    return yield* exports.directory(specifier, parentURL, opts)
  }

  return yield* exports.package(specifier, parentURL, opts)
}

exports.url = function* (url, parentURL, opts = {}) {
  const { imports = null, deferredProtocol = 'deferred:' } = opts

  let resolution
  try {
    resolution = new URL(url)
  } catch {
    return UNRESOLVED
  }

  if (imports) {
    const status = yield* exports.packageImportsExports(
      resolution.href,
      imports,
      parentURL,
      true,
      opts
    )

    if (status) return status
  }

  if (resolution.protocol === deferredProtocol) {
    const specifier = resolution.pathname

    return yield* exports.module(specifier, parentURL, opts)
  }

  if (resolution.protocol === 'node:') {
    const specifier = resolution.pathname

    if (
      specifier === '.' ||
      specifier === '..' ||
      specifier[0] === '/' ||
      specifier.startsWith('./') ||
      specifier.startsWith('../')
    ) {
      throw errors.INVALID_MODULE_SPECIFIER(
        `Module specifier '${url}' is not a valid package name`
      )
    }

    return yield* exports.package(specifier, parentURL, opts)
  }

  const resolved = yield { resolution }

  return resolved ? RESOLVED : YIELDED
}

exports.preresolved = function* (specifier, resolutions, parentURL, opts = {}) {
  const imports = resolutions[parentURL.href]

  if (typeof imports === 'object' && imports !== null) {
    return yield* exports.packageImportsExports(
      specifier,
      imports,
      parentURL,
      true,
      opts
    )
  }

  return UNRESOLVED
}

exports.deferred = function* (specifier, opts = {}) {
  const { deferredProtocol = 'deferred:', defer = [] } = opts

  if (defer.includes(specifier)) {
    const resolved = yield { resolution: new URL(deferredProtocol + specifier) }

    return resolved ? RESOLVED : YIELDED
  }

  return UNRESOLVED
}

exports.package = function* (packageSpecifier, parentURL, opts = {}) {
  const { builtins = [] } = opts

  if (packageSpecifier === '') {
    throw errors.INVALID_MODULE_SPECIFIER(
      `Module specifier '${packageSpecifier}' is not a valid package name`
    )
  }

  let packageName

  if (packageSpecifier[0] !== '@') {
    packageName = packageSpecifier.split('/', 1).join()
  } else {
    if (!packageSpecifier.includes('/')) {
      throw errors.INVALID_MODULE_SPECIFIER(
        `Module specifier '${packageSpecifier}' is not a valid package name`
      )
    }

    packageName = packageSpecifier.split('/', 2).join('/')
  }

  if (
    packageName[0] === '.' ||
    packageName.includes('\\') ||
    packageName.includes('%')
  ) {
    throw errors.INVALID_MODULE_SPECIFIER(
      `Module specifier '${packageSpecifier}' is not a valid package name`
    )
  }

  let status

  status = yield* exports.builtinTarget(packageSpecifier, null, builtins, opts)

  if (status) return status

  status = yield* exports.deferred(packageSpecifier, opts)

  if (status) return status

  let packageSubpath = '.' + packageSpecifier.substring(packageName.length)

  status = yield* exports.packageSelf(
    packageName,
    packageSubpath,
    parentURL,
    opts
  )

  if (status) return status

  parentURL = new URL(parentURL.href)

  do {
    const packageURL = new URL('node_modules/' + packageName + '/', parentURL)

    parentURL.pathname = parentURL.pathname.substring(
      0,
      parentURL.pathname.lastIndexOf('/')
    )

    const info = yield { package: new URL('package.json', packageURL) }

    if (info) {
      if (info.engines) exports.validateEngines(packageURL, info.engines, opts)

      if (info.exports) {
        return yield* exports.packageExports(
          packageURL,
          packageSubpath,
          info.exports,
          opts
        )
      }

      if (packageSubpath === '.') {
        if (typeof info.main === 'string' && info.main !== '') {
          packageSubpath = info.main
        } else {
          return yield* exports.file('index', packageURL, true, opts)
        }
      }

      status = yield* exports.file(packageSubpath, packageURL, false, opts)

      if (status === RESOLVED) return status

      return yield* exports.directory(packageSubpath, packageURL, opts)
    }
  } while (parentURL.pathname !== '' && parentURL.pathname !== '/')

  return UNRESOLVED
}

exports.packageSelf = function* (
  packageName,
  packageSubpath,
  parentURL,
  opts = {}
) {
  for (const packageURL of exports.lookupPackageScope(parentURL, opts)) {
    const info = yield { package: packageURL }

    if (info) {
      if (info.name !== packageName) return false

      if (info.exports) {
        return yield* exports.packageExports(
          packageURL,
          packageSubpath,
          info.exports,
          opts
        )
      }

      if (packageSubpath === '.') {
        if (typeof info.main === 'string' && info.main !== '') {
          packageSubpath = info.main
        } else {
          return yield* exports.file('index', packageURL, true, opts)
        }
      }

      const status = yield* exports.file(
        packageSubpath,
        packageURL,
        false,
        opts
      )

      if (status === RESOLVED) return status

      return yield* exports.directory(packageSubpath, packageURL, opts)
    }
  }

  return UNRESOLVED
}

exports.packageExports = function* (
  packageURL,
  subpath,
  packageExports,
  opts = {}
) {
  if (subpath === '.') {
    let mainExport

    if (typeof packageExports === 'string' || Array.isArray(packageExports)) {
      mainExport = packageExports
    } else if (typeof packageExports === 'object' && packageExports !== null) {
      const keys = Object.keys(packageExports)

      if (keys.some((key) => key.startsWith('.'))) {
        if ('.' in packageExports) mainExport = packageExports['.']
      } else {
        mainExport = packageExports
      }
    }

    if (mainExport) {
      const status = yield* exports.packageTarget(
        packageURL,
        mainExport,
        null,
        false,
        opts
      )

      if (status) return status
    }
  } else if (typeof packageExports === 'object' && packageExports !== null) {
    const keys = Object.keys(packageExports)

    if (keys.every((key) => key.startsWith('.'))) {
      const status = yield* exports.packageImportsExports(
        subpath,
        packageExports,
        packageURL,
        false,
        opts
      )

      if (status) return status
    }
  }

  packageURL = new URL('package.json', packageURL)

  throw errors.PACKAGE_PATH_NOT_EXPORTED(
    `Package subpath '${subpath}' is not defined by "exports" in '${packageURL}'`
  )
}

exports.packageImports = function* (specifier, parentURL, opts = {}) {
  const { imports = null } = opts

  if (specifier === '#' || specifier.startsWith('#/')) {
    throw errors.INVALID_MODULE_SPECIFIER(
      `Module specifier '${specifier}' is not a valid internal imports specifier`
    )
  }

  for (const packageURL of exports.lookupPackageScope(parentURL, opts)) {
    const info = yield { package: packageURL }

    if (info) {
      if (info.imports) {
        const status = yield* exports.packageImportsExports(
          specifier,
          info.imports,
          packageURL,
          true,
          opts
        )

        if (status) return status
      }

      if (specifier.startsWith('#')) {
        throw errors.PACKAGE_IMPORT_NOT_DEFINED(
          `Package import specifier '${specifier}' is not defined by "imports" in '${packageURL}'`
        )
      }

      break
    }
  }

  if (imports) {
    const status = yield* exports.packageImportsExports(
      specifier,
      imports,
      parentURL,
      true,
      opts
    )

    if (status) return status
  }

  return UNRESOLVED
}

exports.packageImportsExports = function* (
  matchKey,
  matchObject,
  packageURL,
  isImports,
  opts = {}
) {

  if (matchKey in matchObject && !matchKey.includes('*')) {
    const target = matchObject[matchKey]
    return yield* exports.packageTarget(
      packageURL,
      target,
      null,
      isImports,
      opts
    )
  }

  const expansionKeys = Object.keys(matchObject)
    .filter((key) => key.includes('*'))
    .sort(exports.patternKeyCompare)

  for (const expansionKey of expansionKeys) {
    const patternIndex = expansionKey.indexOf('*')
    const patternBase = expansionKey.substring(0, patternIndex)

    if (matchKey.startsWith(patternBase) && matchKey !== patternBase) {
      const patternTrailer = expansionKey.substring(patternIndex + 1)

      if (
        patternTrailer === '' ||
        (matchKey.endsWith(patternTrailer) &&
          matchKey.length >= expansionKey.length)
      ) {
        const target = matchObject[expansionKey]

        const patternMatch = matchKey.substring(
          patternBase.length,
          matchKey.length - patternTrailer.length
        )

        return yield* exports.packageTarget(
          packageURL,
          target,
          patternMatch,
          isImports,
          opts
        )
      }
    }
  }

  return UNRESOLVED
}

exports.validateEngines = function validateEngines(
  packageURL,
  packageEngines,
  opts = {}
) {
  const { engines = {} } = opts

  for (const [engine, range] of Object.entries(packageEngines)) {
    if (engine in engines) {
      const version = engines[engine]

      if (!satisfies(version, range)) {
        packageURL = new URL('package.json', packageURL)

        throw errors.UNSUPPORTED_ENGINE(
          `Package not compatible with engine '${engine}' ${version}, requires range '${range}' defined by "engines" in '${packageURL}'`
        )
      }
    }
  }
}

exports.patternKeyCompare = function patternKeyCompare(keyA, keyB) {
  const patternIndexA = keyA.indexOf('*')
  const patternIndexB = keyB.indexOf('*')
  const baseLengthA = patternIndexA === -1 ? keyA.length : patternIndexA + 1
  const baseLengthB = patternIndexB === -1 ? keyB.length : patternIndexB + 1
  if (baseLengthA > baseLengthB) return -1
  if (baseLengthB > baseLengthA) return 1
  if (patternIndexA === -1) return 1
  if (patternIndexB === -1) return -1
  if (keyA.length > keyB.length) return -1
  if (keyB.length > keyA.length) return 1
  return 0
}

exports.packageTarget = function* (
  packageURL,
  target,
  patternMatch,
  isImports,
  opts = {}
) {
  const { conditions = [], matchedConditions = [] } = opts

  if (typeof target === 'string') {
    if (!target.startsWith('./') && !isImports) {
      packageURL = new URL('package.json', packageURL)

      throw errors.INVALID_PACKAGE_TARGET(
        `Invalid target '${target}' defined by "exports" in '${packageURL}'`
      )
    }

    if (patternMatch !== null) {
      target = target.replaceAll('*', patternMatch)
    }

    const status = yield* exports.url(target, packageURL, opts)

    if (status) return status

    if (
      target === '.' ||
      target === '..' ||
      target[0] === '/' ||
      target.startsWith('./') ||
      target.startsWith('../')
    ) {
      const resolved = yield { resolution: new URL(target, packageURL) }

      return resolved ? RESOLVED : YIELDED
    }

    return yield* exports.package(target, packageURL, opts)
  }

  if (Array.isArray(target)) {
    for (const targetValue of target) {
      const status = yield* exports.packageTarget(
        packageURL,
        targetValue,
        patternMatch,
        isImports,
        opts
      )

      if (status) return status
    }
  } else if (typeof target === 'object' && target !== null) {
    let status = UNRESOLVED

    for (const [condition, targetValue, subset] of exports.conditionMatches(
      target,
      conditions,
      opts
    )) {
      matchedConditions.push(condition)

      status |= yield* exports.packageTarget(
        packageURL,
        targetValue,
        patternMatch,
        isImports,
        { ...opts, conditions: subset }
      )

      matchedConditions.pop()
    }

    if (status) return status
  }

  return UNRESOLVED
}

exports.builtinTarget = function* (
  packageSpecifier,
  packageVersion,
  target,
  opts = {}
) {
  const {
    builtinProtocol = 'builtin:',
    conditions = [],
    matchedConditions = []
  } = opts

  if (typeof target === 'string') {
    const targetParts = target.split('@')

    let targetName
    let targetVersion

    if (target[0] !== '@') {
      targetName = targetParts[0]
      targetVersion = targetParts[1] || null
    } else {
      targetName = targetParts.slice(0, 2).join('@')
      targetVersion = targetParts[2] || null
    }

    if (packageSpecifier === targetName) {
      if (packageVersion === null && targetVersion === null) {
        const resolved = yield {
          resolution: new URL(builtinProtocol + packageSpecifier)
        }

        return resolved ? RESOLVED : YIELDED
      }

      let version = null

      if (packageVersion === null) {
        version = targetVersion
      } else if (targetVersion === null || packageVersion === targetVersion) {
        version = packageVersion
      }

      if (version !== null) {
        const resolved = yield {
          resolution: new URL(
            builtinProtocol + packageSpecifier + '@' + version
          )
        }

        return resolved ? RESOLVED : YIELDED
      }
    }
  } else if (Array.isArray(target)) {
    for (const targetValue of target) {
      const status = yield* exports.builtinTarget(
        packageSpecifier,
        packageVersion,
        targetValue,
        opts
      )

      if (status) return status
    }
  } else if (typeof target === 'object' && target !== null) {
    let status = UNRESOLVED

    for (const [condition, targetValue, subset] of exports.conditionMatches(
      target,
      conditions,
      opts
    )) {
      matchedConditions.push(condition)

      status |= yield* exports.builtinTarget(
        packageSpecifier,
        packageVersion,
        targetValue,
        { ...opts, conditions: subset }
      )

      matchedConditions.pop()
    }

    if (status) return status
  }

  return UNRESOLVED
}

exports.conditionMatches = function* conditionMatches(
  target,
  conditions,
  opts = {}
) {
  if (conditions.every((condition) => typeof condition === 'string')) {
    const keys = Object.keys(target)

    for (const condition of keys) {
      if (condition === 'default' || conditions.includes(condition)) {
        yield [condition, target[condition], conditions]

        return true
      }
    }

    return false
  }

  let yielded = false

  for (const subset of conditions) {
    if (yield* conditionMatches(target, subset, opts)) {
      yielded = true
    }
  }

  return yielded
}

exports.lookupPackageScope = function* lookupPackageScope(url, opts = {}) {
  const { resolutions = null } = opts

  if (resolutions) {
    for (const { resolution } of exports.preresolved(
      '#package',
      resolutions,
      url,
      opts
    )) {
      if (resolution) return yield resolution
    }

    // Internal preresolution path, do not depend on this! It will be removed without
    // warning.
    for (const { resolution } of exports.preresolved(
      'bare:package',
      resolutions,
      url,
      opts
    )) {
      if (resolution) return yield resolution
    }
  }

  const scopeURL = new URL(url.href)

  do {
    if (scopeURL.pathname.endsWith('/node_modules')) break

    yield new URL('package.json', scopeURL)

    scopeURL.pathname = scopeURL.pathname.substring(
      0,
      scopeURL.pathname.lastIndexOf('/')
    )

    if (
      scopeURL.pathname.length === 3 &&
      exports.isWindowsDriveLetter(scopeURL.pathname.substring(1))
    ) {
      break
    }
  } while (scopeURL.pathname !== '' && scopeURL.pathname !== '/')
}

exports.file = function* (filename, parentURL, isIndex, opts = {}) {
  if (
    filename === '.' ||
    filename === '..' ||
    filename[filename.length - 1] === '/' ||
    filename[filename.length - 1] === '\\'
  ) {
    return UNRESOLVED
  }

  if (parentURL.protocol === 'file:' && /%2f|%5c/i.test(filename)) {
    throw errors.INVALID_MODULE_SPECIFIER(
      `Module specifier '${filename}' is invalid`
    )
  }

  const { extensions = [] } = opts

  let status = UNRESOLVED

  if (!isIndex) {
    if (yield { resolution: new URL(filename, parentURL) }) {
      return RESOLVED
    }

    status = YIELDED
  }

  for (const ext of extensions) {
    if (yield { resolution: new URL(filename + ext, parentURL) }) {
      return RESOLVED
    }

    status = YIELDED
  }

  return status
}

exports.directory = function* (dirname, parentURL, opts = {}) {
  let directoryURL

  if (
    dirname[dirname.length - 1] === '/' ||
    dirname[dirname.length - 1] === '\\'
  ) {
    directoryURL = new URL(dirname, parentURL)
  } else {
    directoryURL = new URL(dirname + '/', parentURL)
  }

  const info = yield { package: new URL('package.json', directoryURL) }

  if (info) {
    if (info.exports) {
      return yield* exports.packageExports(
        directoryURL,
        '.',
        info.exports,
        opts
      )
    }

    if (typeof info.main === 'string' && info.main !== '') {
      const status = yield* exports.file(info.main, directoryURL, false, opts)

      if (status === RESOLVED) return status

      return yield* exports.directory(info.main, directoryURL, opts)
    }
  }

  return yield* exports.file('index', directoryURL, true, opts)
}

// https://infra.spec.whatwg.org/#ascii-upper-alpha
function isASCIIUpperAlpha(c) {
  return c >= 0x41 && c <= 0x5a
}

// https://infra.spec.whatwg.org/#ascii-lower-alpha
function isASCIILowerAlpha(c) {
  return c >= 0x61 && c <= 0x7a
}

// https://infra.spec.whatwg.org/#ascii-alpha
function isASCIIAlpha(c) {
  return isASCIIUpperAlpha(c) || isASCIILowerAlpha(c)
}

// https://url.spec.whatwg.org/#windows-drive-letter
exports.isWindowsDriveLetter = function isWindowsDriveLetter(input) {
  return (
    input.length >= 2 &&
    isASCIIAlpha(input.charCodeAt(0)) &&
    (input.charCodeAt(1) === 0x3a || input.charCodeAt(1) === 0x7c)
  )
}

// https://url.spec.whatwg.org/#start-with-a-windows-drive-letter
exports.startsWithWindowsDriveLetter = function startsWithWindowsDriveLetter(
  input
) {
  return (
    input.length >= 2 &&
    exports.isWindowsDriveLetter(input) &&
    (input.length === 2 ||
      input.charCodeAt(2) === 0x2f ||
      input.charCodeAt(2) === 0x5c ||
      input.charCodeAt(2) === 0x3f ||
      input.charCodeAt(2) === 0x23)
  )
}
module.exports = class ModuleResolveError extends Error {
  constructor(msg, code, fn = ModuleResolveError) {
    super(`${code}: ${msg}`)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name() {
    return 'ModuleResolveError'
  }

  static INVALID_MODULE_SPECIFIER(msg) {
    return new ModuleResolveError(
      msg,
      'INVALID_MODULE_SPECIFIER',
      ModuleResolveError.INVALID_MODULE_SPECIFIER
    )
  }

  static INVALID_PACKAGE_TARGET(msg) {
    return new ModuleResolveError(
      msg,
      'INVALID_PACKAGE_TARGET',
      ModuleResolveError.INVALID_PACKAGE_TARGET
    )
  }

  static PACKAGE_PATH_NOT_EXPORTED(msg) {
    return new ModuleResolveError(
      msg,
      'PACKAGE_PATH_NOT_EXPORTED',
      ModuleResolveError.PACKAGE_PATH_NOT_EXPORTED
    )
  }

  static PACKAGE_IMPORT_NOT_DEFINED(msg) {
    return new ModuleResolveError(
      msg,
      'PACKAGE_IMPORT_NOT_DEFINED',
      ModuleResolveError.PACKAGE_IMPORT_NOT_DEFINED
    )
  }

  static UNSUPPORTED_ENGINE(msg) {
    return new ModuleResolveError(
      msg,
      'UNSUPPORTED_ENGINE',
      ModuleResolveError.UNSUPPORTED_ENGINE
    )
  }
}
{
  "name": "bare-module-resolve",
  "version": "1.11.1",
  "description": "Low-level module resolution algorithm for Bare",
  "exports": {
    "./package": "./package.json",
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./errors": {
      "types": "./lib/errors.d.ts",
      "default": "./lib/errors.js"
    }
  },
  "files": [
    "index.js",
    "index.d.ts",
    "lib"
  ],
  "scripts": {
    "test": "prettier . --check && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-module-resolve.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-module-resolve/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-module-resolve#readme",
  "dependencies": {
    "bare-semver": "^1.0.0"
  },
  "devDependencies": {
    "bare-url": "^2.1.3",
    "brittle": "^3.2.1",
    "prettier": "^3.3.3",
    "prettier-config-standard": "^7.0.0"
  },
  "peerDependencies": {
    "bare-url": "*"
  },
  "peerDependenciesMeta": {
    "bare-url": {
      "optional": true
    }
  }
}
const { lookupPackageScope, conditionMatches } = require('bare-module-resolve')
const { lookupPrebuildsScope } = require('bare-addon-resolve')
const lex = require('bare-module-lexer')
const resolve = require('./lib/resolve')
const errors = require('./lib/errors')

module.exports = exports = function traverse(
  entry,
  opts,
  readModule,
  listPrefix
) {
  if (typeof opts === 'function') {
    listPrefix = readModule
    readModule = opts
    opts = {}
  }

  return {
    *[Symbol.iterator]() {
      const artifacts = { addons: [], assets: [] }

      const queue = [
        exports.module(entry, null, {}, artifacts, new Set(), opts)
      ]

      while (queue.length > 0) {
        const generator = queue.pop()

        let next = generator.next()

        while (next.done !== true) {
          const value = next.value

          if (value.module) {
            next = generator.next(readModule(value.module))
          } else if (value.prefix) {
            const result = []

            if (typeof listPrefix === 'function') {
              for (const url of listPrefix(value.prefix)) {
                result.push(url)
              }
            } else {
              if (readModule(value.prefix) !== null) {
                result.push(value.prefix)
              }
            }

            next = generator.next(result)
          } else {
            if (value.children) queue.push(value.children)
            else yield value.dependency

            next = generator.next()
          }
        }
      }

      return artifacts
    },

    async *[Symbol.asyncIterator]() {
      const artifacts = { addons: [], assets: [] }

      const queue = [
        exports.module(entry, null, {}, artifacts, new Set(), opts)
      ]

      while (queue.length > 0) {
        const generator = queue.pop()

        let next = generator.next()

        while (next.done !== true) {
          const value = next.value

          if (value.module) {
            next = generator.next(await readModule(value.module))
          } else if (value.prefix) {
            const result = []

            if (typeof listPrefix === 'function') {
              for await (const url of listPrefix(value.prefix)) {
                result.push(url)
              }
            } else {
              if ((await readModule(value.prefix)) !== null) {
                result.push(value.prefix)
              }
            }

            next = generator.next(result)
          } else {
            if (value.children) queue.push(value.children)
            else yield value.dependency

            next = generator.next()
          }
        }
      }

      return artifacts
    }
  }
}

exports.resolve = resolve

exports.module = function* (
  url,
  source,
  attributes,
  artifacts,
  visited,
  opts = {}
) {
  const { resolutions = null } = opts

  if (visited.has(url.href)) return false

  if (source === null) {
    source = yield { module: url }

    if (source === null) {
      throw errors.MODULE_NOT_FOUND(
        `Cannot find module '${url.href}'`,
        url.href
      )
    }
  }

  visited.add(url.href)

  if (resolutions) {
    if (
      yield* exports.preresolved(
        url,
        source,
        resolutions,
        artifacts,
        visited,
        opts
      )
    ) {
      return true
    }
  }

  const imports = {}

  for (const packageURL of lookupPackageScope(url, opts)) {
    const source = yield { module: packageURL }

    if (source !== null) {
      imports['#package'] = packageURL.href

      yield {
        children: exports.package(packageURL, source, artifacts, visited, opts)
      }

      break
    }
  }

  if (
    typeof attributes === 'object' &&
    attributes !== null &&
    typeof attributes.imports === 'string'
  ) {
    const url = new URL(attributes.imports)

    const source = yield { module: url }

    if (source !== null) {
      opts = {
        ...opts,
        imports: mixinImports(opts.imports, JSON.parse(source), url)
      }
    }
  }

  const lexer = { imports: [] }

  yield* exports.imports(url, source, imports, artifacts, lexer, visited, opts)

  yield {
    dependency: { url, source, imports: compressImportsMap(imports), lexer }
  }

  return true
}

exports.package = function* (url, source, artifacts, visited, opts = {}) {
  if (visited.has(url.href)) return false

  visited.add(url.href)

  const info = JSON.parse(source)

  if (info) {
    yield { dependency: { url, source, imports: {}, lexer: { imports: [] } } }

    if (info.addon) {
      yield { children: exports.prebuilds(url, artifacts, visited, opts) }
    }

    if (info.assets) {
      yield {
        children: exports.assets(info.assets, url, artifacts, visited, opts)
      }
    }

    return true
  }

  return false
}

exports.preresolved = function* (
  url,
  source,
  resolutions,
  artifacts,
  visited,
  opts = {}
) {
  const imports = resolutions[url.href]

  if (typeof imports !== 'object' || imports === null) return false

  for (const [specifier, entry] of Object.entries(imports)) {
    const stack = [entry]

    while (stack.length > 0) {
      const entry = stack.pop()

      if (typeof entry === 'string') {
        const url = new URL(entry)

        if (specifier === '#package') {
          yield {
            children: exports.package(url, null, artifacts, visited, opts)
          }
        } else {
          yield {
            children: exports.module(url, null, {}, artifacts, visited, opts)
          }
        }
      } else {
        stack.unshift(...Object.values(entry))
      }
    }
  }

  yield {
    dependency: {
      url,
      source,
      imports: compressImportsMap(imports),
      lexer: { imports: [] }
    }
  }

  return true
}

exports.imports = function* (
  parentURL,
  source,
  imports,
  artifacts,
  lexer,
  visited,
  opts = {}
) {
  const {
    resolve = exports.resolve.default,
    builtinProtocol = 'builtin:',
    linkedProtocol = 'linked:',
    deferredProtocol = 'deferred:',
    matchedConditions = []
  } = opts

  let yielded = false

  const queue = []

  for (const entry of lex(source).imports) {
    let specifier = entry.specifier
    let condition = 'default'

    if (entry.type & lex.constants.ADDON) {
      specifier = specifier || '.'
      condition = 'addon'
    } else if (entry.type & lex.constants.ASSET) {
      condition = 'asset'
    }

    if (entry.attributes.imports) {
      const specifier = entry.attributes.imports

      queue.push({
        entry: {
          type: 0,
          specifier,
          names: [],
          attributes: {},
          position: [0, 0, 0]
        },
        specifier,
        condition: 'default'
      })
    }

    lexer.imports.push(entry)

    queue.push({ entry, specifier, condition })
  }

  while (queue.length > 0) {
    const { entry, specifier, condition } = queue.shift()

    matchedConditions.push(condition)

    const resolver = resolve(entry, parentURL, { ...opts, matchedConditions })
    const candidates = []

    let next = resolver.next()
    let resolutions = 0

    while (next.done !== true) {
      const value = next.value

      if (value.package) {
        next = resolver.next(JSON.parse(yield { module: value.package }))
      } else {
        const url = value.resolution

        candidates.push(url)

        let resolved = false

        if (
          url.protocol === builtinProtocol ||
          url.protocol === linkedProtocol ||
          url.protocol === deferredProtocol
        ) {
          addResolution(imports, specifier, matchedConditions, url)

          resolved = yielded = true
        } else if (condition === 'asset') {
          const prefix = yield { prefix: url }

          if (prefix.length !== 0) {
            addResolution(imports, specifier, matchedConditions, url)

            for (const url of prefix) {
              yield {
                children: exports.module(
                  url,
                  null,
                  {},
                  artifacts,
                  visited,
                  opts
                )
              }

              addURL(artifacts.assets, url)
            }

            resolved = yielded = true
          }
        } else {
          const source = yield { module: url }

          if (source !== null) {
            addResolution(imports, specifier, matchedConditions, url)

            const attributes = entry.attributes

            if (attributes.imports) {
              attributes.imports = imports[attributes.imports].default
            }

            yield {
              children: exports.module(
                url,
                source,
                attributes,
                artifacts,
                visited,
                opts
              )
            }

            resolved = yielded = true
          }
        }

        if (resolved) {
          if (condition === 'addon') addURL(artifacts.addons, url)

          resolutions++
        }

        next = resolver.next(resolved)
      }
    }

    matchedConditions.pop()

    if (resolutions === 0) {
      let message = `Cannot find ${condition === 'default' ? 'module' : condition} '${specifier}' imported from '${parentURL.href}'`

      if (candidates.length > 0) {
        message += '\nCandidates:'
        message += '\n' + candidates.map((url) => '- ' + url.href).join('\n')
      }

      switch (condition) {
        case 'addon':
          throw errors.ADDON_NOT_FOUND(
            message,
            specifier,
            parentURL,
            candidates
          )
        case 'asset':
          throw errors.ASSET_NOT_FOUND(
            message,
            specifier,
            parentURL,
            candidates
          )
        default:
          throw errors.MODULE_NOT_FOUND(
            message,
            specifier,
            parentURL,
            candidates
          )
      }
    }
  }

  return yielded
}

exports.prebuilds = function* (packageURL, artifacts, visited, opts = {}) {
  const {
    host = null, // Shorthand for single host resolution
    hosts = host !== null ? [host] : [],
    matchedConditions = []
  } = opts

  const [prebuildsURL = null] = lookupPrebuildsScope(packageURL, opts)

  if (prebuildsURL === null) return false

  let yielded = false

  for (const host of hosts) {
    const prefix = new URL(host + '/', prebuildsURL)

    const conditions = host.split('-')

    matchedConditions.push(...conditions)

    for (const url of yield { prefix }) {
      const source = yield { module: url }

      if (source !== null) {
        addURL(artifacts.addons, url)

        yield {
          children: exports.module(url, source, {}, artifacts, visited, opts)
        }

        yielded = true
      }
    }

    for (const _ of conditions) matchedConditions.pop()
  }

  return yielded
}

exports.assets = function* (
  patterns,
  parentURL,
  artifacts,
  visited,
  opts = {}
) {
  const matches = yield* exports.patternMatches(patterns, parentURL, [], opts)

  let yielded = false

  for (const url of matches) {
    const source = yield { module: url }

    if (source !== null) {
      addURL(artifacts.assets, url)

      yield {
        children: exports.module(url, source, {}, artifacts, visited, opts)
      }

      yielded = true
    }
  }

  return yielded
}

exports.patternMatches = function* patternMatches(
  pattern,
  parentURL,
  matches,
  opts = {}
) {
  const { conditions = [], matchedConditions = [] } = opts

  if (typeof pattern === 'string') {
    let patternNegate = false
    let patternBase
    let patternTrailer

    if (pattern[0] === '!') {
      pattern = pattern.substring(1)
      patternNegate = true
    }

    const patternIndex = pattern.indexOf('*')

    if (patternIndex === -1) {
      patternBase = pattern
      patternTrailer = ''
    } else {
      patternBase = pattern.substring(0, patternIndex)
      patternTrailer = pattern.substring(patternIndex + 1)
    }

    const prefix = new URL(patternBase, parentURL)

    for (const url of yield { prefix }) {
      if (patternIndex === -1) {
        if (patternNegate) removeURL(matches, url)
        else addURL(matches, url)
      } else if (patternTrailer === '' || url.href.endsWith(patternTrailer)) {
        addURL(matches, url)
      } else if (patternNegate) {
        removeURL(matches, url)
      }
    }
  } else if (Array.isArray(pattern)) {
    for (const patternValue of pattern) {
      yield* patternMatches(patternValue, parentURL, matches, opts)
    }
  } else if (typeof pattern === 'object' && pattern !== null) {
    let yielded = false

    for (const [condition, patternValue, subset] of conditionMatches(
      pattern,
      conditions,
      opts
    )) {
      matchedConditions.push(condition)

      if (
        yield* patternMatches(patternValue, parentURL, matches, {
          ...opts,
          conditions: subset
        })
      ) {
        yielded = true
      }

      matchedConditions.pop()
    }

    if (yielded) return true
  }

  return matches
}

function addURL(array, url) {
  let lo = 0
  let hi = array.length - 1

  while (lo <= hi) {
    const mid = lo + ((hi - lo) >> 1)
    const found = array[mid]

    if (found.href === url.href) return

    if (found.href < url.href) {
      lo = mid + 1
    } else {
      hi = mid - 1
    }
  }

  array.splice(lo, 0, url)
}

function removeURL(array, url) {
  let lo = 0
  let hi = array.length - 1

  while (lo <= hi) {
    const mid = lo + ((hi - lo) >> 1)
    const found = array[mid]

    if (found.href === url.href) break

    if (found.href < url.href) {
      lo = mid + 1
    } else {
      hi = mid - 1
    }
  }

  if (array[lo].href === url.href) array.splice(lo, 1)
}

function addResolution(imports, specifier, conditions, url) {
  imports[specifier] = imports[specifier] || {}

  let current = imports[specifier]

  for (let i = 0, n = conditions.length - 1; i < n; i++) {
    const key = conditions[i]

    if (key in current === false) {
      current[key] = {}
    } else if (typeof current[key] !== 'object') {
      current[key] = { default: current[key] }
    }

    current = current[key]
  }

  const last = conditions[conditions.length - 1]

  current[last] = url.href

  if ('default' in current) {
    const value = current.default

    delete current.default

    current.default = value
  }
}

function compressImportsMap(imports) {
  const entries = []

  for (const entry of Object.entries(imports)) {
    entry[1] = compressImportsMapEntry(entry[1])

    entries.push(entry)
  }

  return Object.fromEntries(entries)
}

function compressImportsMapEntry(resolved) {
  if (typeof resolved === 'string') return resolved

  let entries = []
  let primary = null

  for (const entry of Object.entries(resolved)) {
    entry[1] = compressImportsMapEntry(entry[1])

    entries.push(entry)

    if (entry[0] === 'default') primary = entry[1]
  }

  entries = entries.filter(
    ([condition, resolved]) => condition === 'default' || resolved !== primary
  )

  if (entries.length === 1) return entries[0][1]

  return Object.fromEntries(entries)
}

function mixinImports(target, imports, url) {
  if (typeof imports === 'object' && imports !== null && 'imports' in imports) {
    imports = imports.imports
  }

  if (typeof imports !== 'object' || imports === null) {
    throw errors.INVALID_IMPORTS_MAP(
      `Imports map at '${url.href}' is not valid`
    )
  }

  return { ...target, ...imports }
}
module.exports = class ModuleTraverseError extends Error {
  constructor(msg, code, fn = ModuleTraverseError) {
    super(`${code}: ${msg}`)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name() {
    return 'ModuleTraverseError'
  }

  static MODULE_NOT_FOUND(msg, specifier, referrer = null, candidates = []) {
    const err = new ModuleTraverseError(
      msg,
      'MODULE_NOT_FOUND',
      ModuleTraverseError.MODULE_NOT_FOUND
    )

    err.specifier = specifier
    err.referrer = referrer
    err.candidates = candidates

    return err
  }

  static ADDON_NOT_FOUND(msg, specifier, referrer = null, candidates = []) {
    const err = new ModuleTraverseError(
      msg,
      'ADDON_NOT_FOUND',
      ModuleTraverseError.ADDON_NOT_FOUND
    )

    err.specifier = specifier
    err.referrer = referrer
    err.candidates = candidates

    return err
  }

  static ASSET_NOT_FOUND(msg, specifier, referrer = null, candidates = []) {
    const err = new ModuleTraverseError(
      msg,
      'ASSET_NOT_FOUND',
      ModuleTraverseError.ASSET_NOT_FOUND
    )

    err.specifier = specifier
    err.referrer = referrer
    err.candidates = candidates

    return err
  }

  static INVALID_IMPORTS_MAP(msg) {
    return new ModuleTraverseError(
      msg,
      'INVALID_IMPORTS_MAP',
      ModuleTraverseError.INVALID_IMPORTS_MAP
    )
  }
}
exports.module = require('bare-module-resolve').module
exports.addon = require('bare-addon-resolve').addon

exports.default = require('./resolve/default')
exports.bare = require('./resolve/bare')
exports.node = require('./resolve/node')
const lex = require('bare-module-lexer')
const resolve = require('../resolve')
const runtime = require('../runtime')

module.exports = function (entry, parentURL, opts = {}) {
  const {
    linked = false,
    platform = runtime.platform,
    arch = runtime.arch,
    simulator = runtime.simulator,
    host = `${platform}-${arch}${simulator ? '-simulator' : ''}`,
    target = [host]
  } = opts

  let extensions
  let conditions = target.map((host) => ['bare', 'node', ...host.split('-')])

  if (entry.type & lex.constants.ADDON) {
    extensions = linked ? [] : ['.bare', '.node']
    conditions = conditions.map((conditions) => ['addon', ...conditions])

    return resolve.addon(entry.specifier || '.', parentURL, {
      extensions,
      conditions,
      hosts: target,
      linked,
      ...opts
    })
  }

  if (entry.type & lex.constants.ASSET) {
    conditions = conditions.map((conditions) => ['asset', ...conditions])
  } else {
    extensions = ['.js', '.cjs', '.mjs', '.json', '.bare', '.node']

    if (entry.type & lex.constants.REQUIRE) {
      conditions = conditions.map((conditions) => ['require', ...conditions])
    } else if (entry.type & lex.constants.IMPORT) {
      conditions = conditions.map((conditions) => ['import', ...conditions])
    }
  }

  return resolve.module(entry.specifier, parentURL, {
    extensions,
    conditions,
    ...opts
  })
}
const lex = require('bare-module-lexer')
const resolve = require('../resolve')

module.exports = function (entry, parentURL, opts) {
  if (entry.type & lex.constants.ADDON) {
    return resolve.addon(entry.specifier || '.', parentURL, opts)
  }

  return resolve.module(entry.specifier, parentURL, opts)
}
const lex = require('bare-module-lexer')
const resolve = require('../resolve')
const runtime = require('../runtime')

module.exports = function (entry, parentURL, opts = {}) {
  const {
    platform = runtime.platform,
    arch = runtime.arch,
    simulator = runtime.simulator,
    host = `${platform}-${arch}${simulator ? '-simulator' : ''}`,
    target = [host]
  } = opts

  let extensions
  let conditions

  if (entry.type & lex.constants.ADDON) {
    extensions = ['.node']
    conditions = target.map((host) => ['node', 'addon', ...host.split('-')])

    return resolve.addon(entry.specifier || '.', parentURL, {
      extensions,
      conditions,
      host,
      ...opts
    })
  }

  if (entry.type & lex.constants.ASSET) {
    conditions = ['node', 'asset']
  } else if (entry.type & lex.constants.REQUIRE) {
    extensions = ['.js', '.json', '.node']
    conditions = ['node', 'require']
  } else if (entry.type & lex.constants.IMPORT) {
    conditions = ['node', 'import']
  } else {
    conditions = ['node']
  }

  return resolve.module(entry.specifier, parentURL, {
    extensions,
    conditions,
    ...opts
  })
}
module.exports = require('#runtime')
exports.platform = process.platform
exports.arch = process.arch
exports.simulator = false
{
  "name": "bare-module-traverse",
  "version": "1.7.0",
  "description": "Low-level module graph traversal for Bare",
  "exports": {
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./resolve": {
      "types": "./lib/resolve.d.ts",
      "default": "./lib/resolve.js"
    },
    "./resolve/default": {
      "types": "./lib/resolve/default.d.ts",
      "default": "./lib/resolve/default.js"
    },
    "./resolve/bare": {
      "types": "./lib/resolve/bare.d.ts",
      "default": "./lib/resolve/bare.js"
    },
    "./resolve/node": {
      "types": "./lib/resolve/node.d.ts",
      "default": "./lib/resolve/node.js"
    },
    "./package": "./package.json"
  },
  "files": [
    "index.js",
    "index.d.ts",
    "lib"
  ],
  "imports": {
    "#runtime": {
      "bare": "./lib/runtime/bare.js",
      "node": "./lib/runtime/node.js"
    }
  },
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-module-traverse.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-module-traverse/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-module-traverse#readme",
  "dependencies": {
    "bare-addon-resolve": "^1.5.0",
    "bare-module-lexer": "^1.4.0",
    "bare-module-resolve": "^1.7.0"
  },
  "devDependencies": {
    "bare-buffer": "^3.0.2",
    "bare-url": "^2.1.3",
    "brittle": "^3.2.1",
    "prettier": "^3.4.2",
    "prettier-config-standard": "^7.0.0"
  },
  "peerDependencies": {
    "bare-buffer": "*",
    "bare-url": "*"
  },
  "peerDependenciesMeta": {
    "bare-buffer": {
      "optional": true
    },
    "bare-url": {
      "optional": true
    }
  }
}
const pack = require('bare-pack')

module.exports = async function (drive, entry = '/index.js', opts = {}) {
  if (typeof entry === 'object' && entry !== null) {
    opts = entry
    entry = '/index.js'
  }

  const root = new URL('drive:///')

  const bundle = await pack(new URL(entry, root), opts, readModule, listPrefix)

  return bundle.unmount(root)

  async function readModule(url) {
    return drive.get(url.pathname)
  }

  async function* listPrefix(url) {
    const entry = await drive.get(url.pathname)

    if (entry !== null) return yield url

    if (url.pathname[url.pathname.length - 1] !== '/') {
      url.pathname += '/'
    }

    for await (const { key } of drive.list(url.pathname, { recursive: true })) {
      yield new URL(key, url)
    }
  }
}
{
  "name": "bare-pack-drive",
  "version": "1.0.2",
  "description": "Pack drives to Bare bundles",
  "exports": {
    ".": "./index.js",
    "./package": "./package.json"
  },
  "files": [
    "index.js"
  ],
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-pack-drive.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-pack-drive/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-pack-drive#readme",
  "dependencies": {
    "bare-pack": "^1.0.1"
  },
  "peerDependencies": {
    "bare-bundle": "^1.8.0"
  },
  "devDependencies": {
    "bare-bundle": "*",
    "brittle": "^3.1.1",
    "localdrive": "^1.12.1",
    "prettier": "^3.3.3",
    "prettier-config-standard": "^7.0.0"
  }
}
const Semaphore = require('promaphore')
const Bundle = require('bare-bundle')
const traverse = require('bare-module-traverse')
const preset = require('./lib/preset')

module.exports = async function pack(entry, opts, readModule, listPrefix) {
  if (typeof opts === 'function') {
    listPrefix = readModule
    readModule = opts
    opts = {}
  }

  opts = withPreset(opts)

  const { concurrency = 0 } = opts

  const semaphore = concurrency > 0 ? new Semaphore(concurrency) : null

  const bundle = new Bundle()

  const addons = []
  const assets = []

  await process(
    traverse.module(
      entry,
      await readModule(entry),
      null,
      { addons, assets },
      new Set(),
      opts
    )
  )

  bundle.addons = addons.map((url) => url.href)
  bundle.assets = assets.map((url) => url.href)

  return bundle

  async function process(generator) {
    if (semaphore !== null) await semaphore.wait()

    const queue = []

    let next = generator.next()

    while (next.done !== true) {
      const value = next.value

      if (value.module) {
        next = generator.next(await readModule(value.module))
      } else if (value.prefix) {
        const result = []

        if (typeof listPrefix === 'function') {
          for await (const url of listPrefix(value.prefix)) {
            result.push(url)
          }
        } else {
          if ((await readModule(value.prefix)) !== null) {
            result.push(value.prefix)
          }
        }

        next = generator.next(result)
      } else {
        if (value.children) {
          queue.push(value.children)
        } else {
          const { url, source, imports } = value.dependency

          bundle.write(url.href, source, {
            main: url.href === entry.href,
            imports
          })
        }

        next = generator.next()
      }
    }

    if (semaphore !== null) semaphore.signal()

    await Promise.all(queue.map(process))
  }
}

function withPreset(opts = {}) {
  if (opts.preset) {
    if (opts.preset in preset === false) {
      throw new Error(`Unknown preset '${opts.preset}'`)
    }

    opts = Object.assign({}, opts, preset[opts.preset])
  }

  return opts
}
exports.android = require('./preset/android')
exports.darwin = require('./preset/darwin')
exports.desktop = require('./preset/desktop')
exports.ios = require('./preset/ios')
exports.linux = require('./preset/linux')
exports.mobile = require('./preset/mobile')
exports.win32 = require('./preset/win32')
module.exports = {
  linked: true,
  target: ['android-arm', 'android-arm64', 'android-ia32', 'android-x64']
}
module.exports = {
  target: ['darwin-arm64', 'darwin-x64']
}
module.exports = {
  target: [
    'darwin-arm64',
    'darwin-x64',
    'linux-arm64',
    'linux-x64',
    'win32-arm64',
    'win32-x64'
  ]
}
module.exports = {
  linked: true,
  target: ['ios-arm64', 'ios-arm64-simulator', 'ios-x64-simulator']
}
module.exports = {
  target: ['linux-arm64', 'linux-x64']
}
module.exports = {
  linked: true,
  target: [
    'android-arm',
    'android-arm64',
    'android-ia32',
    'android-x64',
    'ios-arm64',
    'ios-arm64-simulator',
    'ios-x64-simulator'
  ]
}
module.exports = {
  target: ['win32-arm64', 'win32-x64']
}
{
  "name": "bare-pack",
  "version": "1.4.8",
  "description": "Bundle packing for Bare",
  "exports": {
    "./package": "./package.json",
    ".": {
      "types": "./index.d.ts",
      "default": "./index.js"
    },
    "./fs": "./lib/fs.js",
    "./preset": "./lib/preset.js",
    "./preset/*": "./lib/preset/*.js"
  },
  "imports": {
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    },
    "path": {
      "bare": "bare-path",
      "default": "path"
    },
    "url": {
      "bare": "bare-url",
      "default": "url"
    }
  },
  "bin": {
    "bare-pack": "bin.js"
  },
  "files": [
    "index.js",
    "index.d.ts",
    "bin.js",
    "lib"
  ],
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-pack.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-pack/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-pack#readme",
  "dependencies": {
    "bare-bundle": "^1.8.3",
    "bare-bundle-id": "^1.0.0",
    "bare-fs": "^4.2.1",
    "bare-module-traverse": "~1.7.0",
    "bare-path": "^3.0.0",
    "bare-url": "*",
    "paparam": "^1.5.0",
    "promaphore": "^1.0.0"
  },
  "devDependencies": {
    "bare-buffer": "^3.0.2",
    "brittle": "^3.1.1",
    "prettier": "^3.3.3",
    "prettier-config-standard": "^7.0.0"
  },
  "peerDependencies": {
    "bare-buffer": "*",
    "bare-url": "*"
  },
  "peerDependenciesMeta": {
    "bare-buffer": {
      "optional": true
    },
    "bare-url": {
      "optional": true
    }
  }
}
exports.constants = require('./lib/constants')
exports.errors = require('./lib/errors')

const Version = exports.Version = require('./lib/version')
const Range = exports.Range = require('./lib/range')
exports.Comparator = require('./lib/comparator')

exports.satisfies = function satisfies (version, range) {
  if (typeof version === 'string') version = Version.parse(version)
  if (typeof range === 'string') range = Range.parse(range)

  return range.test(version)
}
const constants = require('./constants')

const symbols = {
  [constants.EQ]: '=',
  [constants.LT]: '<',
  [constants.LTE]: '<=',
  [constants.GT]: '>',
  [constants.GTE]: '>='
}

module.exports = class Comparator {
  constructor (operator, version) {
    this.operator = operator
    this.version = version
  }

  test (version) {
    const result = version.compare(this.version)

    switch (this.operator) {
      case constants.LT: return result < 0
      case constants.LTE: return result <= 0
      case constants.GT: return result > 0
      case constants.GTE: return result >= 0
      default: return result === 0
    }
  }

  toString () {
    return symbols[this.operator] + this.version
  }
}
module.exports = {
  EQ: 1,
  LT: 2,
  LTE: 3,
  GT: 4,
  GTE: 5
}
module.exports = class SemVerError extends Error {
  constructor (msg, code, fn = SemVerError) {
    super(`${code}: ${msg}`)
    this.code = code

    if (Error.captureStackTrace) {
      Error.captureStackTrace(this, fn)
    }
  }

  get name () {
    return 'SemVerError'
  }

  static INVALID_VERSION (msg, fn = SemVerError.INVALID_VERSION) {
    return new SemVerError(msg, 'INVALID_VERSION', fn)
  }

  static INVALID_RANGE (msg, fn = SemVerError.INVALID_RANGE) {
    return new SemVerError(msg, 'INVALID_RANGE', fn)
  }
}
const constants = require('./constants')
const errors = require('./errors')
const Version = require('./version')
const Comparator = require('./comparator')

const Range = module.exports = exports = class Range {
  constructor (comparators = []) {
    this.comparators = comparators
  }

  test (version) {
    for (const set of this.comparators) {
      let matches = true

      for (const comparator of set) {
        if (comparator.test(version)) continue
        matches = false
        break
      }

      if (matches) return true
    }

    return false
  }

  toString () {
    let result = ''
    let first = true

    for (const set of this.comparators) {
      if (first) first = false
      else result += ' || '

      result += set.join(' ')
    }

    return result
  }
}

exports.parse = function parse (input, state = { position: 0, partial: false }) {
  let i = state.position
  let c

  const unexpected = (expected) => {
    let msg

    if (i >= input.length) {
      msg = `Unexpected end of input in '${input}'`
    } else {
      msg = `Unexpected token '${input[i]}' in '${input}' at position ${i}`
    }

    if (expected) msg += `, ${expected}`

    throw errors.INVALID_VERSION(msg, unexpected)
  }

  const comparators = []

  while (i < input.length) {
    const set = []

    while (i < input.length) {
      c = input[i]

      let operator = constants.EQ

      if (c === '<') {
        operator = constants.LT
        c = input[++i]

        if (c === '=') {
          operator = constants.LTE
          c = input[++i]
        }
      } else if (c === '>') {
        operator = constants.GT
        c = input[++i]

        if (c === '=') {
          operator = constants.GTE
          c = input[++i]
        }
      } else if (c === '=') {
        c = input[++i]
      }

      const state = { position: i, partial: true }

      set.push(new Comparator(operator, Version.parse(input, state)))

      c = input[i = state.position]

      while (c === ' ') c = input[++i]

      if (c === '|' && input[i + 1] === '|') {
        c = input[i += 2]

        while (c === ' ') c = input[++i]

        break
      }

      if (c && c !== '<' && c !== '>') unexpected('expected \'||\', \'<\', or \'>\'')
    }

    if (set.length) comparators.push(set)
  }

  if (i < input.length && state.partial === false) unexpected('expected end of input')

  state.position = i

  return new Range(comparators)
}
const errors = require('./errors')

const Version = module.exports = exports = class Version {
  constructor (major, minor, patch, opts = {}) {
    const {
      prerelease = [],
      build = []
    } = opts

    this.major = major
    this.minor = minor
    this.patch = patch
    this.prerelease = prerelease
    this.build = build
  }

  compare (version) {
    return exports.compare(this, version)
  }

  toString () {
    let result = `${this.major}.${this.minor}.${this.patch}`

    if (this.prerelease.length) {
      result += '-' + this.prerelease.join('.')
    }

    if (this.build.length) {
      result += '+' + this.build.join('.')
    }

    return result
  }
}

exports.parse = function parse (input, state = { position: 0, partial: false }) {
  let i = state.position
  let c

  const unexpected = (expected) => {
    let msg

    if (i >= input.length) {
      msg = `Unexpected end of input in '${input}'`
    } else {
      msg = `Unexpected token '${input[i]}' in '${input}' at position ${i}`
    }

    if (expected) msg += `, ${expected}`

    throw errors.INVALID_VERSION(msg, unexpected)
  }

  const components = []

  while (components.length < 3) {
    c = input[i]

    if (components.length > 0) {
      if (c === '.') c = input[++i]
      else unexpected('expected \'.\'')
    }

    if (c === '0') {
      components.push(0)

      i++
    } else if (c >= '1' && c <= '9') {
      let j = 0
      do c = input[i + ++j]
      while (c >= '0' && c <= '9')

      components.push(parseInt(input.substring(i, i + j)))

      i += j
    } else unexpected('expected /[0-9]/')
  }

  const prerelease = []

  if (input[i] === '-') {
    i++

    while (true) {
      c = input[i]

      let tag = ''
      let j = 0

      while (c >= '0' && c <= '9') c = input[i + ++j]

      let isNumeric = false

      if (j) {
        tag += input.substring(i, i + j)

        c = input[i += j]

        isNumeric = tag[0] !== '0' || tag.length === 1
      }

      j = 0

      while ((c >= '0' && c <= '9') || (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') || c === '-') c = input[i + ++j]

      if (j) {
        tag += input.substring(i, i + j)

        c = input[i += j]
      } else if (!isNumeric) unexpected('expected /[a-zA-Z-]/')

      prerelease.push(tag)

      if (c === '.') c = input[++i]
      else break
    }
  }

  const build = []

  if (input[i] === '+') {
    i++

    while (true) {
      c = input[i]

      let tag = ''
      let j = 0

      while ((c >= '0' && c <= '9') || (c >= 'a' && c <= 'z') || (c >= 'A' && c <= 'Z') || c === '-') c = input[i + ++j]

      if (j) {
        tag += input.substring(i, i + j)

        c = input[i += j]
      } else unexpected('expected /[0-9a-zA-Z-]/')

      build.push(tag)

      if (c === '.') c = input[++i]
      else break
    }
  }

  if (i < input.length && state.partial === false) unexpected('expected end of input')

  state.position = i

  return new Version(...components, { prerelease, build })
}

const integer = /^[0-9]+$/

exports.compare = function compare (a, b) {
  if (a.major > b.major) return 1
  if (a.major < b.major) return -1

  if (a.minor > b.minor) return 1
  if (a.minor < b.minor) return -1

  if (a.patch > b.patch) return 1
  if (a.patch < b.patch) return -1

  if (a.prerelease.length === 0) return b.prerelease.length === 0 ? 0 : 1
  if (b.prerelease.length === 0) return -1

  let i = 0
  do {
    let x = a.prerelease[i]
    let y = b.prerelease[i]

    if (x === undefined) return y === undefined ? 0 : -1
    if (y === undefined) return 1

    if (x === y) continue

    const xInt = integer.test(x)
    const yInt = integer.test(y)

    if (xInt && yInt) {
      x = +x
      y = +y
    } else {
      if (xInt) return -1
      if (yInt) return 1
    }

    return x > y ? 1 : -1
  } while (++i)
}
{
  "name": "bare-semver",
  "version": "1.0.1",
  "description": "Minimal semantic versioning library for Bare",
  "exports": {
    ".": "./index.js",
    "./package": "./package.json",
    "./constants": "./lib/constants.js",
    "./errors": "./lib/errors.js",
    "./version": "./lib/version.js",
    "./range": "./lib/range.js",
    "./comparator": "./lib/comparator.js"
  },
  "files": [
    "index.js",
    "lib"
  ],
  "scripts": {
    "test": "standard && bare test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-semver.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-semver/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-semver#readme",
  "devDependencies": {
    "brittle": "^3.2.1",
    "standard": "^17.0.0"
  }
}
const Semaphore = require('promaphore')
const Bundle = require('bare-bundle')

module.exports = async function unpack(bundle, opts, writeFile) {
  if (typeof opts === 'function') {
    writeFile = opts
    opts = {}
  }

  const { files = true, addons = files, assets = files, concurrency = 0 } = opts

  const semaphore = concurrency > 0 ? new Semaphore(concurrency) : null

  const unpack = new Set()

  if (files === true) {
    for (const key of bundle.keys()) unpack.add(key)

    if (addons !== true) {
      for (const key of bundle.addons) unpack.delete(key)
    }

    if (assets !== true) {
      for (const key of bundle.assets) unpack.delete(key)
    }
  } else {
    if (addons === true) {
      for (const key of bundle.addons) unpack.add(key)
    }

    if (assets === true) {
      for (const key of bundle.assets) unpack.add(key)
    }
  }

  const repack = new Set()
  const rewrites = new Map()
  const promises = []

  for (const key of bundle.keys()) promises.push(process(key))

  await Promise.all(promises)

  const result = new Bundle()

  if (files !== true) result.main = bundle.main

  result.imports = rewriteImportsMap(bundle.imports, rewrites)

  if (addons !== true) result.addons = bundle.addons
  if (assets !== true) result.assets = bundle.assets

  for (const key of repack) {
    result.write(key, bundle.read(key), {
      mode: bundle.mode(key),
      imports: rewriteImportsMap(bundle.resolutions[key], rewrites)
    })
  }

  return result

  async function process(key) {
    if (semaphore !== null) await semaphore.wait()

    if (unpack.has(key)) {
      rewrites.set(key, String(await writeFile(key)))
    } else {
      repack.add(key)
    }

    if (semaphore !== null) semaphore.signal()
  }
}

function rewriteImportsMap(imports, rewrites) {
  if (typeof imports !== 'object' || imports === null) return null

  return transformImportsMap(imports, (value) => rewrites.get(value) || value)
}

function transformImportsMap(value, fn) {
  const imports = {}

  for (const entry of Object.entries(value)) {
    const condition = entry[0]

    imports[condition] = transformImportsMapEntry(entry[1], fn)
  }

  // console.log('IMPORT', imports)

  return imports
}

function transformImportsMapEntry(value, fn) {
  if (typeof value === 'string') return fn(value)

  return transformImportsMap(value, fn)
}
{
  "name": "bare-unpack",
  "version": "1.1.0",
  "description": "Bundle unpacking for Bare",
  "exports": {
    "./package": "./package.json",
    ".": "./index.js"
  },
  "imports": {
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    },
    "path": {
      "bare": "bare-path",
      "default": "path"
    }
  },
  "bin": {
    "bare-unpack": "bin.js"
  },
  "files": [
    "index.js",
    "bin.js"
  ],
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/bare-unpack.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/bare-unpack/issues"
  },
  "homepage": "https://github.com/holepunchto/bare-unpack#readme",
  "dependencies": {
    "bare-bundle": "^1.8.3",
    "bare-fs": "^4.2.3",
    "bare-path": "^3.0.0",
    "paparam": "^1.8.5",
    "promaphore": "^1.0.0"
  },
  "devDependencies": {
    "brittle": "^3.1.1",
    "prettier": "^3.3.3",
    "prettier-config-standard": "^7.0.0"
  }
}
const b4a = require('b4a')

module.exports = function (a, b) {
  return new Promise((resolve, reject) => binaryEquals(a, b, resolve, reject))
}

function binaryEquals (a, b, resolve, reject) {
  let aBuf = null
  let aEnded = false

  let bBuf = null
  let bEnded = false

  let closed = 0
  let done = false
  let error = null
  let equals = false

  a.on('readable', tick)
  a.on('end', onend)
  a.on('error', onerror)
  a.on('close', onclose)

  b.on('readable', tick)
  b.on('end', onend)
  b.on('error', onerror)
  b.on('close', onclose)

  function onerror (err) {
    error = err
    a.destroy()
    b.destroy()
  }

  function onclose () {
    if (++closed !== 2) return
    if (error !== null && done === false) reject(error)
    else resolve(equals)
  }

  function ondone (eq) {
    if (done) return
    done = true

    equals = eq

    a.destroy()
    b.destroy()
  }

  function onend () {
    if (this === a) aEnded = true
    else bEnded = true
    tick()
  }

  function tick () {
    while (done === false) {
      if (aBuf === null) aBuf = a.read()
      if (bBuf === null) bBuf = b.read()

      if (aBuf === null && bBuf === null && aEnded && bEnded) {
        ondone(true)
        return
      }

      if (aBuf !== null && (bBuf === null && bEnded)) {
        ondone(false)
        return
      }

      if (bBuf !== null && (aBuf === null && aEnded)) {
        ondone(false)
        return
      }

      if (aBuf === null || bBuf === null) return // read pending

      if (aBuf.byteLength === bBuf.byteLength) {
        if (b4a.equals(aBuf, bBuf)) {
          aBuf = bBuf = null
          continue
        }

        ondone(false)
        return
      }

      const min = Math.min(aBuf.byteLength, bBuf.byteLength)

      if (b4a.equals(aBuf.subarray(0, min), bBuf.subarray(0, min))) {
        aBuf = aBuf.byteLength === min ? null : aBuf.subarray(min)
        bBuf = bBuf.byteLength === min ? null : bBuf.subarray(min)
        continue
      }

      ondone(false)
      return
    }
  }
}
{
  "name": "binary-stream-equals",
  "version": "1.0.0",
  "description": "Check if two binary streams have the same content",
  "main": "index.js",
  "dependencies": {
    "b4a": "^1.3.1"
  },
  "devDependencies": {
    "brittle": "^2.2.7",
    "standard": "^16.0.4",
    "streamx": "^2.12.4"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/binary-stream-equals.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/binary-stream-equals/issues"
  },
  "homepage": "https://github.com/mafintosh/binary-stream-equals"
}
module.exports = require('events')
{
  "name": "events-universal",
  "version": "1.0.1",
  "description": "Universal wrapper for the Node.js events module",
  "exports": {
    "./package": "./package.json",
    ".": {
      "bare": "./bare.js",
      "react-native": "./react-native.js",
      "default": "./default.js"
    }
  },
  "files": [
    "index.js",
    "default.js",
    "bare.js",
    "react-native.js"
  ],
  "scripts": {
    "test": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/events-universal.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/events-universal/issues"
  },
  "homepage": "https://github.com/holepunchto/events-universal#readme",
  "dependencies": {
    "bare-events": "^2.7.0"
  },
  "devDependencies": {
    "prettier": "^3.4.2",
    "prettier-config-holepunch": "^1.0.0"
  }
}
module.exports = class FixedFIFO {
  constructor (hwm) {
    if (!(hwm > 0) || ((hwm - 1) & hwm) !== 0) throw new Error('Max size for a FixedFIFO should be a power of two')
    this.buffer = new Array(hwm)
    this.mask = hwm - 1
    this.top = 0
    this.btm = 0
    this.next = null
  }

  clear () {
    this.top = this.btm = 0
    this.next = null
    this.buffer.fill(undefined)
  }

  push (data) {
    if (this.buffer[this.top] !== undefined) return false
    this.buffer[this.top] = data
    this.top = (this.top + 1) & this.mask
    return true
  }

  shift () {
    const last = this.buffer[this.btm]
    if (last === undefined) return undefined
    this.buffer[this.btm] = undefined
    this.btm = (this.btm + 1) & this.mask
    return last
  }

  peek () {
    return this.buffer[this.btm]
  }

  isEmpty () {
    return this.buffer[this.btm] === undefined
  }
}
const FixedFIFO = require('./fixed-size')

module.exports = class FastFIFO {
  constructor (hwm) {
    this.hwm = hwm || 16
    this.head = new FixedFIFO(this.hwm)
    this.tail = this.head
    this.length = 0
  }

  clear () {
    this.head = this.tail
    this.head.clear()
    this.length = 0
  }

  push (val) {
    this.length++
    if (!this.head.push(val)) {
      const prev = this.head
      this.head = prev.next = new FixedFIFO(2 * this.head.buffer.length)
      this.head.push(val)
    }
  }

  shift () {
    if (this.length !== 0) this.length--
    const val = this.tail.shift()
    if (val === undefined && this.tail.next) {
      const next = this.tail.next
      this.tail.next = null
      this.tail = next
      return this.tail.shift()
    }

    return val
  }

  peek () {
    const val = this.tail.peek()
    if (val === undefined && this.tail.next) return this.tail.next.peek()
    return val
  }

  isEmpty () {
    return this.length === 0
  }
}
{
  "name": "fast-fifo",
  "version": "1.3.2",
  "description": "A fast fifo implementation similar to the one powering nextTick in Node.js core",
  "main": "index.js",
  "files": [
    "./index.js",
    "./fixed-size.js"
  ],
  "dependencies": {},
  "devDependencies": {
    "standard": "^17.1.0",
    "brittle": "^3.3.2"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/fast-fifo.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/fast-fifo/issues"
  },
  "homepage": "https://github.com/mafintosh/fast-fifo"
}
const fs = require('fs')
const fsp = require('fs/promises')
const path = require('path')
const b4a = require('b4a')
const unixPathResolve = require('unix-path-resolve')
const { FileReadStream, FileWriteStream } = require('./streams.js')
const mutexify = require('mutexify/promise')
const MirrorDrive = require('mirror-drive')

module.exports = class Localdrive {
  constructor (root, opts = {}) {
    this.root = path.resolve(root)
    this.metadata = handleMetadataHooks(opts.metadata) || {}
    this.supportsMetadata = !!opts.metadata

    this._roots = []
    this._stat = opts.followLinks ? stat : lstat
    this._lock = mutexify()
    this._atomics = opts.atomic ? new Set() : null

    if (opts.roots) {
      for (const prefix of Object.keys(opts.roots)) {
        this._roots.push({
          from: unixPathResolve('/', prefix),
          to: path.resolve(opts.roots[prefix])
        })
      }
    }
  }

  _root (keyname) {
    for (const { from, to } of this._roots) {
      if (keyname.startsWith(from)) return { prefix: from, root: to }
    }

    return { prefix: null, root: this.root }
  }

  _resolve (key) {
    const keyname = unixPathResolve('/', key)
    const { prefix, root } = this._root(keyname)
    const filename = path.join(root, prefix ? keyname.replace(prefix, '') : keyname)
    return { root, keyname, filename }
  }

  async ready () { /* No-op, compatibility */ }
  async close () { /* No-op, compatibility */ }
  async flush () { /* No-op, compatibility */ }

  batch () {
    return this
  }

  checkout () {
    return this
  }

  toPath (key) {
    return this._resolve(key).filename
  }

  async entry (name, opts) {
    if (!opts || !opts.follow) return this._entry(name)

    for (let i = 0; i < 16; i++) {
      const node = await this._entry(name)
      if (!node || !node.value.linkname) return node

      name = unixPathResolve(node.key, node.value.linkname)
    }

    throw new Error('Recursive symlink')
  }

  async _entry (key) {
    if (typeof key === 'object') key = key.key

    const { root, keyname, filename } = this._resolve(key)

    const st = await this._stat(filename)
    if (!st || st.isDirectory()) {
      return null
    }

    const entry = {
      key: keyname,
      value: {
        executable: false,
        linkname: null,
        blob: null,
        metadata: null
      },
      mtime: st.mtimeMs
    }

    if (st.isSymbolicLink()) {
      let link = await fsp.readlink(filename)
      if (link.startsWith(root)) link = link.slice(root.length)
      entry.value.linkname = link.replace(/\\/g, '/')
      return entry
    }

    entry.value.executable = isExecutable(st.mode)
    if (this.metadata.get) entry.value.metadata = await this.metadata.get(keyname)

    if (st.isFile()) {
      const blockLength = st.blocks || Math.ceil(st.size / st.blksize) * 8
      entry.value.blob = { byteOffset: 0, blockOffset: 0, blockLength, byteLength: st.size }
      return entry
    }

    return null
  }

  async get (key, opts) {
    const entry = await this.entry(key, opts)
    if (!entry || !entry.value.blob) return null

    const rs = this.createReadStream(key)
    const chunks = []
    for await (const chunk of rs) {
      chunks.push(chunk)
    }
    return b4a.concat(chunks)
  }

  put (key, buffer, opts) {
    return new Promise((resolve, reject) => {
      const ws = this.createWriteStream(key, opts)
      let error = null
      ws.on('error', (err) => {
        error = err
      })
      ws.on('close', () => {
        if (error) reject(error)
        else resolve()
      })
      ws.end(buffer)
    })
  }

  async del (key) {
    const { root, keyname, filename } = this._resolve(key)

    try {
      await fsp.unlink(filename)
    } catch (error) {
      if (error.code === 'ENOENT') return
      throw error
    }

    const release = await this._lock()
    try {
      await gcEmptyFolders(root, path.dirname(filename))
    } finally {
      release()
    }

    if (this.metadata.del) await this.metadata.del(keyname)
  }

  async symlink (key, linkname) {
    const entry = await this.entry(key)
    if (entry) await this.del(key)

    const { filename: pointer } = this._resolve(key)

    const release = await this._lock()
    try {
      await fsp.mkdir(path.dirname(pointer), { recursive: true })

      const target = linkname.startsWith('/')
        ? this._resolve(linkname).filename
        : linkname.replace(/\//g, path.sep)

      const st = await this._stat(target)
      const type = st && st.isDirectory() ? 'junction' : null

      await fsp.symlink(target, pointer, type)
    } finally {
      release()
    }
  }

  compare (a, b) {
    const diff = a.mtime - b.mtime
    return diff > 0 ? 1 : (diff < 0 ? -1 : 0)
  }

  async * list (folder, opts = {}) {
    if (typeof folder === 'object') {
      opts = folder
      folder = undefined
    }

    const ignore = opts.ignore ? [].concat(opts.ignore).map(e => unixPathResolve('/', e)) : []
    const { keyname, filename: fulldir } = this._resolve(folder || '/')
    const iterator = await opendir(fulldir)

    if (!iterator) return

    for await (const dirent of iterator) {
      const key = unixPathResolve(keyname, dirent.name)

      if (ignore.includes(key)) continue

      if (dirent.isDirectory()) {
        yield * this.list(key, opts)
        continue
      }

      const entry = await this.entry(key)
      if (entry) yield entry
    }
  }

  async * readdir (folder) {
    const { keyname, filename: fulldir } = this._resolve(folder || '/')
    const iterator = await readdir(fulldir)

    if (!iterator) return

    for await (const dirent of iterator) {
      const key = unixPathResolve(keyname, dirent.name)

      let suffix = key.slice(keyname.length)
      const i = suffix.indexOf('/')
      if (i > -1) suffix = suffix.slice(i + 1)

      if (dirent.isDirectory()) {
        if (!(await isEmptyDirectory(this, key))) {
          yield suffix
        }
        continue
      }

      const entry = await this.entry(key)
      if (entry) yield suffix
    }
  }

  mirror (out, opts) {
    return new MirrorDrive(this, out, opts)
  }

  createReadStream (key, opts) {
    if (typeof key === 'object') key = key.key

    const { filename } = this._resolve(key)
    return new FileReadStream(filename, opts)
  }

  createWriteStream (key, opts) {
    const { keyname, filename } = this._resolve(key)
    return new FileWriteStream(filename, keyname, this, opts)
  }

  _alloc (filename) {
    if (!this._atomics) return filename
    let c = 0
    while (this._atomics.has(filename + '.' + c + '.localdrive.tmp')) c++
    filename += '.' + c + '.localdrive.tmp'
    this._atomics.add(filename)
    return filename
  }

  _free (atomicFilename) {
    this._atomics.delete(atomicFilename)
  }
}

function handleMetadataHooks (metadata) {
  if (metadata instanceof Map) {
    return {
      get: (key) => metadata.has(key) ? metadata.get(key) : null,
      put: (key, value) => metadata.set(key, value),
      del: (key) => metadata.delete(key)
    }
  }

  return metadata
}

function isExecutable (mode) {
  return !!(mode & fs.constants.S_IXUSR)
}

async function lstat (filename) {
  try {
    return await fsp.lstat(filename)
  } catch {
    return null
  }
}

async function stat (filename) {
  try {
    return await fsp.stat(filename)
  } catch {
    return null
  }
}

async function opendir (dir) {
  try {
    return await fsp.opendir(dir)
  } catch {
    return null
  }
}

async function readdir (dir) {
  try {
    return await fsp.readdir(dir, { withFileTypes: true })
  } catch {
    return null
  }
}

async function gcEmptyFolders (root, dir) {
  try {
    while (dir !== root) {
      await fsp.rmdir(dir)
      dir = path.dirname(dir)
    }
  } catch {
    // silent error
  }
}

async function isEmptyDirectory (drive, key) {
  for await (const entry of drive.list(key)) { // eslint-disable-line
    return false
  }
  return true
}
{
  "name": "localdrive",
  "version": "1.12.2",
  "description": "File system interoperable with Hyperdrive",
  "main": "index.js",
  "files": [
    "index.js",
    "streams.js"
  ],
  "imports": {
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    },
    "fs/*": {
      "bare": "bare-fs/*",
      "default": "fs/*"
    },
    "path": {
      "bare": "bare-path",
      "default": "path"
    }
  },
  "scripts": {
    "test": "standard && brittle test/*.js && brittle test/*.js --relative-tmp-dir",
    "lint": "standard"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/localdrive.git"
  },
  "author": "Lucas Barrena (@LuKks)",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/localdrive/issues"
  },
  "homepage": "https://github.com/holepunchto/localdrive",
  "dependencies": {
    "b4a": "^1.6.1",
    "bare-fs": "^4.0.1",
    "bare-path": "^3.0.0",
    "mirror-drive": "^1.2.0",
    "mutexify": "^1.4.0",
    "streamx": "^2.12.5",
    "unix-path-resolve": "^1.0.2"
  },
  "devDependencies": {
    "brittle": "^3.1.0",
    "standard": "^17.0.0"
  }
}
const { Readable, Writable } = require('streamx')
const fs = require('fs')
const fsp = require('fs/promises')
const path = require('path')
const b4a = require('b4a')

class FileWriteStream extends Writable {
  constructor (filename, key, drive, opts = {}) {
    super({ map })

    this.filename = filename
    this.atomicFilename = this.filename
    this.key = key
    this.drive = drive
    this.executable = !!opts.executable
    this.metadata = opts.metadata || null
    this.fd = 0
  }

  _open (cb) {
    this._openp().then(cb, cb)
  }

  _final (cb) {
    this._finalp().then(cb, cb)
  }

  _destroy (cb) {
    this._destroyp().then(cb, cb)
  }

  async _openp () {
    this.atomicFilename = this.drive._alloc(this.filename)

    const release = await this.drive._lock()
    const mode = this.executable ? 0o744 : 0o644

    try {
      await fsp.mkdir(path.dirname(this.filename), { recursive: true })
      this.fd = await openFilePromise(this.atomicFilename, fs.constants.O_WRONLY | fs.constants.O_CREAT | fs.constants.O_TRUNC | fs.constants.O_APPEND, mode)
    } finally {
      release()
    }

    const st = await fstatPromise(this.fd)
    if (this.executable !== !!(st.mode & fs.constants.S_IXUSR)) {
      await fchmodPromise(this.fd, mode)
    }
  }

  _writev (datas, cb) {
    fs.writev(this.fd, datas, cb)
  }

  async _destroyp (cb) {
    if (this.fd) await closeFilePromise(this.fd)

    if (this.atomicFilename !== this.filename) {
      await unlinkSafe(this.atomicFilename)
      this._free()
    }
  }

  async _finalp () {
    if (this.metadata === null) {
      if (this.drive.metadata.del) {
        await this.drive.metadata.del(this.key)
      }
    } else if (this.drive.metadata.put) {
      await this.drive.metadata.put(this.key, this.metadata)
    }

    const fd = this.fd
    this.fd = 0
    await closeFilePromise(fd)

    if (this.atomicFilename !== this.filename) {
      await renameFilePromise(this.atomicFilename, this.filename)
      this._free()
    }
  }

  _free () {
    if (this.atomicFilename === this.filename) return
    this.drive._free(this.atomicFilename)
    this.atomicFilename = this.filename
  }
}

class FileReadStream extends Readable {
  constructor (filename, opts = {}) {
    super()

    this.filename = filename
    this.fd = 0

    this._offset = opts.start || 0
    this._missing = 0

    if (opts.length) this._missing = opts.length
    else if (typeof opts.end === 'number') this._missing = opts.end - this._offset + 1
    else this._missing = -1
  }

  _open (cb) {
    fs.open(this.filename, fs.constants.O_RDONLY, (err, fd) => {
      if (err) return cb(err)

      const onerror = (err) => fs.close(fd, () => cb(err))

      fs.fstat(fd, (err, st) => {
        if (err) return onerror(err)
        if (!st.isFile()) return onerror(new Error(this.filename + ' is not a file'))

        this.fd = fd
        if (this._missing === -1) this._missing = st.size

        if (st.size < this._offset) {
          this._offset = st.size
          this._missing = 0
          return cb(null)
        }
        if (st.size < this._offset + this._missing) {
          this._missing = st.size - this._offset
          return cb(null)
        }

        cb(null)
      })
    })
  }

  _read (cb) {
    if (!this._missing) {
      this.push(null)
      return cb(null)
    }

    const data = b4a.allocUnsafe(Math.min(this._missing, 65536))

    fs.read(this.fd, data, 0, data.byteLength, this._offset, (err, read) => {
      if (err) return cb(err)

      if (!read) {
        this.push(null)
        return cb(null)
      }

      if (this._missing < read) read = this._missing
      this.push(data.subarray(0, read))
      this._missing -= read
      this._offset += read
      if (!this._missing) this.push(null)

      cb(null)
    })
  }

  _destroy (cb) {
    if (!this.fd) return cb(null)
    fs.close(this.fd, () => cb(null))
  }
}

module.exports = { FileWriteStream, FileReadStream }

function map (s) {
  return typeof s === 'string' ? b4a.from(s) : s
}

function openFilePromise (filename, flags, mode) {
  return new Promise((resolve, reject) => {
    fs.open(filename, flags, mode, function (error, fd) {
      if (error) reject(error)
      else resolve(fd)
    })
  })
}

function fstatPromise (fd) {
  return new Promise((resolve, reject) => {
    fs.fstat(fd, function (error, stats) {
      if (error) reject(error)
      else resolve(stats)
    })
  })
}

function fchmodPromise (fd, mode) {
  return new Promise((resolve, reject) => {
    fs.fchmod(fd, mode, function (error) {
      if (error) reject(error)
      else resolve()
    })
  })
}

function closeFilePromise (fd) {
  return new Promise((resolve, reject) => {
    fs.close(fd, function (error) {
      if (error) reject(error)
      else resolve()
    })
  })
}

function renameFilePromise (oldPath, newPath) {
  return new Promise((resolve, reject) => {
    fs.rename(oldPath, newPath, function (err) {
      if (err) reject(err)
      else resolve()
    })
  })
}

async function unlinkSafe (filename) {
  try {
    await fsp.unlink(filename)
  } catch (err) {
    if (err.code !== 'ENOENT') throw err
  }
}
const sameData = require('same-data')
const unixPathResolve = require('unix-path-resolve')
const streamEquals = require('binary-stream-equals')
const { pipelinePromise, isStream } = require('streamx')

module.exports = class MirrorDrive {
  constructor (src, dst, opts = {}) {
    this.src = src
    this.dst = dst

    this.prefix = opts.prefix || '/'
    this.dryRun = !!opts.dryRun
    this.prune = opts.prune !== false
    this.includeEquals = !!opts.includeEquals
    this.filter = opts.filter || null
    this.metadataEquals = opts.metadataEquals || null
    this.batch = !!opts.batch
    this.entries = opts.entries || null
    this.transformers = opts.transformers || []

    this.count = { files: 0, add: 0, remove: 0, change: 0 }
    this.bytesRemoved = 0
    this.bytesAdded = 0
    this.iterator = this._mirror()
    this._ignore = opts.ignore ? toIgnoreFunction(opts.ignore) : null
  }

  [Symbol.asyncIterator] () {
    return this.iterator
  }

  async done () {
    while (true) {
      const { done } = await this.iterator.next()
      if (done) break
    }
  }

  async * _mirror () {
    await this.src.ready()
    await this.dst.ready()

    if (this.dst.core && !this.dst.core.writable) throw new Error('Destination must be writable')

    const dst = this.batch ? this.dst.batch() : this.dst

    if (this.prune) {
      for await (const [key, dstEntry, srcEntry] of this._list(this.dst, this.src)) {
        if (srcEntry) continue

        this.count.remove++
        this.bytesRemoved += blobLength(dstEntry)
        yield { op: 'remove', key, bytesRemoved: blobLength(dstEntry), bytesAdded: 0 }

        if (!this.dryRun) await dst.del(key)
      }
    }

    if (this.src.download && !this.entries) {
      const dl = this.src.download(this.prefix)
      if (dl.catch) dl.catch(noop)
    }

    for await (const [key, srcEntry, dstEntry] of this._list(this.src, dst, { filter: this.filter })) {
      if (!srcEntry) continue // Due entries option, src entry might not exist probably because it was pruned

      this.count.files++

      // If transformers are provided, we can't know if same before running them
      const hasTransformers = this.transformers && this.transformers.length > 0

      const isSame = hasTransformers === false && await same(this, srcEntry, dstEntry)

      if (isSame) {
        if (this.includeEquals) {
          yield { op: 'equal', key, bytesRemoved: 0, bytesAdded: 0 }
        }
        continue
      }

      if (dstEntry) {
        this.count.change++
        this.bytesRemoved += blobLength(dstEntry)
        this.bytesAdded += blobLength(srcEntry)
        yield { op: 'change', key, bytesRemoved: blobLength(dstEntry), bytesAdded: blobLength(srcEntry) }
      } else {
        this.count.add++
        this.bytesAdded += blobLength(srcEntry)
        yield { op: 'add', key, bytesRemoved: 0, bytesAdded: blobLength(srcEntry) }
      }

      if (this.dryRun) {
        continue
      }

      const transformers = []

      for (const transformer of this.transformers) {
        if (typeof transformer !== 'function') throw new Error('transformer must be a function')

        const stream = transformer(key)

        if (stream === null) continue
        if (!isStream(stream)) throw new Error('transformer must return a stream')

        transformers.push(stream)
      }

      if (srcEntry.value.linkname) {
        await dst.symlink(key, srcEntry.value.linkname)
      } else {
        await pipelinePromise(
          this.src.createReadStream(srcEntry),
          ...transformers,
          dst.createWriteStream(key, { executable: srcEntry.value.executable, metadata: srcEntry.value.metadata })
        )
      }
    }

    if (this.batch) await dst.flush()
  }

  async * _list (a, b, opts) {
    const list = this.entries || a.list(this.prefix, { ignore: this._ignore })

    for await (const entry of list) {
      const key = typeof entry === 'object' ? entry.key : entry

      if (opts && opts.filter && !opts.filter(key)) continue

      const entryA = await a.entry(entry)
      const entryB = await b.entry(key)

      yield [key, entryA, entryB]
    }
  }
}

function blobLength (entry) {
  return entry.value.blob ? entry.value.blob.byteLength : 0
}

async function same (m, srcEntry, dstEntry) {
  if (!dstEntry) return false

  if (srcEntry.value.linkname || dstEntry.value.linkname) {
    return srcEntry.value.linkname === dstEntry.value.linkname
  }

  if (srcEntry.value.executable !== dstEntry.value.executable) return false

  if (!sizeEquals(srcEntry, dstEntry)) return false

  if (!metadataEquals(m, srcEntry, dstEntry)) return false

  return streamEquals(m.src.createReadStream(srcEntry), m.dst.createReadStream(dstEntry))
}

function sizeEquals (srcEntry, dstEntry) {
  const srcBlob = srcEntry.value.blob
  const dstBlob = dstEntry.value.blob

  if (!srcBlob && !dstBlob) return true
  if (!srcBlob || !dstBlob) return false

  return srcBlob.byteLength === dstBlob.byteLength
}

function metadataEquals (m, srcEntry, dstEntry) {
  if (!m.src.supportsMetadata || !m.dst.supportsMetadata) return true

  const srcMetadata = srcEntry.value.metadata
  const dstMetadata = dstEntry.value.metadata

  if (m.metadataEquals) {
    return m.metadataEquals(srcMetadata, dstMetadata)
  }

  const noMetadata = !srcMetadata && !dstMetadata
  const identicalMetadata = !!(srcMetadata && dstMetadata && sameData(srcMetadata, dstMetadata))

  return noMetadata || identicalMetadata
}

function toIgnoreFunction (ignore) {
  if (typeof ignore === 'function') return ignore

  const all = [].concat(ignore).map(e => unixPathResolve('/', e))
  return key => all.some(path => path === key || key.startsWith(path + '/'))
}

function noop () {}
{
  "name": "mirror-drive",
  "version": "1.6.0",
  "description": "Mirror a hyperdrive or localdrive into another one",
  "main": "index.js",
  "files": [
    "index.js",
    "lib/**.js"
  ],
  "scripts": {
    "test": "standard && brittle test/*.js",
    "lint": "standard"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/mirror-drive.git"
  },
  "author": "Lucas Barrena (@LuKks)",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/mirror-drive/issues"
  },
  "homepage": "https://github.com/holepunchto/mirror-drive",
  "devDependencies": {
    "b4a": "^1.6.0",
    "brittle": "^3.1.0",
    "corestore": "^6.0.6",
    "hyperdrive": "^12.2.0",
    "localdrive": "^2.2.0",
    "standard": "^17.0.0"
  },
  "dependencies": {
    "binary-stream-equals": "^1.0.0",
    "same-data": "^1.0.0",
    "streamx": "^2.22.1",
    "unix-path-resolve": "^1.0.2"
  }
}
var queueTick = require('queue-tick')

var mutexify = function () {
  var queue = []
  var used = null

  var call = function () {
    used(release)
  }

  var acquire = function (fn) {
    if (used) return queue.push(fn)
    used = fn
    acquire.locked = true
    queueTick(call)
    return 0
  }

  acquire.locked = false

  var release = function (fn, err, value) {
    used = null
    acquire.locked = false
    if (queue.length) acquire(queue.shift())
    if (fn) fn(err, value)
  }

  return acquire
}

module.exports = mutexify
{
  "name": "mutexify",
  "version": "1.4.0",
  "description": "mutex lock for javascript",
  "main": "index.js",
  "dependencies": {
    "queue-tick": "^1.0.0"
  },
  "devDependencies": {
    "standard": "^14.3.3",
    "tape": "^3.0.2"
  },
  "scripts": {
    "test": "tape test.js",
    "posttest": "npm run lint",
    "lint": "standard"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/mutexify.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/mutexify/issues"
  },
  "keywords": [
    "mutex",
    "lock"
  ],
  "homepage": "https://github.com/mafintosh/mutexify"
}
var mutexify = require('.')

var mutexifyPromise = function () {
  var lock = mutexify()

  var acquire = function () {
    return new Promise(lock)
  }

  Object.defineProperty(acquire, 'locked', {
    get: function () { return lock.locked },
    enumerable: true
  })

  return acquire
}

module.exports = mutexifyPromise
'use strict'
const ReadyResource = require('ready-resource')
const ref = require('pear-ref')

module.exports = class AppDrive extends ReadyResource {
  #ipc = global.Pear?.[global.Pear?.constructor.IPC]
  constructor () {
    super()
    if (!this.#ipc) throw new Error('pear-appdrive is designed for Pear - IPC missing')
  }

  _open () {
    ref.ref()
  }

  _close () {
    ref.unref()
  }

  batch () { return this }

  checkout () { return this }

  get = (key, opts = {}) => ref.track(this.#ipc.get({ key, ...opts }))

  exists = (key) => ref.track(this.#ipc.exists({ key }))

  entry = (key) => ref.track(this.#ipc.entry({ key }))

  compare = (keyA, keyB) => ref.track(this.#ipc.compare({ keyA, keyB }))

  list = (key, opts = {}) => ref.track(this.#ipc.list({ key, ...opts }))

  put () { throw Error('not implemented') }

  del () { throw Error('not implemented') }

  symlink () { throw Error('not implemented') }

  readdir () { throw Error('not implemented') }

  mirror () { throw Error('not implemented') }

  createReadStream () { throw Error('not implemented') }

  createWriteStream () { throw Error('not implemented') }
}
{
  "name": "pear-appdrive",
  "version": "1.1.1",
  "main": "index.js",
  "type": "commonjs",
  "description": "Pear application drive class, minimal hyperdrive compat, read-only",
  "license": "Apache-2.0",
  "files": [],
  "scripts": {
    "test:gen": "brittle -r test/all.js test/*.test.js",
    "test": "brittle-bare --coverage test/all.js",
    "lint": "standard"
  },
  "devDependencies": {
    "b4a": "^1.6.7",
    "brittle": "^3.0.0",
    "sodium-native": "^5.0.6",
    "standard": "^17.1.2",
    "which-runtime": "^1.3.0"
  },
  "dependencies": {
    "pear-ipc": "^6.6.1",
    "pear-ref": "^1.0.1",
    "ready-resource": "^1.1.2"
  }
}
'use strict'
const sodium = require('sodium-native')
const pack = require('bare-pack-drive')
const unpack = require('bare-unpack')
const lex = require('bare-module-lexer')
const traverse = require('bare-module-traverse')

module.exports = async function pearPack(drive, opts = {}) {
  const {
    entry = '/boot.js',
    hosts,
    builtins,
    imports,
    prebuildPrefix = '',
    conditions: conds = ['node', 'bare'],
    extensions: exts = ['.node', '.bare']
  } = opts
  const bundle = await pack(drive, entry, {
    resolve,
    hosts,
    builtins,
    imports
  })
  const prebuilds = new Map()
  const rebundle = await unpack(
    bundle,
    { addons: true, files: false },
    async (key) => {
      const extIx = key.lastIndexOf('.')
      if (extIx === -1) return key
      const extname = key.slice(extIx)
      if (extname !== '.node' && extname !== '.bare') return key
      const hash = Buffer.allocUnsafe(32)
      const addon = await drive.get(key)
      sodium.crypto_generichash(hash, addon)
      const prebuildsPath = key.slice(key.indexOf('/prebuilds/'))
      const prebuild =
        prebuildsPath.slice(0, prebuildsPath.lastIndexOf('/') + 1) +
        hash.toString('hex') +
        extname
      prebuilds.set(prebuild, addon)
      return prebuildPrefix + prebuild
    }
  )
  return {
    bundle: rebundle.toBuffer(),
    prebuilds: prebuilds
  }
  function resolve(entry, parentURL, opts = {}) {
    let extensions
    let conditions = opts.hosts.map((host) => [...conds, ...host.split('-')])

    if (entry.type & lex.constants.ADDON) {
      extensions = [...exts]
      conditions = conditions.map((conditions) => ['addon', ...conditions])

      return traverse.resolve.addon(entry.specifier || '.', parentURL, {
        extensions,
        conditions,
        hosts: opts.hosts,
        linked: false,
        ...opts
      })
    }

    if (entry.type & lex.constants.ASSET) {
      conditions = conditions.map((conditions) => ['asset', ...conditions])
    } else {
      extensions = ['.js', '.cjs', '.mjs', '.json', ...exts]

      if (entry.type & lex.constants.REQUIRE) {
        conditions = conditions.map((conditions) => ['require', ...conditions])
      } else if (entry.type & lex.constants.IMPORT) {
        conditions = conditions.map((conditions) => ['import', ...conditions])
      }
    }
    return traverse.resolve.module(entry.specifier, parentURL, {
      extensions,
      conditions,
      ...opts
    })
  }
}
{
  "name": "pear-pack",
  "version": "1.1.2",
  "description": "",
  "main": "index.js",
  "scripts": {
    "format": "prettier --write .",
    "lint": "prettier --check .",
    "test": "brittle-bare --coverage test/index.test.js"
  },
  "keywords": [],
  "author": "",
  "license": "Apache-2.0",
  "dependencies": {
    "bare-module-lexer": "^1.2.0",
    "bare-module-traverse": "^1.4.0",
    "bare-pack-drive": "^1.0.2",
    "bare-unpack": "^1.0.2",
    "sodium-native": "^5.0.9"
  },
  "devDependencies": {
    "bare-path": "^3.0.0",
    "brittle": "^3.19.0",
    "localdrive": "^2.2.0",
    "prettier": "^3.6.2",
    "prettier-config-holepunch": "^1.0.0"
  },
  "imports": {
    "path": {
      "bare": "bare-path",
      "default": "path"
    }
  }
}
'use strict'
const EventEmitter = require('events')

class Ref extends EventEmitter {
  refs = 0 // read-only

  constructor () {
    if (!global.Pear) throw new Error('pear-ref is designed for use with Pear')
    const REF = global.Pear?.[global.Pear?.constructor.REF]
    if (REF) return REF // all apps use the same ref counter
    super()
  }

  ref () {
    this.refs++
    this.emit('ref', this.refs)
  }

  async track (promise) {
    this.ref()
    try {
      return await promise
    } finally {
      this.unref()
    }
  }

  unref () {
    this.refs--
    this.emit('unref', this.refs)
  }
}

module.exports = new Ref()
{
  "name": "pear-ref",
  "version": "1.0.2",
  "main": "index.js",
  "type": "commonjs",
  "description": "Pear core ref counter",
  "license": "Apache-2.0",
  "files": [],
  "scripts": {
    "test:gen": "brittle -r test/all.js test/*.test.js",
    "test": "brittle-bare --coverage test/all.js",
    "lint": "standard"
  },
  "imports": {
    "events": {
      "bare": "bare-events"
    }
  },
  "devDependencies": {
    "brittle": "^3.0.0",
    "standard": "^17.1.2"
  },
  "dependencies": {
    "bare-events": "^2.6.1"
  }
}
const DONE = Promise.resolve(true)
const DESTROYED = Promise.resolve(false)

module.exports = class Semaphore {
  constructor (limit = 1) {
    this.limit = limit
    this.active = 0
    this.waiting = []
    this.destroyed = false

    this._onwait = (resolve) => { this.waiting.push(resolve) }
  }

  wait () {
    if (this.destroyed === true) return DESTROYED

    if (this.active < this.limit && this.waiting.length === 0) {
      this.active++
      return DONE
    }

    return new Promise(this._onwait)
  }

  signal () {
    if (this.destroyed === true) return

    this.active--
    while (this.active < this.limit && this.waiting.length > 0 && this.destroyed === false) {
      this.active++
      this.waiting.shift()(true)
    }
  }

  async flush () {
    if (this.destroyed === true) return
    this.limit = 1
    await this.wait()
    this.signal()
  }

  destroy () {
    this.destroyed = true
    this.active = 0
    while (this.waiting.length) this.waiting.pop()(false)
  }
}
{
  "name": "promaphore",
  "version": "1.0.0",
  "description": "Promise Semaphore",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {},
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/promaphore.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/promaphore/issues"
  },
  "homepage": "https://github.com/mafintosh/promaphore"
}
{
  "name": "queue-tick",
  "version": "1.0.1",
  "description": "Next tick shim that prefers process.nextTick over queueMicrotask for compat",
  "main": "./process-next-tick.js",
  "browser": "./queue-microtask.js",
  "dependencies": {},
  "devDependencies": {
    "standard": "^16.0.3",
    "tape": "^5.3.1"
  },
  "scripts": {
    "test": "standard && tape test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/queue-tick.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/queue-tick/issues"
  },
  "homepage": "https://github.com/mafintosh/queue-tick"
}
module.exports = (typeof process !== 'undefined' && typeof process.nextTick === 'function')
  ? process.nextTick.bind(process)
  : require('./queue-microtask')
module.exports = typeof queueMicrotask === 'function' ? queueMicrotask : (fn) => Promise.resolve().then(fn)
const EventEmitter = require('events')

module.exports = class ReadyResource extends EventEmitter {
  constructor () {
    super()

    this.opening = null
    this.closing = null

    this.opened = false
    this.closed = false
  }

  ready () {
    if (this.opening !== null) return this.opening
    this.opening = open(this)
    return this.opening
  }

  close () {
    if (this.closing !== null) return this.closing
    this.closing = close(this)
    return this.closing
  }

  async _open () {
    // add impl here
  }

  async _close () {
    // add impl here
  }
}

async function open (self) {
  // open after close
  if (self.closing !== null) return

  try {
    await self._open()
  } catch (err) {
    self.close() // safe to run in bg
    throw err
  }

  self.opened = true
  self.emit('ready')
}

async function close (self) {
  try {
    if (self.opened === false && self.opening !== null) await self.opening
  } catch {
    // ignore errors on closing
  }
  if (self.opened === true || self.opening === null) await self._close()
  self.closed = true
  self.emit('close')
}
{
  "name": "ready-resource",
  "version": "1.2.0",
  "description": "Modern single resource management",
  "main": "index.js",
  "imports": {
    "events": {
      "bare": "bare-events",
      "default": "events"
    }
  },
  "types": "index.d.ts",
  "files": [
    "index.js",
    "index.d.ts"
  ],
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "devDependencies": {
    "brittle": "^3.1.0",
    "standard": "^17.0.0"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/ready-resource.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/holepunchto/ready-resource/issues"
  },
  "homepage": "https://github.com/holepunchto/ready-resource",
  "dependencies": {
    "bare-events": "^2.2.0"
  }
}
const runtime = require('./lib/runtime')

if (runtime === 'bare') {
  module.exports = require('./lib/runtime/bare')
} else if (runtime === 'node') {
  module.exports = require('./lib/runtime/node')
} else {
  module.exports = require('./lib/runtime/default')
}
module.exports =
  typeof Bare !== 'undefined'
    ? 'bare'
    : typeof process !== 'undefined'
      ? 'node'
      : 'unknown'
module.exports = require.addon.bind(require)
if (typeof require.addon === 'function') {
  module.exports = require.addon.bind(require)
} else {
  module.exports = function addon(specifier, parentURL) {
    throw new Error(
      `Cannot find addon '${specifier}' imported from '${parentURL}'`
    )
  }
}
if (typeof require.addon === 'function') {
  module.exports = require.addon.bind(require)
} else {
  const url = require('url')
  const resolve = require('bare-addon-resolve')

  const host = process.platform + '-' + process.arch
  const conditions = ['node', process.platform, process.arch]
  const extensions = ['.node']

  module.exports = function addon(specifier, parentURL) {
    if (typeof parentURL === 'string') parentURL = url.pathToFileURL(parentURL)

    for (const resolution of resolve(
      specifier,
      parentURL,
      { host, conditions, extensions },
      readPackage
    )) {
      switch (resolution.protocol) {
        case 'file:':
          try {
            return require(url.fileURLToPath(resolution))
          } catch {
            continue
          }
      }
    }

    throw new Error(
      `Cannot find addon '${specifier}' imported from '${parentURL.href}'`
    )

    function readPackage(packageURL) {
      try {
        return require(url.fileURLToPath(packageURL))
      } catch (err) {
        return null
      }
    }
  }
}
{
  "name": "require-addon",
  "version": "1.1.0",
  "description": "Import native addons across JavaScript runtimes",
  "exports": {
    ".": "./index.js",
    "./package": "./package.json"
  },
  "imports": {
    "fs": {
      "bare": "bare-fs",
      "default": "fs"
    },
    "path": {
      "bare": "bare-path",
      "default": "path"
    },
    "url": {
      "bare": "bare-url",
      "default": "url"
    }
  },
  "files": [
    "index.js",
    "lib"
  ],
  "scripts": {
    "test": "npm run lint && npm run test:bare && npm run test:node",
    "test:bare": "bare test.js",
    "test:node": "node test.js",
    "lint": "prettier . --check"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/require-addon.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/require-addon/issues"
  },
  "homepage": "https://github.com/holepunchto/require-addon#readme",
  "engines": {
    "bare": ">=1.10.0"
  },
  "dependencies": {
    "bare-addon-resolve": "^1.3.0",
    "bare-url": "^2.1.0"
  },
  "devDependencies": {
    "bare-bundle": "^1.8.1",
    "bare-bundle-evaluate": "^1.1.0",
    "bare-fs": "^4.0.0",
    "bare-path": "^3.0.0",
    "brittle": "^3.7.0",
    "prettier": "^3.4.1",
    "prettier-config-standard": "^7.0.0"
  }
}
module.exports = sameData

function type (o) {
  const t = typeof o

  return t === 'object'
    ? Array.isArray(o)
      ? 'array'
      : isTypedArray(o)
        ? (typeof o.equals === 'function') ? 'buffer' : 'array'
        : (o === null ? 'null' : 'object')
    : t
}

function isTypedArray (a) {
  return !!a && typeof a.length === 'number' && ArrayBuffer.isView(a.array)
}

function sameData (a, b) {
  if (a === b) return true

  const ta = type(a)
  const tb = type(b)

  if (ta !== tb) return false

  if (ta === 'buffer') return a.equals(b)

  if (ta === 'array') {
    if (a.length !== b.length) return false

    for (let i = 0; i < a.length; i++) {
      if (!sameData(a[i], b[i])) return false
    }

    return true
  }

  if (ta !== 'object') return false

  const ea = Object.entries(a)
  const eb = Object.entries(b)

  if (ea.length !== eb.length) return false

  ea.sort(cmp)
  eb.sort(cmp)

  for (let i = 0; i < ea.length; i++) {
    if (ea[i][0] !== eb[i][0] || !sameData(ea[i][1], eb[i][1])) return false
  }

  return true
}

function cmp (a, b) {
  return a[0] === b[0] ? 0 : a[0] < b[0] ? -1 : 1
}
{
  "name": "same-data",
  "version": "1.0.0",
  "description": "Deep equal with no deps and only for 'data' objects, ie basic objects, arrays, primitives and typed arrays",
  "main": "index.js",
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "devDependencies": {
    "brittle": "^3.1.1",
    "standard": "^17.0.0"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/same-data.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/same-data/issues"
  },
  "homepage": "https://github.com/mafintosh/same-data"
}
require.addon = require('require-addon')
module.exports = require.addon('.', __filename)
const binding = require('./binding')
const { isNode } = require('which-runtime')

const OPTIONAL = Buffer.from(new ArrayBuffer(0))

module.exports = exports = { ...binding }

// memory

exports.sodium_memzero = function (buf) {
  binding.sodium_memzero(buf)
}

exports.sodium_mlock = function (buf) {
  const res = binding.sodium_mlock(buf)
  if (res !== 0) throw new Error('memory lock failed')
}

exports.sodium_munlock = function (buf) {
  const res = binding.sodium_munlock(buf)
  if (res !== 0) throw new Error('memory unlock failed')
}

exports.sodium_malloc = function (size) {
  if (size < 0) throw new Error('invalid size')
  const buf = Buffer.from(binding.sodium_malloc(size))
  buf.secure = true

  return buf
}

exports.sodium_free = function (buf) {
  if (!buf?.secure) return

  binding.sodium_free(buf.buffer)
}

exports.sodium_mprotect_noaccess = function (buf) {
  const res = binding.sodium_mprotect_noaccess(buf.buffer)

  if (res !== 0) throw new Error('failed to lock buffer')
}

exports.sodium_mprotect_readonly = function (buf) {
  const res = binding.sodium_mprotect_readonly(buf.buffer)

  if (res !== 0) throw new Error('failed to unlock buffer')
}

exports.sodium_mprotect_readwrite = function (buf) {
  const res = binding.sodium_mprotect_readwrite(buf.buffer)

  if (res !== 0) throw new Error('failed to unlock buffer')
}

// crypto_randombytes

exports.randombytes_buf = function (buffer) {
  binding.randombytes_buf(
    buffer.buffer,
    buffer.byteOffset,
    buffer.byteLength
  )
}

exports.randombytes_buf_deterministic = function (buffer, seed) {
  binding.randombytes_buf_deterministic(
    buffer.buffer,
    buffer.byteOffset,
    buffer.byteLength,

    seed.buffer,
    seed.byteOffset,
    seed.byteLength
  )
}

// sodium_helpers

exports.sodium_memcmp = function (a, b) {
  if (a?.byteLength !== b?.byteLength) throw new Error('buffers must be of same length"')
  return binding.sodium_memcmp(a, b)
}

exports.sodium_add = function (a, b) {
  if (a?.byteLength !== b?.byteLength) throw new Error('buffers must be of same length"')
  binding.sodium_add(a, b)
}

exports.sodium_sub = function (a, b) {
  if (a?.byteLength !== b?.byteLength) throw new Error('buffers must be of same length"')
  binding.sodium_sub(a, b)
}

exports.sodium_compare = function (a, b) {
  if (a?.byteLength !== b?.byteLength) throw new Error('buffers must be of same length"')
  return binding.sodium_compare(a, b)
}

exports.sodium_is_zero = function (buffer, length) {
  if (!buffer) throw new Error('invalid buffer')
  length ??= buffer.byteLength
  if (length > buffer.byteLength || length < 0) throw new Error('invalid length')

  return binding.sodium_is_zero(buffer, length)
}

exports.sodium_pad = function (buffer, unpaddedBuflen, blockSize) {
  if (unpaddedBuflen > buffer.byteLength) throw new Error('unpadded length cannot exceed buffer length')
  if (blockSize > buffer.byteLength) throw new Error('block size cannot exceed buffer length')
  if (blockSize < 1) throw new Error('block sizemust be at least 1 byte')
  if (buffer?.byteLength < unpaddedBuflen + (blockSize - (unpaddedBuflen % blockSize))) throw new Error('buf not long enough')

  return binding.sodium_pad(buffer, unpaddedBuflen, blockSize)
}

exports.sodium_unpad = function (buffer, paddedBuflen, blockSize) {
  if (paddedBuflen > buffer.byteLength) throw new Error('unpadded length cannot exceed buffer length')
  if (blockSize > buffer.byteLength) throw new Error('block size cannot exceed buffer length')
  if (blockSize < 1) throw new Error('block size must be at least 1 byte')

  return binding.sodium_unpad(buffer, paddedBuflen, blockSize)
}

// crypto_sign

exports.crypto_sign_keypair = function (pk, sk) {
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('pk')

  const res = binding.crypto_sign_keypair(pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_seed_keypair = function (pk, sk, seed) {
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('pk')

  const res = binding.crypto_sign_seed_keypair(pk, sk, seed)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign = function (sm, m, sk) {
  if (sm?.byteLength !== binding.crypto_sign_BYTES + m.byteLength) throw new Error('sm must be "m.byteLength + crypto_sign_BYTES" bytes')
  if (sk?.byteLength !== binding.crypto_sign_SECRETKEYBYTES) throw new Error('sk')

  const res = binding.crypto_sign(sm, m, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_open = function (m, sm, pk) {
  if (sm?.byteLength < binding.crypto_sign_BYTES) throw new Error('sm')
  if (m?.byteLength !== sm.byteLength - binding.crypto_sign_BYTES) throw new Error('m must be "sm.byteLength - crypto_sign_BYTES" bytes')
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('pk')

  const res = binding.crypto_sign_open(m, sm, pk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_open = function (m, sm, pk) {
  if (sm?.byteLength < binding.crypto_sign_BYTES) throw new Error('sm')
  if (m?.byteLength !== sm.byteLength - binding.crypto_sign_BYTES) throw new Error('m must be "sm.byteLength - crypto_sign_BYTES" bytes')
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('pk')

  return binding.crypto_sign_open(m, sm, pk)
}

exports.crypto_sign_detached = function (sig, m, sk) {
  if (sig?.byteLength !== binding.crypto_sign_BYTES) throw new Error('sig')
  if (sk?.byteLength !== binding.crypto_sign_SECRETKEYBYTES) throw new Error('sk')

  const res = binding.crypto_sign_detached(sig, m, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_verify_detached = function (sig, m, pk) {
  return binding.crypto_sign_verify_detached(
    sig.buffer,
    sig.byteOffset,
    sig.byteLength,

    m.buffer,
    m.byteOffset,
    m.byteLength,

    pk.buffer,
    pk.byteOffset,
    pk.byteLength
  )
}

exports.crypto_sign_ed25519_sk_to_pk = function (pk, sk) {
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('pk')
  if (sk?.byteLength !== binding.crypto_sign_SECRETKEYBYTES) throw new Error('sk')

  const res = binding.crypto_sign_ed25519_sk_to_pk(pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_ed25519_pk_to_curve25519 = function (x25519pk, ed25519pk) {
  if (x25519pk?.byteLength !== binding.crypto_box_PUBLICKEYBYTES) throw new Error('x25519_pk')
  if (ed25519pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('ed25519_pk')

  const res = binding.crypto_sign_ed25519_pk_to_curve25519(x25519pk, ed25519pk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_sign_ed25519_sk_to_curve25519 = function (x25519sk, ed25519sk) {
  if (x25519sk?.byteLength !== binding.crypto_box_SECRETKEYBYTES) throw new Error('x25519_sk')

  const edLen = ed25519sk.byteLength

  if (edLen !== binding.crypto_sign_SECRETKEYBYTES && edLen !== binding.crypto_box_SECRETKEYBYTES) {
    throw new Error('ed25519_sk should either be \'crypto_sign_SECRETKEYBYTES\' bytes or \'crypto_sign_SECRETKEYBYTES - crypto_sign_PUBLICKEYBYTES\' bytes')
  }

  const res = binding.crypto_sign_ed25519_sk_to_curve25519(x25519sk, ed25519sk)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_box

exports.crypto_box_keypair = function (pk, sk) {
  if (pk?.byteLength !== binding.crypto_box_PUBLICKEYBYTES) throw new Error('pk')
  if (sk?.byteLength !== binding.crypto_box_SECRETKEYBYTES) throw new Error('sk')

  const res = binding.crypto_box_keypair(pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_box_seed_keypair = function (pk, sk, seed) {
  if (pk?.byteLength !== binding.crypto_box_PUBLICKEYBYTES) throw new Error('pk')
  if (sk?.byteLength !== binding.crypto_box_SECRETKEYBYTES) throw new Error('sk')

  const res = binding.crypto_box_seed_keypair(pk, sk, seed)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_box_easy = function (c, m, n, pk, sk) {
  const res = binding.crypto_box_easy(c, m, n, pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_box_detached = function (c, mac, m, n, pk, sk) {
  const res = binding.crypto_box_detached(c, mac, m, n, pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_box_seal = function (c, m, pk) {
  const res = binding.crypto_box_seal(c, m, pk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_box_seal_open = function (m, c, pk, sk) {
  return binding.crypto_box_seal_open(
    m.buffer,
    m.byteOffset,
    m.byteLength,

    c.buffer,
    c.byteOffset,
    c.byteLength,

    pk.buffer,
    pk.byteOffset,
    pk.byteLength,

    sk.buffer,
    sk.byteOffset,
    sk.byteLength
  )
}

// crypto_secretbox

exports.crypto_secretbox_easy = function (c, m, n, k) {
  if (c?.byteLength !== m.byteLength + binding.crypto_secretbox_MACBYTES) throw new Error('c must be "m.byteLength + crypto_secretbox_MACBYTES" bytes')
  if (n?.byteLength !== binding.crypto_secretbox_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_secretbox_KEYBYTES) throw new Error('k')

  const res = binding.crypto_secretbox_easy(c, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_secretbox_open_easy = function (m, c, n, k) {
  if (m?.byteLength !== c.byteLength - binding.crypto_secretbox_MACBYTES) throw new Error('m must be "c - crypto_secretbox_MACBYTES" bytes')
  if (c?.byteLength < binding.crypto_secretbox_MACBYTES) throw new Error('c')
  if (n?.byteLength !== binding.crypto_secretbox_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_secretbox_KEYBYTES) throw new Error('k')

  return binding.crypto_secretbox_open_easy(m, c, n, k)
}

exports.crypto_secretbox_detached = function (c, mac, m, n, k) {
  if (c?.byteLength !== m.byteLength) throw new Error('c must "m.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_secretbox_MACBYTES) throw new Error('mac')
  if (n?.byteLength !== binding.crypto_secretbox_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_secretbox_KEYBYTES) throw new Error('k')

  const res = binding.crypto_secretbox_detached(c, mac, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_secretbox_open_detached = function (m, c, mac, n, k) {
  if (m?.byteLength !== c.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_secretbox_MACBYTES) throw new Error('mac')
  if (n?.byteLength !== binding.crypto_secretbox_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_secretbox_KEYBYTES) throw new Error('k')

  return binding.crypto_secretbox_open_detached(m, c, mac, n, k)
}

// crypto_generichash

exports.crypto_generichash = function (output, input, key = OPTIONAL) {
  const res = binding.crypto_generichash(
    output.buffer,
    output.byteOffset,
    output.byteLength,

    input.buffer,
    input.byteOffset,
    input.byteLength,

    key.buffer,
    key.byteOffset,
    key.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_generichash_batch = function (output, batch, key) {
  if (isNode || batch.length < 4) {
    const res = binding.crypto_generichash_batch(output, batch, !!key, key || OPTIONAL)
    if (res !== 0) throw new Error('status: ' + res)
  } else {
    const state = Buffer.alloc(binding.crypto_generichash_STATEBYTES)

    exports.crypto_generichash_init(state, key, output.byteLength)

    for (const buf of batch) {
      exports.crypto_generichash_update(state, buf)
    }

    exports.crypto_generichash_final(state, output)
  }
}

exports.crypto_generichash_keygen = function (key) {
  const res = binding.crypto_generichash_keygen(
    key.buffer, key.byteOffset, key.byteLength
  )
  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_generichash_init = function (state, key, outputLength) {
  key ||= OPTIONAL

  const res = binding.crypto_generichash_init(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    key.buffer,
    key.byteOffset,
    key.byteLength,

    outputLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_generichash_update = function (state, input) {
  const res = binding.crypto_generichash_update(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    input.buffer,
    input.byteOffset,
    input.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_generichash_final = function (state, output) {
  const res = binding.crypto_generichash_final(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    output.buffer,
    output.byteOffset,
    output.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

// secretstream

exports.crypto_secretstream_xchacha20poly1305_keygen = function (k) {
  binding.crypto_secretstream_xchacha20poly1305_keygen(k.buffer, k.byteOffset, k.byteLength)
}

exports.crypto_secretstream_xchacha20poly1305_init_push = function (state, header, k) {
  const res = binding.crypto_secretstream_xchacha20poly1305_init_push(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    header.buffer,
    header.byteOffset,
    header.byteLength,

    k.buffer,
    k.byteOffset,
    k.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_secretstream_xchacha20poly1305_init_pull = function (state, header, k) {
  const res = binding.crypto_secretstream_xchacha20poly1305_init_pull(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    header.buffer,
    header.byteOffset,
    header.byteLength,

    k.buffer,
    k.byteOffset,
    k.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_secretstream_xchacha20poly1305_push = function (state, c, m, ad, tag) {
  ad ||= OPTIONAL

  const res = binding.crypto_secretstream_xchacha20poly1305_push(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    c.buffer,
    c.byteOffset,
    c.byteLength,

    m.buffer,
    m.byteOffset,
    m.byteLength,

    ad.buffer,
    ad.byteOffset,
    ad.byteLength,

    tag
  )

  if (res < 0) throw new Error('push failed')

  return res
}

exports.crypto_secretstream_xchacha20poly1305_pull = function (state, m, tag, c, ad) {
  ad ||= OPTIONAL

  if (c?.byteLength < binding.crypto_secretstream_xchacha20poly1305_ABYTES) throw new Error('invalid cipher length')
  if (m?.byteLength !== c.byteLength - binding.crypto_secretstream_xchacha20poly1305_ABYTES) throw new Error('invalid message length')

  const res = binding.crypto_secretstream_xchacha20poly1305_pull(
    state.buffer,
    state.byteOffset,
    state.byteLength,

    m.buffer,
    m.byteOffset,
    m.byteLength,

    tag.buffer,
    tag.byteOffset,
    tag.byteLength,

    c.buffer,
    c.byteOffset,
    c.byteLength,

    ad.buffer,
    ad.byteOffset,
    ad.byteLength
  )

  if (res < 0) throw new Error('pull failed')

  return res
}

exports.crypto_secretstream_xchacha20poly1305_rekey = function (state) {
  binding.crypto_secretstream_xchacha20poly1305_rekey(state.buffer, state.byteOffset, state.byteLength)
}

// crypto_stream

exports.crypto_stream = function (c, n, k) {
  if (n?.byteLength !== binding.crypto_stream_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream(c, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_xor = function (c, m, n, k) {
  const res = binding.crypto_stream_xor(
    c.buffer,
    c.byteOffset,
    c.byteLength,

    m.buffer,
    m.byteOffset,
    m.byteLength,

    n.buffer,
    n.byteOffset,
    n.byteLength,

    k.buffer,
    k.byteOffset,
    k.byteLength
  )

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20 = function (c, n, k) {
  if (n?.byteLength !== binding.crypto_stream_chacha20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_chacha20(c, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20_xor = function (c, m, n, k) {
  if (c?.byteLength !== m.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_chacha20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_chacha20_xor(c, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20_xor_ic = function (c, m, n, ic, k) {
  if (c?.byteLength !== m.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_chacha20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_chacha20_xor_ic(c, m, n, ic, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20_ietf = function (c, n, k) {
  if (n?.byteLength !== binding.crypto_stream_chacha20_ietf_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_chacha20_ietf(c, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20_ietf_xor = function (c, m, n, k) {
  if (c?.byteLength !== m.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_chacha20_ietf_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_chacha20_ietf_xor(c, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_chacha20_ietf_xor_ic = function (c, m, n, ic, k) {
  if (c?.byteLength !== m.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_chacha20_ietf_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_chacha20_ietf_xor_ic(c, m, n, ic, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_xchacha20 = function (c, n, k) {
  if (n?.byteLength !== binding.crypto_stream_xchacha20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_xchacha20_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_xchacha20(c, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_xchacha20_xor = function (c, m, n, k) {
  if (c?.byteLength !== m.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_xchacha20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_xchacha20_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_xchacha20_xor(c, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_xchacha20_xor_ic = function (c, m, n, ic, k) {
  if (c?.byteLength !== m.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_xchacha20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_xchacha20_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_xchacha20_xor_ic(c, m, n, ic, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_salsa20 = function (c, n, k) {
  if (n?.byteLength !== binding.crypto_stream_salsa20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_salsa20_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_salsa20(c, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_salsa20_xor = function (c, m, n, k) {
  if (c?.byteLength !== m.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_salsa20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_salsa20_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_salsa20_xor(c, m, n, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_stream_salsa20_xor_ic = function (c, m, n, ic, k) {
  if (c?.byteLength !== m.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (n?.byteLength !== binding.crypto_stream_salsa20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_salsa20_KEYBYTES) throw new Error('k')

  const res = binding.crypto_stream_salsa20_xor_ic(c, m, n, ic, k)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_auth

exports.crypto_auth = function (out, input, k) {
  if (out?.byteLength !== binding.crypto_auth_BYTES) throw new Error('out')
  if (k?.byteLength !== binding.crypto_auth_KEYBYTES) throw new Error('k')

  const res = binding.crypto_auth(out, input, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_auth_verify = function (h, input, k) {
  if (h?.byteLength !== binding.crypto_auth_BYTES) throw new Error('h')
  if (k?.byteLength !== binding.crypto_auth_KEYBYTES) throw new Error('k')

  return binding.crypto_auth_verify(h, input, k)
}

// crypto_onetimeauth

exports.crypto_onetimeauth = function (out, input, k) {
  if (out?.byteLength !== binding.crypto_onetimeauth_BYTES) throw new Error('out')
  if (k?.byteLength !== binding.crypto_onetimeauth_KEYBYTES) throw new Error('k')

  const res = binding.crypto_onetimeauth(out, input, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_onetimeauth_init = function (state, k) {
  if (state?.byteLength !== binding.crypto_onetimeauth_STATEBYTES) throw new Error("state must be 'crypto_onetimeauth_STATEBYTES' bytes")
  if (k?.byteLength !== binding.crypto_onetimeauth_KEYBYTES) throw new Error('k')

  const res = binding.crypto_onetimeauth_init(state, k)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_onetimeauth_update = function (state, input) {
  if (state?.byteLength !== binding.crypto_onetimeauth_STATEBYTES) throw new Error("state must be 'crypto_onetimeauth_STATEBYTES' bytes")

  const res = binding.crypto_onetimeauth_update(state, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_onetimeauth_final = function (state, out) {
  if (state?.byteLength !== binding.crypto_onetimeauth_STATEBYTES) throw new Error("state must be 'crypto_onetimeauth_STATEBYTES' bytes")
  if (out?.byteLength !== binding.crypto_onetimeauth_BYTES) throw new Error('out')

  const res = binding.crypto_onetimeauth_final(state, out)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_onetimeauth_verify = function (h, input, k) {
  if (h?.byteLength !== binding.crypto_onetimeauth_BYTES) throw new Error('h')
  if (k?.byteLength !== binding.crypto_onetimeauth_KEYBYTES) throw new Error('k')

  return binding.crypto_onetimeauth_verify(h, input, k)
}

// crypto_pwhash

exports.crypto_pwhash = function (out, passwd, salt, opslimit, memlimit, alg) {
  if (out?.byteLength < binding.crypto_pwhash_BYTES_MIN) throw new Error('out')
  if (out?.byteLength > binding.crypto_pwhash_BYTES_MAX) throw new Error('out')
  if (salt?.byteLength !== binding.crypto_pwhash_SALTBYTES) throw new Error('salt')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')
  if (alg < 1 || alg > 2) throw new Error('alg must be either Argon2i 1.3 or Argon2id 1.3')

  const res = binding.crypto_pwhash(out, passwd, salt, opslimit, memlimit, alg)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_pwhash_async = function (out, passwd, salt, opslimit, memlimit, alg, callback = undefined) {
  if (out?.byteLength < binding.crypto_pwhash_BYTES_MIN) throw new Error('out')
  if (out?.byteLength > binding.crypto_pwhash_BYTES_MAX) throw new Error('out')
  if (salt?.byteLength !== binding.crypto_pwhash_SALTBYTES) throw new Error('salt')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')
  if (alg < 1 || alg > 2) throw new Error('alg must be either Argon2i 1.3 or Argon2id 1.3')

  const [done, promise] = checkStatus(callback)

  binding.crypto_pwhash_async(
    out.buffer,
    out.byteOffset,
    out.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    salt.buffer,
    salt.byteOffset,
    salt.byteLength,

    opslimit,
    memlimit,
    alg,

    done
  )

  return promise
}

exports.crypto_pwhash_str = function (out, passwd, opslimit, memlimit) {
  if (out?.byteLength !== binding.crypto_pwhash_STRBYTES) throw new Error('out')
  if (typeof opslimit !== 'number') throw new Error('opslimit')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (typeof memlimit !== 'number') throw new Error('memlimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')

  const res = binding.crypto_pwhash_str(out, passwd, opslimit, memlimit)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_pwhash_str_async = function (out, passwd, opslimit, memlimit, callback = undefined) {
  if (out?.byteLength !== binding.crypto_pwhash_STRBYTES) throw new Error('out')
  if (!passwd?.byteLength) throw new Error('passwd')
  if (typeof opslimit !== 'number') throw new Error('opslimit')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (typeof memlimit !== 'number') throw new Error('memlimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')

  const [done, promise] = checkStatus(callback)

  binding.crypto_pwhash_str_async(
    out.buffer,
    out.byteOffset,
    out.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    opslimit,
    memlimit,

    done
  )

  return promise
}

exports.crypto_pwhash_str_verify = function (str, passwd) {
  if (str?.byteLength !== binding.crypto_pwhash_STRBYTES) throw new Error('str')

  return binding.crypto_pwhash_str_verify(str, passwd)
}

exports.crypto_pwhash_str_verify_async = function (str, passwd, callback = undefined) {
  if (str?.byteLength !== binding.crypto_pwhash_STRBYTES) throw new Error('str')
  if (!passwd?.byteLength) throw new Error('passwd')

  const [done, promise] = checkStatus(callback, true)

  binding.crypto_pwhash_str_verify_async(
    str.buffer,
    str.byteOffset,
    str.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    done
  )

  return promise
}

exports.crypto_pwhash_str_needs_rehash = function (str, opslimit, memlimit) {
  if (str?.byteLength !== binding.crypto_pwhash_STRBYTES) throw new Error('str')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')

  return binding.crypto_pwhash_str_needs_rehash(str, opslimit, memlimit)
}

exports.crypto_pwhash_scryptsalsa208sha256 = function (out, passwd, salt, opslimit, memlimit) {
  if (out?.byteLength < binding.crypto_pwhash_scryptsalsa208sha256_BYTES_MIN) throw new Error('out')
  if (out?.byteLength > binding.crypto_pwhash_scryptsalsa208sha256_BYTES_MAX) throw new Error('out')
  if (salt?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_SALTBYTES) throw new Error('salt')
  if (opslimit < binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MAX) throw new Error('memlimit')

  const res = binding.crypto_pwhash_scryptsalsa208sha256(out, passwd, salt, opslimit, memlimit)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_pwhash_scryptsalsa208sha256_async = function (out, passwd, salt, opslimit, memlimit, callback = undefined) {
  if (out?.byteLength < binding.crypto_pwhash_scryptsalsa208sha256_BYTES_MIN) throw new Error('out')
  if (out?.byteLength > binding.crypto_pwhash_scryptsalsa208sha256_BYTES_MAX) throw new Error('out')
  if (!passwd?.byteLength) throw new Error('passwd')
  if (salt?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_SALTBYTES) throw new Error('salt')
  if (opslimit < binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MAX) throw new Error('memlimit')

  const [done, promise] = checkStatus(callback)

  binding.crypto_pwhash_scryptsalsa208sha256_async(
    out.buffer,
    out.byteOffset,
    out.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    salt.buffer,
    salt.byteOffset,
    salt.byteLength,

    opslimit,
    memlimit,

    done
  )

  return promise
}

exports.crypto_pwhash_scryptsalsa208sha256_str_async = function (out, passwd, opslimit, memlimit, callback = undefined) {
  if (out?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_STRBYTES) throw new Error('out')
  if (!passwd?.byteLength) throw new Error('passwd')
  if (opslimit < binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MAX) throw new Error('memlimit')

  const [done, promise] = checkStatus(callback)

  binding.crypto_pwhash_scryptsalsa208sha256_str_async(
    out.buffer,
    out.byteOffset,
    out.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    opslimit,
    memlimit,

    done
  )

  return promise
}

exports.crypto_pwhash_scryptsalsa208sha256_str = function (out, passwd, opslimit, memlimit) {
  if (out?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_STRBYTES) throw new Error('out')
  if (!passwd?.byteLength) throw new Error('passwd')
  if (opslimit < binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_scryptsalsa208sha256_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_scryptsalsa208sha256_MEMLIMIT_MAX) throw new Error('memlimit')

  const res = binding.crypto_pwhash_scryptsalsa208sha256_str(out, passwd, opslimit, memlimit)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_pwhash_scryptsalsa208sha256_str_verify_async = function (str, passwd, callback = undefined) {
  if (str?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_STRBYTES) throw new Error('str')
  if (!passwd?.byteLength) throw new Error('passwd')

  const [done, promise] = checkStatus(callback, true)

  binding.crypto_pwhash_scryptsalsa208sha256_str_verify_async(
    str.buffer,
    str.byteOffset,
    str.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    done
  )

  return promise
}

exports.crypto_pwhash_scryptsalsa208sha256_str_verify = function (str, passwd) {
  if (str?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_STRBYTES) throw new Error('str')
  if (!passwd?.byteLength) throw new Error('passwd')

  return binding.crypto_pwhash_scryptsalsa208sha256_str_verify(str, passwd)
}

exports.crypto_pwhash_scryptsalsa208sha256_str_needs_rehash = function (str, opslimit, memlimit) {
  if (str?.byteLength !== binding.crypto_pwhash_scryptsalsa208sha256_STRBYTES) throw new Error('str')
  if (opslimit < binding.crypto_pwhash_OPSLIMIT_MIN) throw new Error('opslimit')
  if (opslimit > binding.crypto_pwhash_OPSLIMIT_MAX) throw new Error('opslimit')
  if (memlimit < binding.crypto_pwhash_MEMLIMIT_MIN) throw new Error('memlimit')
  if (memlimit > binding.crypto_pwhash_MEMLIMIT_MAX) throw new Error('memlimit')

  return binding.crypto_pwhash_scryptsalsa208sha256_str_needs_rehash(str, opslimit, memlimit)
}

// crypto_kx

exports.crypto_kx_keypair = function (pk, sk) {
  if (pk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES) throw new Error('pk')
  if (sk?.byteLength !== binding.crypto_kx_SECRETKEYBYTES) throw new Error('sk')

  const res = binding.crypto_kx_keypair(pk, sk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_kx_seed_keypair = function (pk, sk, seed) {
  if (pk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES) throw new Error('pk')
  if (sk?.byteLength !== binding.crypto_kx_SECRETKEYBYTES) throw new Error('sk')
  if (seed?.byteLength !== binding.crypto_kx_SEEDBYTES) throw new Error('seed')

  const res = binding.crypto_kx_seed_keypair(pk, sk, seed)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_kx_client_session_keys = function (rx, tx, clientPk, clientSk, serverPk) {
  // match `std::optional` by coercing null to undefined
  rx ??= undefined
  tx ??= undefined

  if (!rx && !tx) throw new Error('at least one session key must be specified')

  if (rx) {
    if (rx?.byteLength !== binding.crypto_kx_SESSIONKEYBYTES) throw new Error('receiving key buffer must be "crypto_kx_SESSIONKEYBYTES" bytes or null')
  } else {
    if (tx?.byteLength !== binding.crypto_kx_SESSIONKEYBYTES) throw new Error('transmitting key buffer must be "crypto_kx_SESSIONKEYBYTES" bytes or null')
  }

  if (clientPk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES) throw new Error('client_pk')
  if (clientSk?.byteLength !== binding.crypto_kx_SECRETKEYBYTES) throw new Error('client_sk')
  if (serverPk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES) throw new Error('server_pk')

  const res = binding.crypto_kx_client_session_keys(rx, tx, clientPk, clientSk, serverPk)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_kx_server_session_keys = function (rx, tx, serverPk, serverSk, clientPk) {
  rx ??= undefined
  tx ??= undefined

  if (!rx && !tx) throw new Error('at least one session key must be specified')

  if (rx) {
    if (rx?.byteLength !== binding.crypto_kx_SESSIONKEYBYTES) throw new Error('receiving key buffer must be "crypto_kx_SESSIONKEYBYTES" bytes or null')
  } else {
    if (tx?.byteLength !== binding.crypto_kx_SESSIONKEYBYTES) throw new Error('transmitting key buffer must be "crypto_kx_SESSIONKEYBYTES" bytes or null')
  }

  if (serverPk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES) throw new Error('server_pk')
  if (serverSk?.byteLength !== binding.crypto_kx_SECRETKEYBYTES) throw new Error('server_sk')
  if (clientPk?.byteLength !== binding.crypto_kx_PUBLICKEYBYTES) throw new Error('client_pk')

  const res = binding.crypto_kx_server_session_keys(rx, tx, serverPk, serverSk, clientPk)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_scalarmult

exports.crypto_scalarmult_base = function (q, n) {
  if (q?.byteLength !== binding.crypto_scalarmult_BYTES) throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_SCALARBYTES) throw new Error('n')

  const res = binding.crypto_scalarmult_base(q, n)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_scalarmult = function (q, n, p) {
  if (q?.byteLength !== binding.crypto_scalarmult_BYTES) throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_SCALARBYTES) throw new Error('n')
  if (p?.byteLength !== binding.crypto_scalarmult_BYTES) throw new Error('p')

  const res = binding.crypto_scalarmult(q, n, p)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_scalarmult_ed25519_base = function (q, n) {
  if (q?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES) throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_ed25519_SCALARBYTES) throw new Error('n')

  const res = binding.crypto_scalarmult_ed25519_base(q, n)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_scalarmult_ed25519 = function (q, n, p) {
  if (q?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES) throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_ed25519_SCALARBYTES) throw new Error('n')
  if (p?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES) throw new Error('p')

  const res = binding.crypto_scalarmult_ed25519(q, n, p)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_core_ed25519_is_valid_point = function (p) {
  if (p?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('p')

  return binding.crypto_core_ed25519_is_valid_point(p)
}

exports.crypto_core_ed25519_from_uniform = function (p, r) {
  if (p?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('p')
  if (r?.byteLength !== binding.crypto_core_ed25519_UNIFORMBYTES) throw new Error('r')

  const res = binding.crypto_core_ed25519_from_uniform(p, r)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_scalarmult_ed25519_base_noclamp = function (q, n) {
  if (q?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES) throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_ed25519_SCALARBYTES) throw new Error('n')

  const res = binding.crypto_scalarmult_ed25519_base_noclamp(q, n)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_scalarmult_ed25519_noclamp = function (q, n, p) {
  if (q?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES) throw new Error('q')
  if (n?.byteLength !== binding.crypto_scalarmult_ed25519_SCALARBYTES) throw new Error('n')
  if (p?.byteLength !== binding.crypto_scalarmult_ed25519_BYTES) throw new Error('p')

  const res = binding.crypto_scalarmult_ed25519_noclamp(q, n, p)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_core

exports.crypto_core_ed25519_add = function (r, p, q) {
  if (r?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('r')
  if (p?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('p')
  if (q?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('q')

  const res = binding.crypto_core_ed25519_add(r, p, q)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_core_ed25519_sub = function (r, p, q) {
  if (r?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('r')
  if (p?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('p')
  if (q?.byteLength !== binding.crypto_core_ed25519_BYTES) throw new Error('q')

  const res = binding.crypto_core_ed25519_sub(r, p, q)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_core_ed25519_scalar_random = function (r) {
  if (r?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('r')

  binding.crypto_core_ed25519_scalar_random(r)
}

exports.crypto_core_ed25519_scalar_reduce = function (r, s) {
  if (r?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('r')
  if (s?.byteLength !== binding.crypto_core_ed25519_NONREDUCEDSCALARBYTES) throw new Error('s')

  binding.crypto_core_ed25519_scalar_reduce(r, s)
}

exports.crypto_core_ed25519_scalar_invert = function (recip, s) {
  if (recip?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('recip')
  if (s?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('s')

  binding.crypto_core_ed25519_scalar_invert(recip, s)
}

exports.crypto_core_ed25519_scalar_negate = function (neg, s) {
  if (neg?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('neg')
  if (s?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('s')

  binding.crypto_core_ed25519_scalar_negate(neg, s)
}

exports.crypto_core_ed25519_scalar_complement = function (comp, s) {
  if (comp?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('comp')
  if (s?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('s')

  binding.crypto_core_ed25519_scalar_complement(comp, s)
}

exports.crypto_core_ed25519_scalar_add = function (z, x, y) {
  if (z?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('z')
  if (x?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('x')
  if (y?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('y')

  binding.crypto_core_ed25519_scalar_add(z, x, y)
}

exports.crypto_core_ed25519_scalar_sub = function (z, x, y) {
  if (z?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('z')
  if (x?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('x')
  if (y?.byteLength !== binding.crypto_core_ed25519_SCALARBYTES) throw new Error('y')

  binding.crypto_core_ed25519_scalar_sub(z, x, y)
}

// crypto_shorthash

exports.crypto_shorthash = function (out, input, k) {
  if (out?.byteLength !== binding.crypto_shorthash_BYTES) throw new Error('out')
  if (k?.byteLength !== binding.crypto_shorthash_KEYBYTES) throw new Error('k')

  const res = binding.crypto_shorthash(out, input, k)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_kdf

exports.crypto_kdf_keygen = function (key) {
  if (key?.byteLength !== binding.crypto_kdf_KEYBYTES) throw new Error('key')

  binding.crypto_kdf_keygen(key)
}

exports.crypto_kdf_derive_from_key = function (subkey, subkeyId, ctx, key) {
  if (subkey?.byteLength < binding.crypto_kdf_BYTES_MIN) throw new Error('subkey')
  if (subkey?.byteLength > binding.crypto_kdf_BYTES_MAX) throw new Error('subkey')
  if (ctx?.byteLength !== binding.crypto_kdf_CONTEXTBYTES) throw new Error('ctx')
  if (key?.byteLength !== binding.crypto_kdf_KEYBYTES) throw new Error('key')

  const res = binding.crypto_kdf_derive_from_key(subkey, subkeyId, ctx, key)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_hash

exports.crypto_hash = function (out, input) {
  if (out?.byteLength !== binding.crypto_hash_BYTES) throw new Error('out')

  const res = binding.crypto_hash(out, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha256 = function (out, input) {
  if (out?.byteLength !== binding.crypto_hash_sha256_BYTES) throw new Error('out')

  const res = binding.crypto_hash_sha256(out, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha256_init = function (state) {
  if (state?.byteLength !== binding.crypto_hash_sha256_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha256_STATEBYTES' bytes")
  }

  const res = binding.crypto_hash_sha256_init(state)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha256_update = function (state, input) {
  if (state?.byteLength !== binding.crypto_hash_sha256_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha256_STATEBYTES' bytes")
  }

  const res = binding.crypto_hash_sha256_update(state, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha256_final = function (state, out) {
  if (state?.byteLength !== binding.crypto_hash_sha256_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha256_STATEBYTES' bytes")
  }
  if (out?.byteLength !== binding.crypto_hash_sha256_BYTES) throw new Error('state')

  const res = binding.crypto_hash_sha256_final(state, out)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha512 = function (out, input) {
  if (out?.byteLength !== binding.crypto_hash_sha512_BYTES) throw new Error('out')

  const res = binding.crypto_hash_sha512(out, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha512_init = function (state) {
  if (state?.byteLength !== binding.crypto_hash_sha512_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha512_STATEBYTES' bytes")
  }

  const res = binding.crypto_hash_sha512_init(state)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha512_update = function (state, input) {
  if (state?.byteLength !== binding.crypto_hash_sha512_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha512_STATEBYTES' bytes")
  }

  const res = binding.crypto_hash_sha512_update(state, input)

  if (res !== 0) throw new Error('status: ' + res)
}

exports.crypto_hash_sha512_final = function (state, out) {
  if (state?.byteLength !== binding.crypto_hash_sha512_STATEBYTES) {
    throw new Error("state must be 'crypto_hash_sha512_STATEBYTES' bytes")
  }
  if (out?.byteLength !== binding.crypto_hash_sha512_BYTES) throw new Error('out')

  const res = binding.crypto_hash_sha512_final(state, out)

  if (res !== 0) throw new Error('status: ' + res)
}

// crypto_aead

exports.crypto_aead_xchacha20poly1305_ietf_keygen = function (k) {
  if (k?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_KEYBYTES) throw new Error('k')

  binding.crypto_aead_xchacha20poly1305_ietf_keygen(k)
}

exports.crypto_aead_xchacha20poly1305_ietf_encrypt = function (c, m, ad, nsec = null, npub, k) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (c?.byteLength !== m.byteLength + binding.crypto_aead_xchacha20poly1305_ietf_ABYTES) throw new Error('c must "m.byteLength + crypto_aead_xchacha20poly1305_ietf_ABYTES" bytes')
  if (c?.byteLength > 0xffffffff) throw new Error('c.byteLength must be a 32bit integer')
  if (npub?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_NPUBBYTES) throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_aead_xchacha20poly1305_ietf_encrypt(c, m, ad, npub, k)
  if (res < 0) throw new Error('could not encrypt data')

  return res
}

exports.crypto_aead_xchacha20poly1305_ietf_decrypt = function (m, nsec = null, c, ad, npub, k) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (m?.byteLength !== c.byteLength - binding.crypto_aead_xchacha20poly1305_ietf_ABYTES) throw new Error('m must "c.byteLength - crypto_aead_xchacha20poly1305_ietf_ABYTES" bytes')
  if (m?.byteLength > 0xffffffff) throw new Error('m.byteLength must be a 32bit integer')
  if (npub?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_NPUBBYTES) throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_aead_xchacha20poly1305_ietf_decrypt(m, c, ad, npub, k)
  if (res < 0) throw new Error('could not verify data')

  return res
}

exports.crypto_aead_xchacha20poly1305_ietf_encrypt_detached = function (c, mac, m, ad, nsec = null, npub, k) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (c?.byteLength !== m.byteLength) throw new Error('c must be "m.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_ABYTES) throw new Error('mac')
  if (npub?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_NPUBBYTES) throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_aead_xchacha20poly1305_ietf_encrypt_detached(c, mac, m, ad, npub, k)
  if (res < 0) throw new Error('could not encrypt data')

  return res
}

exports.crypto_aead_xchacha20poly1305_ietf_decrypt_detached = function (m, nsec = null, c, mac, ad, npub, k) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (m?.byteLength !== c.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_ABYTES) throw new Error('mac')
  if (npub?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_NPUBBYTES) throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_xchacha20poly1305_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_aead_xchacha20poly1305_ietf_decrypt_detached(m, c, mac, ad, npub, k)
  if (res !== 0) throw new Error('could not verify data')
}

exports.crypto_aead_chacha20poly1305_ietf_keygen = function (k) {
  if (k?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_KEYBYTES) throw new Error('k')

  binding.crypto_aead_chacha20poly1305_ietf_keygen(k)
}

exports.crypto_aead_chacha20poly1305_ietf_encrypt = function (c, m, ad, nsec = null, npub, k) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (c?.byteLength !== m.byteLength + binding.crypto_aead_chacha20poly1305_ietf_ABYTES) throw new Error('c must "m.byteLength + crypto_aead_chacha20poly1305_ietf_ABYTES" bytes')
  if (c?.byteLength > 0xffffffff) throw new Error('c.byteLength must be a 32bit integer')
  if (npub?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_NPUBBYTES) throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_aead_chacha20poly1305_ietf_encrypt(c, m, ad, npub, k)
  if (res < 0) throw new Error('could not encrypt data')

  return res
}

exports.crypto_aead_chacha20poly1305_ietf_decrypt = function (m, nsec = null, c, ad, npub, k) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (m?.byteLength !== c.byteLength - binding.crypto_aead_chacha20poly1305_ietf_ABYTES) throw new Error('m must "c.byteLength - crypto_aead_chacha20poly1305_ietf_ABYTES" bytes')
  if (m?.byteLength > 0xffffffff) throw new Error('m.byteLength must be a 32bit integer')
  if (npub?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_NPUBBYTES) throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_aead_chacha20poly1305_ietf_decrypt(m, c, ad, npub, k)
  if (res < 0) throw new Error('could not verify data')

  return res
}

exports.crypto_aead_chacha20poly1305_ietf_encrypt_detached = function (c, mac, m, ad, nsec = null, npub, k) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (c?.byteLength !== m.byteLength) throw new Error('c must be "m.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_ABYTES) throw new Error('mac')
  if (npub?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_NPUBBYTES) throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_aead_chacha20poly1305_ietf_encrypt_detached(c, mac, m, ad, npub, k)
  if (res < 0) throw new Error('could not encrypt data')

  return res
}

exports.crypto_aead_chacha20poly1305_ietf_decrypt_detached = function (m, nsec = null, c, mac, ad, npub, k) {
  ad ??= undefined
  if (nsec !== null) throw new Error('nsec must always be set to null')
  if (m?.byteLength !== c.byteLength) throw new Error('m must be "c.byteLength" bytes')
  if (mac?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_ABYTES) throw new Error('mac')
  if (npub?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_NPUBBYTES) throw new Error('npub')
  if (k?.byteLength !== binding.crypto_aead_chacha20poly1305_ietf_KEYBYTES) throw new Error('k')

  const res = binding.crypto_aead_chacha20poly1305_ietf_decrypt_detached(m, c, mac, ad, npub, k)
  if (res !== 0) throw new Error('could not verify data')
}

// crypto_stream

exports.crypto_stream_xor_wrap_init = function (state, n, k) {
  if (state?.byteLength !== binding.sn_crypto_stream_xor_STATEBYTES) {
    throw new Error("state must be 'sn_crypto_stream_xor_STATEBYTES' bytes")
  }
  if (n?.byteLength !== binding.crypto_stream_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_KEYBYTES) throw new Error('k')

  binding.crypto_stream_xor_wrap_init(state, n, k)
}

exports.crypto_stream_xor_wrap_update = function (state, c, m) {
  if (state?.byteLength !== binding.sn_crypto_stream_xor_STATEBYTES) {
    throw new Error("state must be 'sn_crypto_stream_xor_STATEBYTES' bytes")
  }
  if (c?.byteLength !== m.byteLength) throw new Error('c must be "m.byteLength" bytes')

  binding.crypto_stream_xor_wrap_update(state, c, m)
}

exports.crypto_stream_xor_wrap_final = function (state) {
  if (state?.byteLength !== binding.sn_crypto_stream_xor_STATEBYTES) {
    throw new Error("state must be 'sn_crypto_stream_xor_STATEBYTES' bytes")
  }

  binding.crypto_stream_xor_wrap_final(state)
}

exports.crypto_stream_chacha20_xor_wrap_init = function (state, n, k) {
  if (state?.byteLength !== binding.crypto_stream_chacha20_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_chacha20_xor_STATEBYTES' bytes")
  }
  if (n?.byteLength !== binding.crypto_stream_chacha20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_KEYBYTES) throw new Error('k')

  binding.crypto_stream_chacha20_xor_wrap_init(state, n, k)
}

exports.crypto_stream_chacha20_xor_wrap_update = function (state, c, m) {
  if (state?.byteLength !== binding.crypto_stream_chacha20_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_chacha20_xor_STATEBYTES' bytes")
  }
  if (c?.byteLength !== m.byteLength) throw new Error('c must be "m.byteLength" bytes')

  binding.crypto_stream_chacha20_xor_wrap_update(state, c, m)
}

exports.crypto_stream_chacha20_xor_wrap_final = function (state) {
  if (state?.byteLength !== binding.crypto_stream_chacha20_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_chacha20_xor_STATEBYTES' bytes")
  }

  binding.crypto_stream_chacha20_xor_wrap_final(state)
}

exports.crypto_stream_chacha20_ietf_xor_wrap_init = function (state, n, k) {
  if (state?.byteLength !== binding.crypto_stream_chacha20_ietf_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_chacha20_ietf_xor_STATEBYTES' bytes")
  }
  if (n?.byteLength !== binding.crypto_stream_chacha20_ietf_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_chacha20_ietf_KEYBYTES) throw new Error('k')

  binding.crypto_stream_chacha20_ietf_xor_wrap_init(state, n, k)
}

exports.crypto_stream_chacha20_ietf_xor_wrap_update = function (state, c, m) {
  if (state?.byteLength !== binding.crypto_stream_chacha20_ietf_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_chacha20_ietf_xor_STATEBYTES' bytes")
  }
  if (c?.byteLength !== m.byteLength) throw new Error('c must be "m.byteLength" bytes')

  binding.crypto_stream_chacha20_ietf_xor_wrap_update(state, c, m)
}

exports.crypto_stream_chacha20_ietf_xor_wrap_final = function (state) {
  if (state?.byteLength !== binding.crypto_stream_chacha20_ietf_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_chacha20_ietf_xor_STATEBYTES' bytes")
  }

  binding.crypto_stream_chacha20_ietf_xor_wrap_final(state)
}

exports.crypto_stream_xchacha20_xor_wrap_init = function (state, n, k) {
  if (state?.byteLength !== binding.crypto_stream_xchacha20_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_xchacha20_xor_STATEBYTES' bytes")
  }
  if (n?.byteLength !== binding.crypto_stream_xchacha20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_xchacha20_KEYBYTES) throw new Error('k')

  binding.crypto_stream_xchacha20_xor_wrap_init(state, n, k)
}

exports.crypto_stream_xchacha20_xor_wrap_update = function (state, c, m) {
  if (state?.byteLength !== binding.crypto_stream_xchacha20_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_xchacha20_xor_STATEBYTES' bytes")
  }
  if (c?.byteLength !== m.byteLength) throw new Error('c must be "m.byteLength" bytes')

  binding.crypto_stream_xchacha20_xor_wrap_update(state, c, m)
}

exports.crypto_stream_xchacha20_xor_wrap_final = function (state) {
  if (state?.byteLength !== binding.crypto_stream_xchacha20_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_xchacha20_xor_STATEBYTES' bytes")
  }

  binding.crypto_stream_xchacha20_xor_wrap_final(state)
}

exports.crypto_stream_salsa20_xor_wrap_init = function (state, n, k) {
  if (state?.byteLength !== binding.crypto_stream_salsa20_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_salsa20_xor_STATEBYTES' bytes")
  }
  if (n?.byteLength !== binding.crypto_stream_salsa20_NONCEBYTES) throw new Error('n')
  if (k?.byteLength !== binding.crypto_stream_salsa20_KEYBYTES) throw new Error('k')

  binding.crypto_stream_salsa20_xor_wrap_init(state, n, k)
}

exports.crypto_stream_salsa20_xor_wrap_update = function (state, c, m) {
  if (state?.byteLength !== binding.crypto_stream_salsa20_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_salsa20_xor_STATEBYTES' bytes")
  }
  if (c?.byteLength !== m.byteLength) throw new Error('c must be "m.byteLength" bytes')

  binding.crypto_stream_salsa20_xor_wrap_update(state, c, m)
}

exports.crypto_stream_salsa20_xor_wrap_final = function (state) {
  if (state?.byteLength !== binding.crypto_stream_salsa20_xor_STATEBYTES) {
    throw new Error("state must be 'crypto_stream_salsa20_xor_STATEBYTES' bytes")
  }

  binding.crypto_stream_salsa20_xor_wrap_final(state)
}

// experimental

exports.extension_tweak_ed25519_base = function (n, p, ns) {
  if (n?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('n')
  if (p?.byteLength !== binding.extension_tweak_ed25519_BYTES) throw new Error('p')

  binding.extension_tweak_ed25519_base(n, p, ns)
}

exports.extension_tweak_ed25519_sign_detached = function (sig, m, scalar, pk) {
  if (sig?.byteLength !== binding.crypto_sign_BYTES) throw new Error('sig')
  if (scalar?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('scalar')
  if (pk && pk.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('pk')

  const res = binding.extension_tweak_ed25519_sign_detached(sig, m, scalar, pk)
  if (res !== 0) throw new Error('failed to compute signature')
}

exports.extension_tweak_ed25519_sk_to_scalar = function (n, sk) {
  if (n?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('n')
  if (sk?.byteLength !== binding.crypto_sign_SECRETKEYBYTES) throw new Error('sk')

  binding.extension_tweak_ed25519_sk_to_scalar(n, sk)
}

exports.extension_tweak_ed25519_scalar = function (scalarOut, scalar, ns) {
  if (scalarOut?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('scalar_out')
  if (scalar?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('scalar')

  binding.extension_tweak_ed25519_scalar(scalarOut, scalar, ns)
}

exports.extension_tweak_ed25519_pk = function (tpk, pk, ns) {
  if (tpk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('tpk')
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('pk')

  const res = binding.extension_tweak_ed25519_pk(tpk, pk, ns)
  if (res !== 0) throw new Error('failed to tweak public key')
}

exports.extension_tweak_ed25519_keypair = function (pk, scalarOut, scalarIn, ns) {
  if (pk?.byteLength !== binding.extension_tweak_ed25519_BYTES) throw new Error('pk')
  if (scalarOut?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('scalar_out')
  if (scalarIn?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('scalar_in')

  binding.extension_tweak_ed25519_keypair(pk, scalarOut, scalarIn, ns)
}

exports.extension_tweak_ed25519_scalar_add = function (scalarOut, scalar, n) {
  if (scalarOut?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('scalar_out')
  if (scalar?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('scalar')
  if (n?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('n')

  binding.extension_tweak_ed25519_scalar_add(scalarOut, scalar, n)
}

exports.extension_tweak_ed25519_pk_add = function (tpk, pk, p) {
  if (tpk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('tpk')
  if (pk?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('pk')
  if (p?.byteLength !== binding.crypto_sign_PUBLICKEYBYTES) throw new Error('p')

  const res = binding.extension_tweak_ed25519_pk_add(tpk, pk, p)
  if (res !== 0) throw new Error('failed to add tweak to public key')
}

exports.extension_tweak_ed25519_keypair_add = function (pk, scalarOut, scalarIn, tweak) {
  if (pk?.byteLength !== binding.extension_tweak_ed25519_BYTES) throw new Error('pk')
  if (scalarOut?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('scalar_out')
  if (scalarIn?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('scalar_in')
  if (tweak?.byteLength !== binding.extension_tweak_ed25519_SCALARBYTES) throw new Error('tweak')

  const res = binding.extension_tweak_ed25519_keypair_add(pk, scalarOut, scalarIn, tweak)
  if (res !== 0) throw new Error('failed to add tweak to keypair')
}

exports.extension_pbkdf2_sha512_async = function (out, passwd, salt, iter, outlen, callback = undefined) {
  if (iter < binding.extension_pbkdf2_sha512_ITERATIONS_MIN) throw new Error('iterations')
  if (outlen > binding.extension_pbkdf2_sha512_BYTES_MAX) throw new Error('outlen')
  if (out?.byteLength < outlen) throw new Error('out')
  if (!out?.byteLength) throw new Error('out')
  if (!passwd?.byteLength) throw new Error('passwd')
  if (!salt?.byteLength) throw new Error('salt')

  const [done, promise] = checkStatus(callback)

  binding.extension_pbkdf2_sha512_async(
    out.buffer,
    out.byteOffset,
    out.byteLength,

    passwd.buffer,
    passwd.byteOffset,
    passwd.byteLength,

    salt.buffer,
    salt.byteOffset,
    salt.byteLength,

    iter,
    outlen,

    done
  )

  return promise
}

exports.extension_pbkdf2_sha512 = function (out, passwd, salt, iter, outlen) {
  if (iter < binding.extension_pbkdf2_sha512_ITERATIONS_MIN) throw new Error('iterations')
  if (outlen > binding.extension_pbkdf2_sha512_BYTES_MAX) throw new Error('outlen')
  if (out?.byteLength < outlen) throw new Error('out')

  const res = binding.extension_pbkdf2_sha512(out, passwd, salt, iter, outlen)

  if (res !== 0) throw new Error('failed to add tweak to public key')
}

function checkStatus (callback, booleanResult = false) {
  let done, promise

  if (typeof callback === 'function') {
    done = function (status) {
      if (booleanResult) callback(null, status === 0)
      else if (status === 0) callback(null)
      else callback(new Error('status: ' + status))
    }
  } else {
    promise = new Promise(function (resolve, reject) {
      done = function (status) {
        if (booleanResult) resolve(status === 0)
        else if (status === 0) resolve()
        else reject(new Error('status: ' + status))
      }
    })
  }

  return [done, promise]
}
{
  "name": "sodium-native",
  "version": "5.0.9",
  "description": "Low level bindings for libsodium",
  "main": "index.js",
  "files": [
    "index.js",
    "binding.cc",
    "binding.js",
    "extensions",
    "prebuilds",
    "CMakeLists.txt"
  ],
  "addon": true,
  "dependencies": {
    "require-addon": "^1.1.0",
    "which-runtime": "^1.2.1"
  },
  "devDependencies": {
    "bare-compat-napi": "^1.3.5",
    "brittle": "^3.16.2",
    "cmake-bare": "^1.6.1",
    "cmake-fetch": "^1.4.3",
    "cmake-napi": "^1.2.1",
    "standard": "^17.1.2"
  },
  "scripts": {
    "test": "standard && npm run test:node && npm run test:bare",
    "test:node": "node test/all.js",
    "test:bare": "bare test/all.js"
  },
  "standard": {
    "ignore": [
      "/test/fixtures/*.js"
    ]
  },
  "engines": {
    "bare": ">=1.16.0"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/sodium-native.git"
  },
  "contributors": [
    "Emil Bay <github@tixz.dk> (http://bayes.dk)",
    "Mathias Buus <mathiasbuus@gmail.com> (https://mafinto.sh)",
    "Christophe Diederichs <chm-diederichs@hyperdivision.dk>"
  ],
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/holepunchto/sodium-native/issues"
  },
  "homepage": "https://github.com/holepunchto/sodium-native"
}
const { EventEmitter } = require('events-universal')
const STREAM_DESTROYED = new Error('Stream was destroyed')
const PREMATURE_CLOSE = new Error('Premature close')

const FIFO = require('fast-fifo')
const TextDecoder = require('text-decoder')

// if we do a future major, expect queue microtask to be there always, for now a bit defensive
const qmt = typeof queueMicrotask === 'undefined' ? fn => global.process.nextTick(fn) : queueMicrotask

/* eslint-disable no-multi-spaces */

// 29 bits used total (4 from shared, 14 from read, and 11 from write)
const MAX = ((1 << 29) - 1)

// Shared state
const OPENING       = 0b0001
const PREDESTROYING = 0b0010
const DESTROYING    = 0b0100
const DESTROYED     = 0b1000

const NOT_OPENING = MAX ^ OPENING
const NOT_PREDESTROYING = MAX ^ PREDESTROYING

// Read state (4 bit offset from shared state)
const READ_ACTIVE           = 0b00000000000001 << 4
const READ_UPDATING         = 0b00000000000010 << 4
const READ_PRIMARY          = 0b00000000000100 << 4
const READ_QUEUED           = 0b00000000001000 << 4
const READ_RESUMED          = 0b00000000010000 << 4
const READ_PIPE_DRAINED     = 0b00000000100000 << 4
const READ_ENDING           = 0b00000001000000 << 4
const READ_EMIT_DATA        = 0b00000010000000 << 4
const READ_EMIT_READABLE    = 0b00000100000000 << 4
const READ_EMITTED_READABLE = 0b00001000000000 << 4
const READ_DONE             = 0b00010000000000 << 4
const READ_NEXT_TICK        = 0b00100000000000 << 4
const READ_NEEDS_PUSH       = 0b01000000000000 << 4
const READ_READ_AHEAD       = 0b10000000000000 << 4

// Combined read state
const READ_FLOWING = READ_RESUMED | READ_PIPE_DRAINED
const READ_ACTIVE_AND_NEEDS_PUSH = READ_ACTIVE | READ_NEEDS_PUSH
const READ_PRIMARY_AND_ACTIVE = READ_PRIMARY | READ_ACTIVE
const READ_EMIT_READABLE_AND_QUEUED = READ_EMIT_READABLE | READ_QUEUED
const READ_RESUMED_READ_AHEAD = READ_RESUMED | READ_READ_AHEAD

const READ_NOT_ACTIVE             = MAX ^ READ_ACTIVE
const READ_NON_PRIMARY            = MAX ^ READ_PRIMARY
const READ_NON_PRIMARY_AND_PUSHED = MAX ^ (READ_PRIMARY | READ_NEEDS_PUSH)
const READ_PUSHED                 = MAX ^ READ_NEEDS_PUSH
const READ_PAUSED                 = MAX ^ READ_RESUMED
const READ_NOT_QUEUED             = MAX ^ (READ_QUEUED | READ_EMITTED_READABLE)
const READ_NOT_ENDING             = MAX ^ READ_ENDING
const READ_PIPE_NOT_DRAINED       = MAX ^ READ_FLOWING
const READ_NOT_NEXT_TICK          = MAX ^ READ_NEXT_TICK
const READ_NOT_UPDATING           = MAX ^ READ_UPDATING
const READ_NO_READ_AHEAD          = MAX ^ READ_READ_AHEAD
const READ_PAUSED_NO_READ_AHEAD   = MAX ^ READ_RESUMED_READ_AHEAD

// Write state (18 bit offset, 4 bit offset from shared state and 14 from read state)
const WRITE_ACTIVE     = 0b00000000001 << 18
const WRITE_UPDATING   = 0b00000000010 << 18
const WRITE_PRIMARY    = 0b00000000100 << 18
const WRITE_QUEUED     = 0b00000001000 << 18
const WRITE_UNDRAINED  = 0b00000010000 << 18
const WRITE_DONE       = 0b00000100000 << 18
const WRITE_EMIT_DRAIN = 0b00001000000 << 18
const WRITE_NEXT_TICK  = 0b00010000000 << 18
const WRITE_WRITING    = 0b00100000000 << 18
const WRITE_FINISHING  = 0b01000000000 << 18
const WRITE_CORKED     = 0b10000000000 << 18

const WRITE_NOT_ACTIVE    = MAX ^ (WRITE_ACTIVE | WRITE_WRITING)
const WRITE_NON_PRIMARY   = MAX ^ WRITE_PRIMARY
const WRITE_NOT_FINISHING = MAX ^ (WRITE_ACTIVE | WRITE_FINISHING)
const WRITE_DRAINED       = MAX ^ WRITE_UNDRAINED
const WRITE_NOT_QUEUED    = MAX ^ WRITE_QUEUED
const WRITE_NOT_NEXT_TICK = MAX ^ WRITE_NEXT_TICK
const WRITE_NOT_UPDATING  = MAX ^ WRITE_UPDATING
const WRITE_NOT_CORKED    = MAX ^ WRITE_CORKED

// Combined shared state
const ACTIVE = READ_ACTIVE | WRITE_ACTIVE
const NOT_ACTIVE = MAX ^ ACTIVE
const DONE = READ_DONE | WRITE_DONE
const DESTROY_STATUS = DESTROYING | DESTROYED | PREDESTROYING
const OPEN_STATUS = DESTROY_STATUS | OPENING
const AUTO_DESTROY = DESTROY_STATUS | DONE
const NON_PRIMARY = WRITE_NON_PRIMARY & READ_NON_PRIMARY
const ACTIVE_OR_TICKING = WRITE_NEXT_TICK | READ_NEXT_TICK
const TICKING = ACTIVE_OR_TICKING & NOT_ACTIVE
const IS_OPENING = OPEN_STATUS | TICKING

// Combined shared state and read state
const READ_PRIMARY_STATUS = OPEN_STATUS | READ_ENDING | READ_DONE
const READ_STATUS = OPEN_STATUS | READ_DONE | READ_QUEUED
const READ_ENDING_STATUS = OPEN_STATUS | READ_ENDING | READ_QUEUED
const READ_READABLE_STATUS = OPEN_STATUS | READ_EMIT_READABLE | READ_QUEUED | READ_EMITTED_READABLE
const SHOULD_NOT_READ = OPEN_STATUS | READ_ACTIVE | READ_ENDING | READ_DONE | READ_NEEDS_PUSH | READ_READ_AHEAD
const READ_BACKPRESSURE_STATUS = DESTROY_STATUS | READ_ENDING | READ_DONE
const READ_UPDATE_SYNC_STATUS = READ_UPDATING | OPEN_STATUS | READ_NEXT_TICK | READ_PRIMARY
const READ_NEXT_TICK_OR_OPENING = READ_NEXT_TICK | OPENING

// Combined write state
const WRITE_PRIMARY_STATUS = OPEN_STATUS | WRITE_FINISHING | WRITE_DONE
const WRITE_QUEUED_AND_UNDRAINED = WRITE_QUEUED | WRITE_UNDRAINED
const WRITE_QUEUED_AND_ACTIVE = WRITE_QUEUED | WRITE_ACTIVE
const WRITE_DRAIN_STATUS = WRITE_QUEUED | WRITE_UNDRAINED | OPEN_STATUS | WRITE_ACTIVE
const WRITE_STATUS = OPEN_STATUS | WRITE_ACTIVE | WRITE_QUEUED | WRITE_CORKED
const WRITE_PRIMARY_AND_ACTIVE = WRITE_PRIMARY | WRITE_ACTIVE
const WRITE_ACTIVE_AND_WRITING = WRITE_ACTIVE | WRITE_WRITING
const WRITE_FINISHING_STATUS = OPEN_STATUS | WRITE_FINISHING | WRITE_QUEUED_AND_ACTIVE | WRITE_DONE
const WRITE_BACKPRESSURE_STATUS = WRITE_UNDRAINED | DESTROY_STATUS | WRITE_FINISHING | WRITE_DONE
const WRITE_UPDATE_SYNC_STATUS = WRITE_UPDATING | OPEN_STATUS | WRITE_NEXT_TICK | WRITE_PRIMARY
const WRITE_DROP_DATA = WRITE_FINISHING | WRITE_DONE | DESTROY_STATUS

const asyncIterator = Symbol.asyncIterator || Symbol('asyncIterator')

class WritableState {
  constructor (stream, { highWaterMark = 16384, map = null, mapWritable, byteLength, byteLengthWritable } = {}) {
    this.stream = stream
    this.queue = new FIFO()
    this.highWaterMark = highWaterMark
    this.buffered = 0
    this.error = null
    this.pipeline = null
    this.drains = null // if we add more seldomly used helpers we might them into a subobject so its a single ptr
    this.byteLength = byteLengthWritable || byteLength || defaultByteLength
    this.map = mapWritable || map
    this.afterWrite = afterWrite.bind(this)
    this.afterUpdateNextTick = updateWriteNT.bind(this)
  }

  get ended () {
    return (this.stream._duplexState & WRITE_DONE) !== 0
  }

  push (data) {
    if ((this.stream._duplexState & WRITE_DROP_DATA) !== 0) return false
    if (this.map !== null) data = this.map(data)

    this.buffered += this.byteLength(data)
    this.queue.push(data)

    if (this.buffered < this.highWaterMark) {
      this.stream._duplexState |= WRITE_QUEUED
      return true
    }

    this.stream._duplexState |= WRITE_QUEUED_AND_UNDRAINED
    return false
  }

  shift () {
    const data = this.queue.shift()

    this.buffered -= this.byteLength(data)
    if (this.buffered === 0) this.stream._duplexState &= WRITE_NOT_QUEUED

    return data
  }

  end (data) {
    if (typeof data === 'function') this.stream.once('finish', data)
    else if (data !== undefined && data !== null) this.push(data)
    this.stream._duplexState = (this.stream._duplexState | WRITE_FINISHING) & WRITE_NON_PRIMARY
  }

  autoBatch (data, cb) {
    const buffer = []
    const stream = this.stream

    buffer.push(data)
    while ((stream._duplexState & WRITE_STATUS) === WRITE_QUEUED_AND_ACTIVE) {
      buffer.push(stream._writableState.shift())
    }

    if ((stream._duplexState & OPEN_STATUS) !== 0) return cb(null)
    stream._writev(buffer, cb)
  }

  update () {
    const stream = this.stream

    stream._duplexState |= WRITE_UPDATING

    do {
      while ((stream._duplexState & WRITE_STATUS) === WRITE_QUEUED) {
        const data = this.shift()
        stream._duplexState |= WRITE_ACTIVE_AND_WRITING
        stream._write(data, this.afterWrite)
      }

      if ((stream._duplexState & WRITE_PRIMARY_AND_ACTIVE) === 0) this.updateNonPrimary()
    } while (this.continueUpdate() === true)

    stream._duplexState &= WRITE_NOT_UPDATING
  }

  updateNonPrimary () {
    const stream = this.stream

    if ((stream._duplexState & WRITE_FINISHING_STATUS) === WRITE_FINISHING) {
      stream._duplexState = stream._duplexState | WRITE_ACTIVE
      stream._final(afterFinal.bind(this))
      return
    }

    if ((stream._duplexState & DESTROY_STATUS) === DESTROYING) {
      if ((stream._duplexState & ACTIVE_OR_TICKING) === 0) {
        stream._duplexState |= ACTIVE
        stream._destroy(afterDestroy.bind(this))
      }
      return
    }

    if ((stream._duplexState & IS_OPENING) === OPENING) {
      stream._duplexState = (stream._duplexState | ACTIVE) & NOT_OPENING
      stream._open(afterOpen.bind(this))
    }
  }

  continueUpdate () {
    if ((this.stream._duplexState & WRITE_NEXT_TICK) === 0) return false
    this.stream._duplexState &= WRITE_NOT_NEXT_TICK
    return true
  }

  updateCallback () {
    if ((this.stream._duplexState & WRITE_UPDATE_SYNC_STATUS) === WRITE_PRIMARY) this.update()
    else this.updateNextTick()
  }

  updateNextTick () {
    if ((this.stream._duplexState & WRITE_NEXT_TICK) !== 0) return
    this.stream._duplexState |= WRITE_NEXT_TICK
    if ((this.stream._duplexState & WRITE_UPDATING) === 0) qmt(this.afterUpdateNextTick)
  }
}

class ReadableState {
  constructor (stream, { highWaterMark = 16384, map = null, mapReadable, byteLength, byteLengthReadable } = {}) {
    this.stream = stream
    this.queue = new FIFO()
    this.highWaterMark = highWaterMark === 0 ? 1 : highWaterMark
    this.buffered = 0
    this.readAhead = highWaterMark > 0
    this.error = null
    this.pipeline = null
    this.byteLength = byteLengthReadable || byteLength || defaultByteLength
    this.map = mapReadable || map
    this.pipeTo = null
    this.afterRead = afterRead.bind(this)
    this.afterUpdateNextTick = updateReadNT.bind(this)
  }

  get ended () {
    return (this.stream._duplexState & READ_DONE) !== 0
  }

  pipe (pipeTo, cb) {
    if (this.pipeTo !== null) throw new Error('Can only pipe to one destination')
    if (typeof cb !== 'function') cb = null

    this.stream._duplexState |= READ_PIPE_DRAINED
    this.pipeTo = pipeTo
    this.pipeline = new Pipeline(this.stream, pipeTo, cb)

    if (cb) this.stream.on('error', noop) // We already error handle this so supress crashes

    if (isStreamx(pipeTo)) {
      pipeTo._writableState.pipeline = this.pipeline
      if (cb) pipeTo.on('error', noop) // We already error handle this so supress crashes
      pipeTo.on('finish', this.pipeline.finished.bind(this.pipeline)) // TODO: just call finished from pipeTo itself
    } else {
      const onerror = this.pipeline.done.bind(this.pipeline, pipeTo)
      const onclose = this.pipeline.done.bind(this.pipeline, pipeTo, null) // onclose has a weird bool arg
      pipeTo.on('error', onerror)
      pipeTo.on('close', onclose)
      pipeTo.on('finish', this.pipeline.finished.bind(this.pipeline))
    }

    pipeTo.on('drain', afterDrain.bind(this))
    this.stream.emit('piping', pipeTo)
    pipeTo.emit('pipe', this.stream)
  }

  push (data) {
    const stream = this.stream

    if (data === null) {
      this.highWaterMark = 0
      stream._duplexState = (stream._duplexState | READ_ENDING) & READ_NON_PRIMARY_AND_PUSHED
      return false
    }

    if (this.map !== null) {
      data = this.map(data)
      if (data === null) {
        stream._duplexState &= READ_PUSHED
        return this.buffered < this.highWaterMark
      }
    }

    this.buffered += this.byteLength(data)
    this.queue.push(data)

    stream._duplexState = (stream._duplexState | READ_QUEUED) & READ_PUSHED

    return this.buffered < this.highWaterMark
  }

  shift () {
    const data = this.queue.shift()

    this.buffered -= this.byteLength(data)
    if (this.buffered === 0) this.stream._duplexState &= READ_NOT_QUEUED
    return data
  }

  unshift (data) {
    const pending = [this.map !== null ? this.map(data) : data]
    while (this.buffered > 0) pending.push(this.shift())

    for (let i = 0; i < pending.length - 1; i++) {
      const data = pending[i]
      this.buffered += this.byteLength(data)
      this.queue.push(data)
    }

    this.push(pending[pending.length - 1])
  }

  read () {
    const stream = this.stream

    if ((stream._duplexState & READ_STATUS) === READ_QUEUED) {
      const data = this.shift()
      if (this.pipeTo !== null && this.pipeTo.write(data) === false) stream._duplexState &= READ_PIPE_NOT_DRAINED
      if ((stream._duplexState & READ_EMIT_DATA) !== 0) stream.emit('data', data)
      return data
    }

    if (this.readAhead === false) {
      stream._duplexState |= READ_READ_AHEAD
      this.updateNextTick()
    }

    return null
  }

  drain () {
    const stream = this.stream

    while ((stream._duplexState & READ_STATUS) === READ_QUEUED && (stream._duplexState & READ_FLOWING) !== 0) {
      const data = this.shift()
      if (this.pipeTo !== null && this.pipeTo.write(data) === false) stream._duplexState &= READ_PIPE_NOT_DRAINED
      if ((stream._duplexState & READ_EMIT_DATA) !== 0) stream.emit('data', data)
    }
  }

  update () {
    const stream = this.stream

    stream._duplexState |= READ_UPDATING

    do {
      this.drain()

      while (this.buffered < this.highWaterMark && (stream._duplexState & SHOULD_NOT_READ) === READ_READ_AHEAD) {
        stream._duplexState |= READ_ACTIVE_AND_NEEDS_PUSH
        stream._read(this.afterRead)
        this.drain()
      }

      if ((stream._duplexState & READ_READABLE_STATUS) === READ_EMIT_READABLE_AND_QUEUED) {
        stream._duplexState |= READ_EMITTED_READABLE
        stream.emit('readable')
      }

      if ((stream._duplexState & READ_PRIMARY_AND_ACTIVE) === 0) this.updateNonPrimary()
    } while (this.continueUpdate() === true)

    stream._duplexState &= READ_NOT_UPDATING
  }

  updateNonPrimary () {
    const stream = this.stream

    if ((stream._duplexState & READ_ENDING_STATUS) === READ_ENDING) {
      stream._duplexState = (stream._duplexState | READ_DONE) & READ_NOT_ENDING
      stream.emit('end')
      if ((stream._duplexState & AUTO_DESTROY) === DONE) stream._duplexState |= DESTROYING
      if (this.pipeTo !== null) this.pipeTo.end()
    }

    if ((stream._duplexState & DESTROY_STATUS) === DESTROYING) {
      if ((stream._duplexState & ACTIVE_OR_TICKING) === 0) {
        stream._duplexState |= ACTIVE
        stream._destroy(afterDestroy.bind(this))
      }
      return
    }

    if ((stream._duplexState & IS_OPENING) === OPENING) {
      stream._duplexState = (stream._duplexState | ACTIVE) & NOT_OPENING
      stream._open(afterOpen.bind(this))
    }
  }

  continueUpdate () {
    if ((this.stream._duplexState & READ_NEXT_TICK) === 0) return false
    this.stream._duplexState &= READ_NOT_NEXT_TICK
    return true
  }

  updateCallback () {
    if ((this.stream._duplexState & READ_UPDATE_SYNC_STATUS) === READ_PRIMARY) this.update()
    else this.updateNextTick()
  }

  updateNextTickIfOpen () {
    if ((this.stream._duplexState & READ_NEXT_TICK_OR_OPENING) !== 0) return
    this.stream._duplexState |= READ_NEXT_TICK
    if ((this.stream._duplexState & READ_UPDATING) === 0) qmt(this.afterUpdateNextTick)
  }

  updateNextTick () {
    if ((this.stream._duplexState & READ_NEXT_TICK) !== 0) return
    this.stream._duplexState |= READ_NEXT_TICK
    if ((this.stream._duplexState & READ_UPDATING) === 0) qmt(this.afterUpdateNextTick)
  }
}

class TransformState {
  constructor (stream) {
    this.data = null
    this.afterTransform = afterTransform.bind(stream)
    this.afterFinal = null
  }
}

class Pipeline {
  constructor (src, dst, cb) {
    this.from = src
    this.to = dst
    this.afterPipe = cb
    this.error = null
    this.pipeToFinished = false
  }

  finished () {
    this.pipeToFinished = true
  }

  done (stream, err) {
    if (err) this.error = err

    if (stream === this.to) {
      this.to = null

      if (this.from !== null) {
        if ((this.from._duplexState & READ_DONE) === 0 || !this.pipeToFinished) {
          this.from.destroy(this.error || new Error('Writable stream closed prematurely'))
        }
        return
      }
    }

    if (stream === this.from) {
      this.from = null

      if (this.to !== null) {
        if ((stream._duplexState & READ_DONE) === 0) {
          this.to.destroy(this.error || new Error('Readable stream closed before ending'))
        }
        return
      }
    }

    if (this.afterPipe !== null) this.afterPipe(this.error)
    this.to = this.from = this.afterPipe = null
  }
}

function afterDrain () {
  this.stream._duplexState |= READ_PIPE_DRAINED
  this.updateCallback()
}

function afterFinal (err) {
  const stream = this.stream
  if (err) stream.destroy(err)
  if ((stream._duplexState & DESTROY_STATUS) === 0) {
    stream._duplexState |= WRITE_DONE
    stream.emit('finish')
  }
  if ((stream._duplexState & AUTO_DESTROY) === DONE) {
    stream._duplexState |= DESTROYING
  }

  stream._duplexState &= WRITE_NOT_FINISHING

  // no need to wait the extra tick here, so we short circuit that
  if ((stream._duplexState & WRITE_UPDATING) === 0) this.update()
  else this.updateNextTick()
}

function afterDestroy (err) {
  const stream = this.stream

  if (!err && this.error !== STREAM_DESTROYED) err = this.error
  if (err) stream.emit('error', err)
  stream._duplexState |= DESTROYED
  stream.emit('close')

  const rs = stream._readableState
  const ws = stream._writableState

  if (rs !== null && rs.pipeline !== null) rs.pipeline.done(stream, err)

  if (ws !== null) {
    while (ws.drains !== null && ws.drains.length > 0) ws.drains.shift().resolve(false)
    if (ws.pipeline !== null) ws.pipeline.done(stream, err)
  }
}

function afterWrite (err) {
  const stream = this.stream

  if (err) stream.destroy(err)
  stream._duplexState &= WRITE_NOT_ACTIVE

  if (this.drains !== null) tickDrains(this.drains)

  if ((stream._duplexState & WRITE_DRAIN_STATUS) === WRITE_UNDRAINED) {
    stream._duplexState &= WRITE_DRAINED
    if ((stream._duplexState & WRITE_EMIT_DRAIN) === WRITE_EMIT_DRAIN) {
      stream.emit('drain')
    }
  }

  this.updateCallback()
}

function afterRead (err) {
  if (err) this.stream.destroy(err)
  this.stream._duplexState &= READ_NOT_ACTIVE
  if (this.readAhead === false && (this.stream._duplexState & READ_RESUMED) === 0) this.stream._duplexState &= READ_NO_READ_AHEAD
  this.updateCallback()
}

function updateReadNT () {
  if ((this.stream._duplexState & READ_UPDATING) === 0) {
    this.stream._duplexState &= READ_NOT_NEXT_TICK
    this.update()
  }
}

function updateWriteNT () {
  if ((this.stream._duplexState & WRITE_UPDATING) === 0) {
    this.stream._duplexState &= WRITE_NOT_NEXT_TICK
    this.update()
  }
}

function tickDrains (drains) {
  for (let i = 0; i < drains.length; i++) {
    // drains.writes are monotonic, so if one is 0 its always the first one
    if (--drains[i].writes === 0) {
      drains.shift().resolve(true)
      i--
    }
  }
}

function afterOpen (err) {
  const stream = this.stream

  if (err) stream.destroy(err)

  if ((stream._duplexState & DESTROYING) === 0) {
    if ((stream._duplexState & READ_PRIMARY_STATUS) === 0) stream._duplexState |= READ_PRIMARY
    if ((stream._duplexState & WRITE_PRIMARY_STATUS) === 0) stream._duplexState |= WRITE_PRIMARY
    stream.emit('open')
  }

  stream._duplexState &= NOT_ACTIVE

  if (stream._writableState !== null) {
    stream._writableState.updateCallback()
  }

  if (stream._readableState !== null) {
    stream._readableState.updateCallback()
  }
}

function afterTransform (err, data) {
  if (data !== undefined && data !== null) this.push(data)
  this._writableState.afterWrite(err)
}

function newListener (name) {
  if (this._readableState !== null) {
    if (name === 'data') {
      this._duplexState |= (READ_EMIT_DATA | READ_RESUMED_READ_AHEAD)
      this._readableState.updateNextTick()
    }
    if (name === 'readable') {
      this._duplexState |= READ_EMIT_READABLE
      this._readableState.updateNextTick()
    }
  }

  if (this._writableState !== null) {
    if (name === 'drain') {
      this._duplexState |= WRITE_EMIT_DRAIN
      this._writableState.updateNextTick()
    }
  }
}

class Stream extends EventEmitter {
  constructor (opts) {
    super()

    this._duplexState = 0
    this._readableState = null
    this._writableState = null

    if (opts) {
      if (opts.open) this._open = opts.open
      if (opts.destroy) this._destroy = opts.destroy
      if (opts.predestroy) this._predestroy = opts.predestroy
      if (opts.signal) {
        opts.signal.addEventListener('abort', abort.bind(this))
      }
    }

    this.on('newListener', newListener)
  }

  _open (cb) {
    cb(null)
  }

  _destroy (cb) {
    cb(null)
  }

  _predestroy () {
    // does nothing
  }

  get readable () {
    return this._readableState !== null ? true : undefined
  }

  get writable () {
    return this._writableState !== null ? true : undefined
  }

  get destroyed () {
    return (this._duplexState & DESTROYED) !== 0
  }

  get destroying () {
    return (this._duplexState & DESTROY_STATUS) !== 0
  }

  destroy (err) {
    if ((this._duplexState & DESTROY_STATUS) === 0) {
      if (!err) err = STREAM_DESTROYED
      this._duplexState = (this._duplexState | DESTROYING) & NON_PRIMARY

      if (this._readableState !== null) {
        this._readableState.highWaterMark = 0
        this._readableState.error = err
      }
      if (this._writableState !== null) {
        this._writableState.highWaterMark = 0
        this._writableState.error = err
      }

      this._duplexState |= PREDESTROYING
      this._predestroy()
      this._duplexState &= NOT_PREDESTROYING

      if (this._readableState !== null) this._readableState.updateNextTick()
      if (this._writableState !== null) this._writableState.updateNextTick()
    }
  }
}

class Readable extends Stream {
  constructor (opts) {
    super(opts)

    this._duplexState |= OPENING | WRITE_DONE | READ_READ_AHEAD
    this._readableState = new ReadableState(this, opts)

    if (opts) {
      if (this._readableState.readAhead === false) this._duplexState &= READ_NO_READ_AHEAD
      if (opts.read) this._read = opts.read
      if (opts.eagerOpen) this._readableState.updateNextTick()
      if (opts.encoding) this.setEncoding(opts.encoding)
    }
  }

  setEncoding (encoding) {
    const dec = new TextDecoder(encoding)
    const map = this._readableState.map || echo
    this._readableState.map = mapOrSkip
    return this

    function mapOrSkip (data) {
      const next = dec.push(data)
      return next === '' && (data.byteLength !== 0 || dec.remaining > 0) ? null : map(next)
    }
  }

  _read (cb) {
    cb(null)
  }

  pipe (dest, cb) {
    this._readableState.updateNextTick()
    this._readableState.pipe(dest, cb)
    return dest
  }

  read () {
    this._readableState.updateNextTick()
    return this._readableState.read()
  }

  push (data) {
    this._readableState.updateNextTickIfOpen()
    return this._readableState.push(data)
  }

  unshift (data) {
    this._readableState.updateNextTickIfOpen()
    return this._readableState.unshift(data)
  }

  resume () {
    this._duplexState |= READ_RESUMED_READ_AHEAD
    this._readableState.updateNextTick()
    return this
  }

  pause () {
    this._duplexState &= (this._readableState.readAhead === false ? READ_PAUSED_NO_READ_AHEAD : READ_PAUSED)
    return this
  }

  static _fromAsyncIterator (ite, opts) {
    let destroy

    const rs = new Readable({
      ...opts,
      read (cb) {
        ite.next().then(push).then(cb.bind(null, null)).catch(cb)
      },
      predestroy () {
        destroy = ite.return()
      },
      destroy (cb) {
        if (!destroy) return cb(null)
        destroy.then(cb.bind(null, null)).catch(cb)
      }
    })

    return rs

    function push (data) {
      if (data.done) rs.push(null)
      else rs.push(data.value)
    }
  }

  static from (data, opts) {
    if (isReadStreamx(data)) return data
    if (data[asyncIterator]) return this._fromAsyncIterator(data[asyncIterator](), opts)
    if (!Array.isArray(data)) data = data === undefined ? [] : [data]

    let i = 0
    return new Readable({
      ...opts,
      read (cb) {
        this.push(i === data.length ? null : data[i++])
        cb(null)
      }
    })
  }

  static isBackpressured (rs) {
    return (rs._duplexState & READ_BACKPRESSURE_STATUS) !== 0 || rs._readableState.buffered >= rs._readableState.highWaterMark
  }

  static isPaused (rs) {
    return (rs._duplexState & READ_RESUMED) === 0
  }

  [asyncIterator] () {
    const stream = this

    let error = null
    let promiseResolve = null
    let promiseReject = null

    this.on('error', (err) => { error = err })
    this.on('readable', onreadable)
    this.on('close', onclose)

    return {
      [asyncIterator] () {
        return this
      },
      next () {
        return new Promise(function (resolve, reject) {
          promiseResolve = resolve
          promiseReject = reject
          const data = stream.read()
          if (data !== null) ondata(data)
          else if ((stream._duplexState & DESTROYED) !== 0) ondata(null)
        })
      },
      return () {
        return destroy(null)
      },
      throw (err) {
        return destroy(err)
      }
    }

    function onreadable () {
      if (promiseResolve !== null) ondata(stream.read())
    }

    function onclose () {
      if (promiseResolve !== null) ondata(null)
    }

    function ondata (data) {
      if (promiseReject === null) return
      if (error) promiseReject(error)
      else if (data === null && (stream._duplexState & READ_DONE) === 0) promiseReject(STREAM_DESTROYED)
      else promiseResolve({ value: data, done: data === null })
      promiseReject = promiseResolve = null
    }

    function destroy (err) {
      stream.destroy(err)
      return new Promise((resolve, reject) => {
        if (stream._duplexState & DESTROYED) return resolve({ value: undefined, done: true })
        stream.once('close', function () {
          if (err) reject(err)
          else resolve({ value: undefined, done: true })
        })
      })
    }
  }
}

class Writable extends Stream {
  constructor (opts) {
    super(opts)

    this._duplexState |= OPENING | READ_DONE
    this._writableState = new WritableState(this, opts)

    if (opts) {
      if (opts.writev) this._writev = opts.writev
      if (opts.write) this._write = opts.write
      if (opts.final) this._final = opts.final
      if (opts.eagerOpen) this._writableState.updateNextTick()
    }
  }

  cork () {
    this._duplexState |= WRITE_CORKED
  }

  uncork () {
    this._duplexState &= WRITE_NOT_CORKED
    this._writableState.updateNextTick()
  }

  _writev (batch, cb) {
    cb(null)
  }

  _write (data, cb) {
    this._writableState.autoBatch(data, cb)
  }

  _final (cb) {
    cb(null)
  }

  static isBackpressured (ws) {
    return (ws._duplexState & WRITE_BACKPRESSURE_STATUS) !== 0
  }

  static drained (ws) {
    if (ws.destroyed) return Promise.resolve(false)
    const state = ws._writableState
    const pending = (isWritev(ws) ? Math.min(1, state.queue.length) : state.queue.length)
    const writes = pending + ((ws._duplexState & WRITE_WRITING) ? 1 : 0)
    if (writes === 0) return Promise.resolve(true)
    if (state.drains === null) state.drains = []
    return new Promise((resolve) => {
      state.drains.push({ writes, resolve })
    })
  }

  write (data) {
    this._writableState.updateNextTick()
    return this._writableState.push(data)
  }

  end (data) {
    this._writableState.updateNextTick()
    this._writableState.end(data)
    return this
  }
}

class Duplex extends Readable { // and Writable
  constructor (opts) {
    super(opts)

    this._duplexState = OPENING | (this._duplexState & READ_READ_AHEAD)
    this._writableState = new WritableState(this, opts)

    if (opts) {
      if (opts.writev) this._writev = opts.writev
      if (opts.write) this._write = opts.write
      if (opts.final) this._final = opts.final
    }
  }

  cork () {
    this._duplexState |= WRITE_CORKED
  }

  uncork () {
    this._duplexState &= WRITE_NOT_CORKED
    this._writableState.updateNextTick()
  }

  _writev (batch, cb) {
    cb(null)
  }

  _write (data, cb) {
    this._writableState.autoBatch(data, cb)
  }

  _final (cb) {
    cb(null)
  }

  write (data) {
    this._writableState.updateNextTick()
    return this._writableState.push(data)
  }

  end (data) {
    this._writableState.updateNextTick()
    this._writableState.end(data)
    return this
  }
}

class Transform extends Duplex {
  constructor (opts) {
    super(opts)
    this._transformState = new TransformState(this)

    if (opts) {
      if (opts.transform) this._transform = opts.transform
      if (opts.flush) this._flush = opts.flush
    }
  }

  _write (data, cb) {
    if (this._readableState.buffered >= this._readableState.highWaterMark) {
      this._transformState.data = data
    } else {
      this._transform(data, this._transformState.afterTransform)
    }
  }

  _read (cb) {
    if (this._transformState.data !== null) {
      const data = this._transformState.data
      this._transformState.data = null
      cb(null)
      this._transform(data, this._transformState.afterTransform)
    } else {
      cb(null)
    }
  }

  destroy (err) {
    super.destroy(err)
    if (this._transformState.data !== null) {
      this._transformState.data = null
      this._transformState.afterTransform()
    }
  }

  _transform (data, cb) {
    cb(null, data)
  }

  _flush (cb) {
    cb(null)
  }

  _final (cb) {
    this._transformState.afterFinal = cb
    this._flush(transformAfterFlush.bind(this))
  }
}

class PassThrough extends Transform {}

function transformAfterFlush (err, data) {
  const cb = this._transformState.afterFinal
  if (err) return cb(err)
  if (data !== null && data !== undefined) this.push(data)
  this.push(null)
  cb(null)
}

function pipelinePromise (...streams) {
  return new Promise((resolve, reject) => {
    return pipeline(...streams, (err) => {
      if (err) return reject(err)
      resolve()
    })
  })
}

function pipeline (stream, ...streams) {
  const all = Array.isArray(stream) ? [...stream, ...streams] : [stream, ...streams]
  const done = (all.length && typeof all[all.length - 1] === 'function') ? all.pop() : null

  if (all.length < 2) throw new Error('Pipeline requires at least 2 streams')

  let src = all[0]
  let dest = null
  let error = null

  for (let i = 1; i < all.length; i++) {
    dest = all[i]

    if (isStreamx(src)) {
      src.pipe(dest, onerror)
    } else {
      errorHandle(src, true, i > 1, onerror)
      src.pipe(dest)
    }

    src = dest
  }

  if (done) {
    let fin = false

    const autoDestroy = isStreamx(dest) || !!(dest._writableState && dest._writableState.autoDestroy)

    dest.on('error', (err) => {
      if (error === null) error = err
    })

    dest.on('finish', () => {
      fin = true
      if (!autoDestroy) done(error)
    })

    if (autoDestroy) {
      dest.on('close', () => done(error || (fin ? null : PREMATURE_CLOSE)))
    }
  }

  return dest

  function errorHandle (s, rd, wr, onerror) {
    s.on('error', onerror)
    s.on('close', onclose)

    function onclose () {
      if (rd && s._readableState && !s._readableState.ended) return onerror(PREMATURE_CLOSE)
      if (wr && s._writableState && !s._writableState.ended) return onerror(PREMATURE_CLOSE)
    }
  }

  function onerror (err) {
    if (!err || error) return
    error = err

    for (const s of all) {
      s.destroy(err)
    }
  }
}

function echo (s) {
  return s
}

function isStream (stream) {
  return !!stream._readableState || !!stream._writableState
}

function isStreamx (stream) {
  return typeof stream._duplexState === 'number' && isStream(stream)
}

function isEnded (stream) {
  return !!stream._readableState && stream._readableState.ended
}

function isFinished (stream) {
  return !!stream._writableState && stream._writableState.ended
}

function getStreamError (stream, opts = {}) {
  const err = (stream._readableState && stream._readableState.error) || (stream._writableState && stream._writableState.error)

  // avoid implicit errors by default
  return (!opts.all && err === STREAM_DESTROYED) ? null : err
}

function isReadStreamx (stream) {
  return isStreamx(stream) && stream.readable
}

function isDisturbed (stream) {
  return (stream._duplexState & OPENING) !== OPENING || (stream._duplexState & ACTIVE_OR_TICKING) !== 0
}

function isTypedArray (data) {
  return typeof data === 'object' && data !== null && typeof data.byteLength === 'number'
}

function defaultByteLength (data) {
  return isTypedArray(data) ? data.byteLength : 1024
}

function noop () {}

function abort () {
  this.destroy(new Error('Stream aborted.'))
}

function isWritev (s) {
  return s._writev !== Writable.prototype._writev && s._writev !== Duplex.prototype._writev
}

module.exports = {
  pipeline,
  pipelinePromise,
  isStream,
  isStreamx,
  isEnded,
  isFinished,
  isDisturbed,
  getStreamError,
  Stream,
  Writable,
  Readable,
  Duplex,
  Transform,
  // Export PassThrough for compatibility with Node.js core's stream module
  PassThrough
}
{
  "name": "streamx",
  "version": "2.23.0",
  "description": "An iteration of the Node.js core streams with a series of improvements",
  "main": "index.js",
  "dependencies": {
    "events-universal": "^1.0.0",
    "fast-fifo": "^1.3.2",
    "text-decoder": "^1.1.0"
  },
  "devDependencies": {
    "b4a": "^1.6.6",
    "brittle": "^3.1.1",
    "end-of-stream": "^1.4.4",
    "standard": "^17.0.0"
  },
  "files": [
    "index.js"
  ],
  "scripts": {
    "test": "standard && node test/all.js",
    "test:bare": "standard && bare test/all.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/streamx.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/streamx/issues"
  },
  "homepage": "https://github.com/mafintosh/streamx"
}
const PassThroughDecoder = require('./lib/pass-through-decoder')
const UTF8Decoder = require('./lib/utf8-decoder')

module.exports = class TextDecoder {
  constructor (encoding = 'utf8') {
    this.encoding = normalizeEncoding(encoding)

    switch (this.encoding) {
      case 'utf8':
        this.decoder = new UTF8Decoder()
        break
      case 'utf16le':
      case 'base64':
        throw new Error('Unsupported encoding: ' + this.encoding)
      default:
        this.decoder = new PassThroughDecoder(this.encoding)
    }
  }

  get remaining () {
    return this.decoder.remaining
  }

  push (data) {
    if (typeof data === 'string') return data
    return this.decoder.decode(data)
  }

  // For Node.js compatibility
  write (data) {
    return this.push(data)
  }

  end (data) {
    let result = ''
    if (data) result = this.push(data)
    result += this.decoder.flush()
    return result
  }
}

function normalizeEncoding (encoding) {
  encoding = encoding.toLowerCase()

  switch (encoding) {
    case 'utf8':
    case 'utf-8':
      return 'utf8'
    case 'ucs2':
    case 'ucs-2':
    case 'utf16le':
    case 'utf-16le':
      return 'utf16le'
    case 'latin1':
    case 'binary':
      return 'latin1'
    case 'base64':
    case 'ascii':
    case 'hex':
      return encoding
    default:
      throw new Error('Unknown encoding: ' + encoding)
  }
};
const b4a = require('b4a')

module.exports = class PassThroughDecoder {
  constructor (encoding) {
    this.encoding = encoding
  }

  get remaining () {
    return 0
  }

  decode (tail) {
    return b4a.toString(tail, this.encoding)
  }

  flush () {
    return ''
  }
}
const b4a = require('b4a')

/**
 * https://encoding.spec.whatwg.org/#utf-8-decoder
 */
module.exports = class UTF8Decoder {
  constructor () {
    this.codePoint = 0
    this.bytesSeen = 0
    this.bytesNeeded = 0
    this.lowerBoundary = 0x80
    this.upperBoundary = 0xbf
  }

  get remaining () {
    return this.bytesSeen
  }

  decode (data) {
    // If we have a fast path, just sniff if the last part is a boundary
    if (this.bytesNeeded === 0) {
      let isBoundary = true

      for (let i = Math.max(0, data.byteLength - 4), n = data.byteLength; i < n && isBoundary; i++) {
        isBoundary = data[i] <= 0x7f
      }

      if (isBoundary) return b4a.toString(data, 'utf8')
    }

    let result = ''

    for (let i = 0, n = data.byteLength; i < n; i++) {
      const byte = data[i]

      if (this.bytesNeeded === 0) {
        if (byte <= 0x7f) {
          result += String.fromCharCode(byte)
        } else {
          this.bytesSeen = 1

          if (byte >= 0xc2 && byte <= 0xdf) {
            this.bytesNeeded = 2
            this.codePoint = byte & 0x1f
          } else if (byte >= 0xe0 && byte <= 0xef) {
            if (byte === 0xe0) this.lowerBoundary = 0xa0
            else if (byte === 0xed) this.upperBoundary = 0x9f
            this.bytesNeeded = 3
            this.codePoint = byte & 0xf
          } else if (byte >= 0xf0 && byte <= 0xf4) {
            if (byte === 0xf0) this.lowerBoundary = 0x90
            if (byte === 0xf4) this.upperBoundary = 0x8f
            this.bytesNeeded = 4
            this.codePoint = byte & 0x7
          } else {
            result += '\ufffd'
          }
        }

        continue
      }

      if (byte < this.lowerBoundary || byte > this.upperBoundary) {
        this.codePoint = 0
        this.bytesNeeded = 0
        this.bytesSeen = 0
        this.lowerBoundary = 0x80
        this.upperBoundary = 0xbf

        result += '\ufffd'

        continue
      }

      this.lowerBoundary = 0x80
      this.upperBoundary = 0xbf

      this.codePoint = (this.codePoint << 6) | (byte & 0x3f)
      this.bytesSeen++

      if (this.bytesSeen !== this.bytesNeeded) continue

      result += String.fromCodePoint(this.codePoint)

      this.codePoint = 0
      this.bytesNeeded = 0
      this.bytesSeen = 0
    }

    return result
  }

  flush () {
    const result = this.bytesNeeded > 0 ? '\ufffd' : ''

    this.codePoint = 0
    this.bytesNeeded = 0
    this.bytesSeen = 0
    this.lowerBoundary = 0x80
    this.upperBoundary = 0xbf

    return result
  }
}
{
  "name": "text-decoder",
  "version": "1.2.3",
  "description": "Streaming text decoder that preserves multibyte Unicode characters",
  "main": "index.js",
  "files": [
    "index.js",
    "lib"
  ],
  "browser": {
    "./lib/pass-through-decoder.js": "./lib/browser-decoder.js",
    "./lib/utf8-decoder.js": "./lib/browser-decoder.js"
  },
  "react-native": {
    "./lib/pass-through-decoder.js": "./lib/pass-through-decoder.js",
    "./lib/utf8-decoder.js": "./lib/utf8-decoder.js"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "git+https://github.com/holepunchto/text-decoder.git"
  },
  "author": "Holepunch",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/text-decoder/issues"
  },
  "homepage": "https://github.com/holepunchto/text-decoder#readme",
  "dependencies": {
    "b4a": "^1.6.4"
  },
  "devDependencies": {
    "brittle": "^3.3.2",
    "standard": "^17.0.0"
  }
}
module.exports = resolve

function parse (addr) {
  const names = addr.split(/[/\\]/)

  const r = {
    isAbsolute: false,
    names
  }

  // don't think this ever happens, but whatevs
  if (names.length === 0) return r

  if (names.length > 1 && names[0].endsWith(':')) {
    r.isAbsolute = true

    if (names[0].length === 2) { // windows
      r.names = names.slice(1)
      return r
    }

    if (names[0] === 'file:') {
      r.names = names.slice(1)
      return r
    }

    r.names = names.slice(3)
    return r
  }

  r.isAbsolute = addr.startsWith('/') || addr.startsWith('\\')

  return r
}

function resolve (a, b = '') {
  const ap = parse(a)
  const bp = parse(b)

  if (bp.isAbsolute) {
    return resolveNames([], bp.names)
  }

  if (!ap.isAbsolute) {
    throw new Error('One of the two paths must be absolute')
  }

  return resolveNames(ap.names, bp.names)
}

function toString (p, names) {
  for (let i = 0; i < names.length; i++) {
    if (names[i] === '') continue
    if (names[i] === '.') continue
    if (names[i] === '..') {
      if (p.length === 1) throw new Error('Path cannot be resolved, too many \'..\'')
      p = p.slice(0, p.lastIndexOf('/')) || '/'
      continue
    }
    p += (p.length === 1 ? names[i] : '/' + names[i])
  }

  return p
}

function resolveNames (a, b) {
  return toString(toString('/', a), b)
}
{
  "name": "unix-path-resolve",
  "version": "1.0.2",
  "description": "Cross platform resolve that always returns a UNIX style `/` seperated path",
  "main": "index.js",
  "dependencies": {},
  "devDependencies": {
    "brittle": "^2.0.1",
    "standard": "^16.0.4"
  },
  "scripts": {
    "test": "standard && brittle test.js"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/mafintosh/unix-path-resolve.git"
  },
  "author": "Mathias Buus (@mafintosh)",
  "license": "MIT",
  "bugs": {
    "url": "https://github.com/mafintosh/unix-path-resolve/issues"
  },
  "homepage": "https://github.com/mafintosh/unix-path-resolve"
}
const path = require('path')
const { isWindows } = require('which-runtime')

exports.fileURLToPath = function fileURLToPath (url) {
  if (typeof url === 'string') {
    url = new URL(url)
  }

  if (url.protocol !== 'file:') {
    throw new Error('The URL must use the file: protocol')
  }

  if (isWindows) {
    if (/%2f|%5c/i.test(url.pathname)) {
      throw new Error('The file: URL path must not include encoded \\ or / characters')
    }
  } else {
    if (url.hostname) {
      throw new Error('The file: URL host must be \'localhost\' or empty')
    }

    if (/%2f/i.test(url.pathname)) {
      throw new Error('The file: URL path must not include encoded / characters')
    }
  }

  const pathname = path.normalize(decodeURIComponent(url.pathname))

  if (isWindows) {
    if (url.hostname) return '\\\\' + url.hostname + pathname

    const letter = pathname.charCodeAt(1) | 0x20

    if (letter < 0x61 /* a */ || letter > 0x7a /* z */ || pathname.charCodeAt(2) !== 0x3a /* : */) {
      throw new Error('The file: URL path must be absolute')
    }

    return pathname.slice(1)
  }

  return pathname
}

exports.pathToFileURL = function pathToFileURL (pathname) {
  let resolved = path.resolve(pathname)

  if (pathname[pathname.length - 1] === '/') {
    resolved += '/'
  } else if (isWindows && pathname[pathname.length - 1] === '\\') {
    resolved += '\\'
  }

  resolved = resolved
    .replaceAll('%', '%25') // Must be first
    .replaceAll('#', '%23')
    .replaceAll('?', '%3f')
    .replaceAll('\n', '%0a')
    .replaceAll('\r', '%0d')
    .replaceAll('\t', '%09')

  if (!isWindows) {
    resolved = resolved.replaceAll('\\', '%5c')
  }

  return new URL('file:' + resolved)
}
{
  "name": "url-file-url",
  "version": "1.0.5",
  "description": "Small module that converts from URLs to filenames to URLs",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "dependencies": {
    "bare-path": "^3.0.0",
    "which-runtime": "^1.2.0"
  },
  "devDependencies": {
    "standard": "^17.1.0"
  },
  "scripts": {
    "test": "standard"
  },
  "imports": {
    "path": {
      "bare": "bare-path",
      "default": "path"
    }
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/url-file-url.git"
  },
  "author": "Holepunch Inc",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/url-file-url/issues"
  },
  "homepage": "https://github.com/holepunchto/url-file-url"
}
const { runtime, platform, arch } = typeof Bare !== 'undefined'
  ? { runtime: 'bare', platform: global.Bare.platform, arch: global.Bare.arch }
  : typeof process !== 'undefined'
    ? { runtime: 'node', platform: global.process.platform, arch: global.process.arch }
    : typeof Window !== 'undefined'
      ? { runtime: 'browser', platform: 'unknown', arch: 'unknown' }
      : { runtime: 'unknown', platform: 'unknown', arch: 'unknown' }

exports.runtime = runtime
exports.platform = platform
exports.arch = arch
exports.isBare = runtime === 'bare'
exports.isBareKit = exports.isBare && typeof BareKit !== 'undefined'
exports.isPear = typeof Pear !== 'undefined'
exports.isNode = runtime === 'node'
exports.isBrowser = runtime === 'browser'
exports.isWindows = platform === 'win32'
exports.isLinux = platform === 'linux'
exports.isMac = platform === 'darwin'
exports.isIOS = platform === 'ios' || platform === 'ios-simulator'
exports.isAndroid = platform === 'android'
exports.isElectron = typeof process !== 'undefined' && !!global.process.versions?.electron
exports.isElectronRenderer = exports.isElectron && global.process.type === 'renderer'
exports.isElectronWorker = exports.isElectron && global.process.type === 'worker'
{
  "name": "which-runtime",
  "version": "1.3.2",
  "description": "Detect if you are in Bare or Node and which os etc",
  "main": "index.js",
  "files": [
    "index.js"
  ],
  "dependencies": {},
  "devDependencies": {
    "standard": "^17.0.0"
  },
  "scripts": {
    "test": "standard"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/holepunchto/which-runtime.git"
  },
  "author": "Holepunch Inc.",
  "license": "Apache-2.0",
  "bugs": {
    "url": "https://github.com/holepunchto/which-runtime/issues"
  },
  "homepage": "https://github.com/holepunchto/which-runtime"
}
{
  "name": "pear-electron",
  "version": "1.7.25",
  "description": "Pear User-Interface Library for Electron",
  "main": "index.js",
  "scripts": {
    "bootstrap": "pear run scripts/bootstrap.js",
    "decal": "pear run scripts/decal.js",
    "bundle": "pear run scripts/bundle.js",
    "prestage": "pear run scripts/prestage.mjs",
    "lint": "standard"
  },
  "files": [
    "api.js",
    "boot.js",
    "bootstrap.js",
    "electron-main.js",
    "gui/",
    "index.js",
    "load.bundle",
    "pre.js",
    "preload.js",
    "runtime.js"
  ],
  "imports": {
    "fs": {
      "bare": "bare-fs"
    },
    "module": {
      "bare": "bare-module"
    },
    "path": {
      "bare": "bare-path"
    }
  },
  "pear": {
    "assets": {
      "ui": {
        "link": "pear://0.1046.q1r7eamni7ibtz76rw3tmjtbpwkenm9yk1g8ft35dwmw7646wagy",
        "name": "Pear Runtime",
        "only": [
          "/boot.bundle",
          "/by-arch/%%HOST%%",
          "/prebuilds/%%HOST%%"
        ]
      }
    },
    "stage": {
      "skipWarmup": "true",
      "only": [
        "boot.bundle",
        "by-arch",
        "CHANGELOG.md",
        "prebuilds",
        "template"
      ]
    }
  },
  "standard": {
    "globals": [
      "Bare",
      "Pear",
      "LOG"
    ],
    "ignore": [
      "template"
    ]
  },
  "keywords": [
    "pear",
    "runtime",
    "electron",
    "Pear Runtime",
    "peer-to-peer",
    "user-interface",
    "UI",
    "interface"
  ],
  "author": "Holepunch",
  "license": "Apache-2.0",
  "dependencies": {
    "bare-bundle": "^1.9.0",
    "bare-bundle-compile": "^1.2.1",
    "bare-bundle-evaluate": "^1.3.2",
    "bare-env": "^3.0.0",
    "bare-fs": "^4.0.1",
    "bare-module": "^5.0.3",
    "bare-os": "^3.4.0",
    "bare-path": "^3.0.0",
    "bare-subprocess": "^5.0.2",
    "blake2b": "^2.1.4",
    "compact-encoding": "^2.16.1",
    "localdrive": "^1.12.1",
    "paparam": "^1.6.1",
    "pear-api": "^1.29.2",
    "pear-appdrive": "^1.1.1",
    "pear-cmd": "^1.0.0",
    "pear-constants": "^1.1.3",
    "pear-crasher": "^1.0.1",
    "pear-errors": "^1.0.0",
    "pear-gunk": "^1.0.0",
    "pear-ipc": "file:../pear-ipc/pear-ipc-6.7.0.tgz",
    "pear-link": "^4.1.0",
    "pear-logger": "^1.0.0",
    "pear-pipe": "^1.0.6",
    "pear-run": "^1.0.0",
    "pear-state": "^1.0.4",
    "pear-tryboot": "^1.0.0",
    "script-linker": "^2.5.3",
    "streamx": "^2.20.2",
    "unix-path-resolve": "^1.0.2",
    "url-file-url": "^1.0.4",
    "which-runtime": "^1.2.1"
  },
  "devDependencies": {
    "@fontsource/open-sans": "^5.1.0",
    "crc-universal": "^1.0.4",
    "fs-native-extensions": "^1.4.2",
    "pear-interface": "^1.0.3",
    "pear-pack": "^1.1.2",
    "pear-terminal": "^1.0.0",
    "quickbit-universal": "^2.2.0",
    "redhat-overpass-font": "^1.0.0",
    "rocksdb-native": "^3.5.5",
    "sodium-native": "^5.0.1",
    "standard": "^17.1.2",
    "udx-native": "^1.18.3"
  }
}
